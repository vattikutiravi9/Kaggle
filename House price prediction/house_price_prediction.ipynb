{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Untitled1.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyN9orObmSjgIgYv2A4O2Kwg"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"wPCVuAYTiThI","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":71},"outputId":"83bff8e9-b12e-44bf-bc51-39a9646b6309","executionInfo":{"status":"ok","timestamp":1591804704489,"user_tz":240,"elapsed":2336,"user":{"displayName":"Ravi Vattikuti","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjyKWq_9kMPQBBkVSgJCrYyroBcPqR0yjOIIrSQjcE=s64","userId":"12788632665998596893"}}},"source":["import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","from sklearn import preprocessing, linear_model\n","from sklearn.preprocessing import OneHotEncoder\n","import xgboost\n","from sklearn.model_selection import RandomizedSearchCV\n","% matplotlib inline"],"execution_count":1,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n","  import pandas.util.testing as tm\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"vuOERdkIi9DQ","colab_type":"code","colab":{"resources":{"http://localhost:8080/nbextensions/google.colab/files.js":{"data":"Ly8gQ29weXJpZ2h0IDIwMTcgR29vZ2xlIExMQwovLwovLyBMaWNlbnNlZCB1bmRlciB0aGUgQXBhY2hlIExpY2Vuc2UsIFZlcnNpb24gMi4wICh0aGUgIkxpY2Vuc2UiKTsKLy8geW91IG1heSBub3QgdXNlIHRoaXMgZmlsZSBleGNlcHQgaW4gY29tcGxpYW5jZSB3aXRoIHRoZSBMaWNlbnNlLgovLyBZb3UgbWF5IG9idGFpbiBhIGNvcHkgb2YgdGhlIExpY2Vuc2UgYXQKLy8KLy8gICAgICBodHRwOi8vd3d3LmFwYWNoZS5vcmcvbGljZW5zZXMvTElDRU5TRS0yLjAKLy8KLy8gVW5sZXNzIHJlcXVpcmVkIGJ5IGFwcGxpY2FibGUgbGF3IG9yIGFncmVlZCB0byBpbiB3cml0aW5nLCBzb2Z0d2FyZQovLyBkaXN0cmlidXRlZCB1bmRlciB0aGUgTGljZW5zZSBpcyBkaXN0cmlidXRlZCBvbiBhbiAiQVMgSVMiIEJBU0lTLAovLyBXSVRIT1VUIFdBUlJBTlRJRVMgT1IgQ09ORElUSU9OUyBPRiBBTlkgS0lORCwgZWl0aGVyIGV4cHJlc3Mgb3IgaW1wbGllZC4KLy8gU2VlIHRoZSBMaWNlbnNlIGZvciB0aGUgc3BlY2lmaWMgbGFuZ3VhZ2UgZ292ZXJuaW5nIHBlcm1pc3Npb25zIGFuZAovLyBsaW1pdGF0aW9ucyB1bmRlciB0aGUgTGljZW5zZS4KCi8qKgogKiBAZmlsZW92ZXJ2aWV3IEhlbHBlcnMgZm9yIGdvb2dsZS5jb2xhYiBQeXRob24gbW9kdWxlLgogKi8KKGZ1bmN0aW9uKHNjb3BlKSB7CmZ1bmN0aW9uIHNwYW4odGV4dCwgc3R5bGVBdHRyaWJ1dGVzID0ge30pIHsKICBjb25zdCBlbGVtZW50ID0gZG9jdW1lbnQuY3JlYXRlRWxlbWVudCgnc3BhbicpOwogIGVsZW1lbnQudGV4dENvbnRlbnQgPSB0ZXh0OwogIGZvciAoY29uc3Qga2V5IG9mIE9iamVjdC5rZXlzKHN0eWxlQXR0cmlidXRlcykpIHsKICAgIGVsZW1lbnQuc3R5bGVba2V5XSA9IHN0eWxlQXR0cmlidXRlc1trZXldOwogIH0KICByZXR1cm4gZWxlbWVudDsKfQoKLy8gTWF4IG51bWJlciBvZiBieXRlcyB3aGljaCB3aWxsIGJlIHVwbG9hZGVkIGF0IGEgdGltZS4KY29uc3QgTUFYX1BBWUxPQURfU0laRSA9IDEwMCAqIDEwMjQ7Ci8vIE1heCBhbW91bnQgb2YgdGltZSB0byBibG9jayB3YWl0aW5nIGZvciB0aGUgdXNlci4KY29uc3QgRklMRV9DSEFOR0VfVElNRU9VVF9NUyA9IDMwICogMTAwMDsKCmZ1bmN0aW9uIF91cGxvYWRGaWxlcyhpbnB1dElkLCBvdXRwdXRJZCkgewogIGNvbnN0IHN0ZXBzID0gdXBsb2FkRmlsZXNTdGVwKGlucHV0SWQsIG91dHB1dElkKTsKICBjb25zdCBvdXRwdXRFbGVtZW50ID0gZG9jdW1lbnQuZ2V0RWxlbWVudEJ5SWQob3V0cHV0SWQpOwogIC8vIENhY2hlIHN0ZXBzIG9uIHRoZSBvdXRwdXRFbGVtZW50IHRvIG1ha2UgaXQgYXZhaWxhYmxlIGZvciB0aGUgbmV4dCBjYWxsCiAgLy8gdG8gdXBsb2FkRmlsZXNDb250aW51ZSBmcm9tIFB5dGhvbi4KICBvdXRwdXRFbGVtZW50LnN0ZXBzID0gc3RlcHM7CgogIHJldHVybiBfdXBsb2FkRmlsZXNDb250aW51ZShvdXRwdXRJZCk7Cn0KCi8vIFRoaXMgaXMgcm91Z2hseSBhbiBhc3luYyBnZW5lcmF0b3IgKG5vdCBzdXBwb3J0ZWQgaW4gdGhlIGJyb3dzZXIgeWV0KSwKLy8gd2hlcmUgdGhlcmUgYXJlIG11bHRpcGxlIGFzeW5jaHJvbm91cyBzdGVwcyBhbmQgdGhlIFB5dGhvbiBzaWRlIGlzIGdvaW5nCi8vIHRvIHBvbGwgZm9yIGNvbXBsZXRpb24gb2YgZWFjaCBzdGVwLgovLyBUaGlzIHVzZXMgYSBQcm9taXNlIHRvIGJsb2NrIHRoZSBweXRob24gc2lkZSBvbiBjb21wbGV0aW9uIG9mIGVhY2ggc3RlcCwKLy8gdGhlbiBwYXNzZXMgdGhlIHJlc3VsdCBvZiB0aGUgcHJldmlvdXMgc3RlcCBhcyB0aGUgaW5wdXQgdG8gdGhlIG5leHQgc3RlcC4KZnVuY3Rpb24gX3VwbG9hZEZpbGVzQ29udGludWUob3V0cHV0SWQpIHsKICBjb25zdCBvdXRwdXRFbGVtZW50ID0gZG9jdW1lbnQuZ2V0RWxlbWVudEJ5SWQob3V0cHV0SWQpOwogIGNvbnN0IHN0ZXBzID0gb3V0cHV0RWxlbWVudC5zdGVwczsKCiAgY29uc3QgbmV4dCA9IHN0ZXBzLm5leHQob3V0cHV0RWxlbWVudC5sYXN0UHJvbWlzZVZhbHVlKTsKICByZXR1cm4gUHJvbWlzZS5yZXNvbHZlKG5leHQudmFsdWUucHJvbWlzZSkudGhlbigodmFsdWUpID0+IHsKICAgIC8vIENhY2hlIHRoZSBsYXN0IHByb21pc2UgdmFsdWUgdG8gbWFrZSBpdCBhdmFpbGFibGUgdG8gdGhlIG5leHQKICAgIC8vIHN0ZXAgb2YgdGhlIGdlbmVyYXRvci4KICAgIG91dHB1dEVsZW1lbnQubGFzdFByb21pc2VWYWx1ZSA9IHZhbHVlOwogICAgcmV0dXJuIG5leHQudmFsdWUucmVzcG9uc2U7CiAgfSk7Cn0KCi8qKgogKiBHZW5lcmF0b3IgZnVuY3Rpb24gd2hpY2ggaXMgY2FsbGVkIGJldHdlZW4gZWFjaCBhc3luYyBzdGVwIG9mIHRoZSB1cGxvYWQKICogcHJvY2Vzcy4KICogQHBhcmFtIHtzdHJpbmd9IGlucHV0SWQgRWxlbWVudCBJRCBvZiB0aGUgaW5wdXQgZmlsZSBwaWNrZXIgZWxlbWVudC4KICogQHBhcmFtIHtzdHJpbmd9IG91dHB1dElkIEVsZW1lbnQgSUQgb2YgdGhlIG91dHB1dCBkaXNwbGF5LgogKiBAcmV0dXJuIHshSXRlcmFibGU8IU9iamVjdD59IEl0ZXJhYmxlIG9mIG5leHQgc3RlcHMuCiAqLwpmdW5jdGlvbiogdXBsb2FkRmlsZXNTdGVwKGlucHV0SWQsIG91dHB1dElkKSB7CiAgY29uc3QgaW5wdXRFbGVtZW50ID0gZG9jdW1lbnQuZ2V0RWxlbWVudEJ5SWQoaW5wdXRJZCk7CiAgaW5wdXRFbGVtZW50LmRpc2FibGVkID0gZmFsc2U7CgogIGNvbnN0IG91dHB1dEVsZW1lbnQgPSBkb2N1bWVudC5nZXRFbGVtZW50QnlJZChvdXRwdXRJZCk7CiAgb3V0cHV0RWxlbWVudC5pbm5lckhUTUwgPSAnJzsKCiAgY29uc3QgcGlja2VkUHJvbWlzZSA9IG5ldyBQcm9taXNlKChyZXNvbHZlKSA9PiB7CiAgICBpbnB1dEVsZW1lbnQuYWRkRXZlbnRMaXN0ZW5lcignY2hhbmdlJywgKGUpID0+IHsKICAgICAgcmVzb2x2ZShlLnRhcmdldC5maWxlcyk7CiAgICB9KTsKICB9KTsKCiAgY29uc3QgY2FuY2VsID0gZG9jdW1lbnQuY3JlYXRlRWxlbWVudCgnYnV0dG9uJyk7CiAgaW5wdXRFbGVtZW50LnBhcmVudEVsZW1lbnQuYXBwZW5kQ2hpbGQoY2FuY2VsKTsKICBjYW5jZWwudGV4dENvbnRlbnQgPSAnQ2FuY2VsIHVwbG9hZCc7CiAgY29uc3QgY2FuY2VsUHJvbWlzZSA9IG5ldyBQcm9taXNlKChyZXNvbHZlKSA9PiB7CiAgICBjYW5jZWwub25jbGljayA9ICgpID0+IHsKICAgICAgcmVzb2x2ZShudWxsKTsKICAgIH07CiAgfSk7CgogIC8vIENhbmNlbCB1cGxvYWQgaWYgdXNlciBoYXNuJ3QgcGlja2VkIGFueXRoaW5nIGluIHRpbWVvdXQuCiAgY29uc3QgdGltZW91dFByb21pc2UgPSBuZXcgUHJvbWlzZSgocmVzb2x2ZSkgPT4gewogICAgc2V0VGltZW91dCgoKSA9PiB7CiAgICAgIHJlc29sdmUobnVsbCk7CiAgICB9LCBGSUxFX0NIQU5HRV9USU1FT1VUX01TKTsKICB9KTsKCiAgLy8gV2FpdCBmb3IgdGhlIHVzZXIgdG8gcGljayB0aGUgZmlsZXMuCiAgY29uc3QgZmlsZXMgPSB5aWVsZCB7CiAgICBwcm9taXNlOiBQcm9taXNlLnJhY2UoW3BpY2tlZFByb21pc2UsIHRpbWVvdXRQcm9taXNlLCBjYW5jZWxQcm9taXNlXSksCiAgICByZXNwb25zZTogewogICAgICBhY3Rpb246ICdzdGFydGluZycsCiAgICB9CiAgfTsKCiAgaWYgKCFmaWxlcykgewogICAgcmV0dXJuIHsKICAgICAgcmVzcG9uc2U6IHsKICAgICAgICBhY3Rpb246ICdjb21wbGV0ZScsCiAgICAgIH0KICAgIH07CiAgfQoKICBjYW5jZWwucmVtb3ZlKCk7CgogIC8vIERpc2FibGUgdGhlIGlucHV0IGVsZW1lbnQgc2luY2UgZnVydGhlciBwaWNrcyBhcmUgbm90IGFsbG93ZWQuCiAgaW5wdXRFbGVtZW50LmRpc2FibGVkID0gdHJ1ZTsKCiAgZm9yIChjb25zdCBmaWxlIG9mIGZpbGVzKSB7CiAgICBjb25zdCBsaSA9IGRvY3VtZW50LmNyZWF0ZUVsZW1lbnQoJ2xpJyk7CiAgICBsaS5hcHBlbmQoc3BhbihmaWxlLm5hbWUsIHtmb250V2VpZ2h0OiAnYm9sZCd9KSk7CiAgICBsaS5hcHBlbmQoc3BhbigKICAgICAgICBgKCR7ZmlsZS50eXBlIHx8ICduL2EnfSkgLSAke2ZpbGUuc2l6ZX0gYnl0ZXMsIGAgKwogICAgICAgIGBsYXN0IG1vZGlmaWVkOiAkewogICAgICAgICAgICBmaWxlLmxhc3RNb2RpZmllZERhdGUgPyBmaWxlLmxhc3RNb2RpZmllZERhdGUudG9Mb2NhbGVEYXRlU3RyaW5nKCkgOgogICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAnbi9hJ30gLSBgKSk7CiAgICBjb25zdCBwZXJjZW50ID0gc3BhbignMCUgZG9uZScpOwogICAgbGkuYXBwZW5kQ2hpbGQocGVyY2VudCk7CgogICAgb3V0cHV0RWxlbWVudC5hcHBlbmRDaGlsZChsaSk7CgogICAgY29uc3QgZmlsZURhdGFQcm9taXNlID0gbmV3IFByb21pc2UoKHJlc29sdmUpID0+IHsKICAgICAgY29uc3QgcmVhZGVyID0gbmV3IEZpbGVSZWFkZXIoKTsKICAgICAgcmVhZGVyLm9ubG9hZCA9IChlKSA9PiB7CiAgICAgICAgcmVzb2x2ZShlLnRhcmdldC5yZXN1bHQpOwogICAgICB9OwogICAgICByZWFkZXIucmVhZEFzQXJyYXlCdWZmZXIoZmlsZSk7CiAgICB9KTsKICAgIC8vIFdhaXQgZm9yIHRoZSBkYXRhIHRvIGJlIHJlYWR5LgogICAgbGV0IGZpbGVEYXRhID0geWllbGQgewogICAgICBwcm9taXNlOiBmaWxlRGF0YVByb21pc2UsCiAgICAgIHJlc3BvbnNlOiB7CiAgICAgICAgYWN0aW9uOiAnY29udGludWUnLAogICAgICB9CiAgICB9OwoKICAgIC8vIFVzZSBhIGNodW5rZWQgc2VuZGluZyB0byBhdm9pZCBtZXNzYWdlIHNpemUgbGltaXRzLiBTZWUgYi82MjExNTY2MC4KICAgIGxldCBwb3NpdGlvbiA9IDA7CiAgICB3aGlsZSAocG9zaXRpb24gPCBmaWxlRGF0YS5ieXRlTGVuZ3RoKSB7CiAgICAgIGNvbnN0IGxlbmd0aCA9IE1hdGgubWluKGZpbGVEYXRhLmJ5dGVMZW5ndGggLSBwb3NpdGlvbiwgTUFYX1BBWUxPQURfU0laRSk7CiAgICAgIGNvbnN0IGNodW5rID0gbmV3IFVpbnQ4QXJyYXkoZmlsZURhdGEsIHBvc2l0aW9uLCBsZW5ndGgpOwogICAgICBwb3NpdGlvbiArPSBsZW5ndGg7CgogICAgICBjb25zdCBiYXNlNjQgPSBidG9hKFN0cmluZy5mcm9tQ2hhckNvZGUuYXBwbHkobnVsbCwgY2h1bmspKTsKICAgICAgeWllbGQgewogICAgICAgIHJlc3BvbnNlOiB7CiAgICAgICAgICBhY3Rpb246ICdhcHBlbmQnLAogICAgICAgICAgZmlsZTogZmlsZS5uYW1lLAogICAgICAgICAgZGF0YTogYmFzZTY0LAogICAgICAgIH0sCiAgICAgIH07CiAgICAgIHBlcmNlbnQudGV4dENvbnRlbnQgPQogICAgICAgICAgYCR7TWF0aC5yb3VuZCgocG9zaXRpb24gLyBmaWxlRGF0YS5ieXRlTGVuZ3RoKSAqIDEwMCl9JSBkb25lYDsKICAgIH0KICB9CgogIC8vIEFsbCBkb25lLgogIHlpZWxkIHsKICAgIHJlc3BvbnNlOiB7CiAgICAgIGFjdGlvbjogJ2NvbXBsZXRlJywKICAgIH0KICB9Owp9CgpzY29wZS5nb29nbGUgPSBzY29wZS5nb29nbGUgfHwge307CnNjb3BlLmdvb2dsZS5jb2xhYiA9IHNjb3BlLmdvb2dsZS5jb2xhYiB8fCB7fTsKc2NvcGUuZ29vZ2xlLmNvbGFiLl9maWxlcyA9IHsKICBfdXBsb2FkRmlsZXMsCiAgX3VwbG9hZEZpbGVzQ29udGludWUsCn07Cn0pKHNlbGYpOwo=","ok":true,"headers":[["content-type","application/javascript"]],"status":200,"status_text":""}},"base_uri":"https://localhost:8080/","height":193},"outputId":"9a21cd78-907e-4a53-dbb1-9eefdc2e9d8e","executionInfo":{"status":"ok","timestamp":1591804740279,"user_tz":240,"elapsed":38056,"user":{"displayName":"Ravi Vattikuti","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjyKWq_9kMPQBBkVSgJCrYyroBcPqR0yjOIIrSQjcE=s64","userId":"12788632665998596893"}}},"source":["from google.colab import files\n","\n","uploaded = files.upload()\n","\n","for fn in uploaded.keys():\n","  print('User uploaded file \"{name}\" with length {length} bytes'.format(name=fn, length=len(uploaded[fn])))"],"execution_count":2,"outputs":[{"output_type":"display_data","data":{"text/html":["\n","     <input type=\"file\" id=\"files-12148c5e-a443-42bf-a230-07c0cd5d3a34\" name=\"files[]\" multiple disabled />\n","     <output id=\"result-12148c5e-a443-42bf-a230-07c0cd5d3a34\">\n","      Upload widget is only available when the cell has been executed in the\n","      current browser session. Please rerun this cell to enable.\n","      </output>\n","      <script src=\"/nbextensions/google.colab/files.js\"></script> "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["Saving sample_submission.csv to sample_submission.csv\n","Saving test.csv to test.csv\n","Saving train.csv to train.csv\n","User uploaded file \"sample_submission.csv\" with length 31939 bytes\n","User uploaded file \"test.csv\" with length 451405 bytes\n","User uploaded file \"train.csv\" with length 460676 bytes\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"bQ3uvDktkJke","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"e8c0604d-c6d5-42e7-ce05-67e971a297ac","executionInfo":{"status":"ok","timestamp":1591804740285,"user_tz":240,"elapsed":38006,"user":{"displayName":"Ravi Vattikuti","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjyKWq_9kMPQBBkVSgJCrYyroBcPqR0yjOIIrSQjcE=s64","userId":"12788632665998596893"}}},"source":["#load data\n","train = pd.read_csv('train.csv')\n","test = pd.read_csv('test.csv')\n","train_test = [train,test]\n","#combine train and test data\n","train_test_data = pd.concat([train,test], axis = 0)\n","\n","train_test_data.shape\n"],"execution_count":3,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(2919, 81)"]},"metadata":{"tags":[]},"execution_count":3}]},{"cell_type":"code","metadata":{"id":"rmFe54kLpKRd","colab_type":"code","colab":{}},"source":["#Drop the features which contains more than 70% missing values\n","for column in train_test_data.columns:\n","  if train_test_data[column].isnull().sum() > 0.7 * len(train_test_data):\n","    train_test_data.drop(column, axis = 1, inplace= True)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"cdGf8lamqurn","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"88dd74b4-8ac8-46fc-9fd8-7906f7f4e8aa","executionInfo":{"status":"ok","timestamp":1591804740296,"user_tz":240,"elapsed":37918,"user":{"displayName":"Ravi Vattikuti","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjyKWq_9kMPQBBkVSgJCrYyroBcPqR0yjOIIrSQjcE=s64","userId":"12788632665998596893"}}},"source":["train_test_data.shape"],"execution_count":5,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(2919, 77)"]},"metadata":{"tags":[]},"execution_count":5}]},{"cell_type":"code","metadata":{"id":"aouKgubsqG3R","colab_type":"code","colab":{}},"source":["# Fill the missing values with mode of the column for strings and mean for numerical data\n","for column in train_test_data.columns:\n","  if train_test_data[column].dtypes == 'O':\n","    train_test_data[column] = train_test_data[column].fillna(train_test_data[column].mode()[0])\n","  else:\n","    train_test_data[column] = train_test_data[column].fillna(train_test_data[column].mean())  "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"vT2rW_T9ozJv","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":439},"outputId":"d39ac2f1-c1b3-4700-8446-12cced77f961","executionInfo":{"status":"ok","timestamp":1591804740662,"user_tz":240,"elapsed":38175,"user":{"displayName":"Ravi Vattikuti","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjyKWq_9kMPQBBkVSgJCrYyroBcPqR0yjOIIrSQjcE=s64","userId":"12788632665998596893"}}},"source":["train_test_data"],"execution_count":7,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Id</th>\n","      <th>MSSubClass</th>\n","      <th>MSZoning</th>\n","      <th>LotFrontage</th>\n","      <th>LotArea</th>\n","      <th>Street</th>\n","      <th>LotShape</th>\n","      <th>LandContour</th>\n","      <th>Utilities</th>\n","      <th>LotConfig</th>\n","      <th>LandSlope</th>\n","      <th>Neighborhood</th>\n","      <th>Condition1</th>\n","      <th>Condition2</th>\n","      <th>BldgType</th>\n","      <th>HouseStyle</th>\n","      <th>OverallQual</th>\n","      <th>OverallCond</th>\n","      <th>YearBuilt</th>\n","      <th>YearRemodAdd</th>\n","      <th>RoofStyle</th>\n","      <th>RoofMatl</th>\n","      <th>Exterior1st</th>\n","      <th>Exterior2nd</th>\n","      <th>MasVnrType</th>\n","      <th>MasVnrArea</th>\n","      <th>ExterQual</th>\n","      <th>ExterCond</th>\n","      <th>Foundation</th>\n","      <th>BsmtQual</th>\n","      <th>BsmtCond</th>\n","      <th>BsmtExposure</th>\n","      <th>BsmtFinType1</th>\n","      <th>BsmtFinSF1</th>\n","      <th>BsmtFinType2</th>\n","      <th>BsmtFinSF2</th>\n","      <th>BsmtUnfSF</th>\n","      <th>TotalBsmtSF</th>\n","      <th>Heating</th>\n","      <th>HeatingQC</th>\n","      <th>CentralAir</th>\n","      <th>Electrical</th>\n","      <th>1stFlrSF</th>\n","      <th>2ndFlrSF</th>\n","      <th>LowQualFinSF</th>\n","      <th>GrLivArea</th>\n","      <th>BsmtFullBath</th>\n","      <th>BsmtHalfBath</th>\n","      <th>FullBath</th>\n","      <th>HalfBath</th>\n","      <th>BedroomAbvGr</th>\n","      <th>KitchenAbvGr</th>\n","      <th>KitchenQual</th>\n","      <th>TotRmsAbvGrd</th>\n","      <th>Functional</th>\n","      <th>Fireplaces</th>\n","      <th>FireplaceQu</th>\n","      <th>GarageType</th>\n","      <th>GarageYrBlt</th>\n","      <th>GarageFinish</th>\n","      <th>GarageCars</th>\n","      <th>GarageArea</th>\n","      <th>GarageQual</th>\n","      <th>GarageCond</th>\n","      <th>PavedDrive</th>\n","      <th>WoodDeckSF</th>\n","      <th>OpenPorchSF</th>\n","      <th>EnclosedPorch</th>\n","      <th>3SsnPorch</th>\n","      <th>ScreenPorch</th>\n","      <th>PoolArea</th>\n","      <th>MiscVal</th>\n","      <th>MoSold</th>\n","      <th>YrSold</th>\n","      <th>SaleType</th>\n","      <th>SaleCondition</th>\n","      <th>SalePrice</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1</td>\n","      <td>60</td>\n","      <td>RL</td>\n","      <td>65.0</td>\n","      <td>8450</td>\n","      <td>Pave</td>\n","      <td>Reg</td>\n","      <td>Lvl</td>\n","      <td>AllPub</td>\n","      <td>Inside</td>\n","      <td>Gtl</td>\n","      <td>CollgCr</td>\n","      <td>Norm</td>\n","      <td>Norm</td>\n","      <td>1Fam</td>\n","      <td>2Story</td>\n","      <td>7</td>\n","      <td>5</td>\n","      <td>2003</td>\n","      <td>2003</td>\n","      <td>Gable</td>\n","      <td>CompShg</td>\n","      <td>VinylSd</td>\n","      <td>VinylSd</td>\n","      <td>BrkFace</td>\n","      <td>196.0</td>\n","      <td>Gd</td>\n","      <td>TA</td>\n","      <td>PConc</td>\n","      <td>Gd</td>\n","      <td>TA</td>\n","      <td>No</td>\n","      <td>GLQ</td>\n","      <td>706.0</td>\n","      <td>Unf</td>\n","      <td>0.0</td>\n","      <td>150.0</td>\n","      <td>856.0</td>\n","      <td>GasA</td>\n","      <td>Ex</td>\n","      <td>Y</td>\n","      <td>SBrkr</td>\n","      <td>856</td>\n","      <td>854</td>\n","      <td>0</td>\n","      <td>1710</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>2</td>\n","      <td>1</td>\n","      <td>3</td>\n","      <td>1</td>\n","      <td>Gd</td>\n","      <td>8</td>\n","      <td>Typ</td>\n","      <td>0</td>\n","      <td>Gd</td>\n","      <td>Attchd</td>\n","      <td>2003.000000</td>\n","      <td>RFn</td>\n","      <td>2.0</td>\n","      <td>548.0</td>\n","      <td>TA</td>\n","      <td>TA</td>\n","      <td>Y</td>\n","      <td>0</td>\n","      <td>61</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>2008</td>\n","      <td>WD</td>\n","      <td>Normal</td>\n","      <td>208500.00000</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>2</td>\n","      <td>20</td>\n","      <td>RL</td>\n","      <td>80.0</td>\n","      <td>9600</td>\n","      <td>Pave</td>\n","      <td>Reg</td>\n","      <td>Lvl</td>\n","      <td>AllPub</td>\n","      <td>FR2</td>\n","      <td>Gtl</td>\n","      <td>Veenker</td>\n","      <td>Feedr</td>\n","      <td>Norm</td>\n","      <td>1Fam</td>\n","      <td>1Story</td>\n","      <td>6</td>\n","      <td>8</td>\n","      <td>1976</td>\n","      <td>1976</td>\n","      <td>Gable</td>\n","      <td>CompShg</td>\n","      <td>MetalSd</td>\n","      <td>MetalSd</td>\n","      <td>None</td>\n","      <td>0.0</td>\n","      <td>TA</td>\n","      <td>TA</td>\n","      <td>CBlock</td>\n","      <td>Gd</td>\n","      <td>TA</td>\n","      <td>Gd</td>\n","      <td>ALQ</td>\n","      <td>978.0</td>\n","      <td>Unf</td>\n","      <td>0.0</td>\n","      <td>284.0</td>\n","      <td>1262.0</td>\n","      <td>GasA</td>\n","      <td>Ex</td>\n","      <td>Y</td>\n","      <td>SBrkr</td>\n","      <td>1262</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1262</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>3</td>\n","      <td>1</td>\n","      <td>TA</td>\n","      <td>6</td>\n","      <td>Typ</td>\n","      <td>1</td>\n","      <td>TA</td>\n","      <td>Attchd</td>\n","      <td>1976.000000</td>\n","      <td>RFn</td>\n","      <td>2.0</td>\n","      <td>460.0</td>\n","      <td>TA</td>\n","      <td>TA</td>\n","      <td>Y</td>\n","      <td>298</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>5</td>\n","      <td>2007</td>\n","      <td>WD</td>\n","      <td>Normal</td>\n","      <td>181500.00000</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>3</td>\n","      <td>60</td>\n","      <td>RL</td>\n","      <td>68.0</td>\n","      <td>11250</td>\n","      <td>Pave</td>\n","      <td>IR1</td>\n","      <td>Lvl</td>\n","      <td>AllPub</td>\n","      <td>Inside</td>\n","      <td>Gtl</td>\n","      <td>CollgCr</td>\n","      <td>Norm</td>\n","      <td>Norm</td>\n","      <td>1Fam</td>\n","      <td>2Story</td>\n","      <td>7</td>\n","      <td>5</td>\n","      <td>2001</td>\n","      <td>2002</td>\n","      <td>Gable</td>\n","      <td>CompShg</td>\n","      <td>VinylSd</td>\n","      <td>VinylSd</td>\n","      <td>BrkFace</td>\n","      <td>162.0</td>\n","      <td>Gd</td>\n","      <td>TA</td>\n","      <td>PConc</td>\n","      <td>Gd</td>\n","      <td>TA</td>\n","      <td>Mn</td>\n","      <td>GLQ</td>\n","      <td>486.0</td>\n","      <td>Unf</td>\n","      <td>0.0</td>\n","      <td>434.0</td>\n","      <td>920.0</td>\n","      <td>GasA</td>\n","      <td>Ex</td>\n","      <td>Y</td>\n","      <td>SBrkr</td>\n","      <td>920</td>\n","      <td>866</td>\n","      <td>0</td>\n","      <td>1786</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>2</td>\n","      <td>1</td>\n","      <td>3</td>\n","      <td>1</td>\n","      <td>Gd</td>\n","      <td>6</td>\n","      <td>Typ</td>\n","      <td>1</td>\n","      <td>TA</td>\n","      <td>Attchd</td>\n","      <td>2001.000000</td>\n","      <td>RFn</td>\n","      <td>2.0</td>\n","      <td>608.0</td>\n","      <td>TA</td>\n","      <td>TA</td>\n","      <td>Y</td>\n","      <td>0</td>\n","      <td>42</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>9</td>\n","      <td>2008</td>\n","      <td>WD</td>\n","      <td>Normal</td>\n","      <td>223500.00000</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>4</td>\n","      <td>70</td>\n","      <td>RL</td>\n","      <td>60.0</td>\n","      <td>9550</td>\n","      <td>Pave</td>\n","      <td>IR1</td>\n","      <td>Lvl</td>\n","      <td>AllPub</td>\n","      <td>Corner</td>\n","      <td>Gtl</td>\n","      <td>Crawfor</td>\n","      <td>Norm</td>\n","      <td>Norm</td>\n","      <td>1Fam</td>\n","      <td>2Story</td>\n","      <td>7</td>\n","      <td>5</td>\n","      <td>1915</td>\n","      <td>1970</td>\n","      <td>Gable</td>\n","      <td>CompShg</td>\n","      <td>Wd Sdng</td>\n","      <td>Wd Shng</td>\n","      <td>None</td>\n","      <td>0.0</td>\n","      <td>TA</td>\n","      <td>TA</td>\n","      <td>BrkTil</td>\n","      <td>TA</td>\n","      <td>Gd</td>\n","      <td>No</td>\n","      <td>ALQ</td>\n","      <td>216.0</td>\n","      <td>Unf</td>\n","      <td>0.0</td>\n","      <td>540.0</td>\n","      <td>756.0</td>\n","      <td>GasA</td>\n","      <td>Gd</td>\n","      <td>Y</td>\n","      <td>SBrkr</td>\n","      <td>961</td>\n","      <td>756</td>\n","      <td>0</td>\n","      <td>1717</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>3</td>\n","      <td>1</td>\n","      <td>Gd</td>\n","      <td>7</td>\n","      <td>Typ</td>\n","      <td>1</td>\n","      <td>Gd</td>\n","      <td>Detchd</td>\n","      <td>1998.000000</td>\n","      <td>Unf</td>\n","      <td>3.0</td>\n","      <td>642.0</td>\n","      <td>TA</td>\n","      <td>TA</td>\n","      <td>Y</td>\n","      <td>0</td>\n","      <td>35</td>\n","      <td>272</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>2006</td>\n","      <td>WD</td>\n","      <td>Abnorml</td>\n","      <td>140000.00000</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>5</td>\n","      <td>60</td>\n","      <td>RL</td>\n","      <td>84.0</td>\n","      <td>14260</td>\n","      <td>Pave</td>\n","      <td>IR1</td>\n","      <td>Lvl</td>\n","      <td>AllPub</td>\n","      <td>FR2</td>\n","      <td>Gtl</td>\n","      <td>NoRidge</td>\n","      <td>Norm</td>\n","      <td>Norm</td>\n","      <td>1Fam</td>\n","      <td>2Story</td>\n","      <td>8</td>\n","      <td>5</td>\n","      <td>2000</td>\n","      <td>2000</td>\n","      <td>Gable</td>\n","      <td>CompShg</td>\n","      <td>VinylSd</td>\n","      <td>VinylSd</td>\n","      <td>BrkFace</td>\n","      <td>350.0</td>\n","      <td>Gd</td>\n","      <td>TA</td>\n","      <td>PConc</td>\n","      <td>Gd</td>\n","      <td>TA</td>\n","      <td>Av</td>\n","      <td>GLQ</td>\n","      <td>655.0</td>\n","      <td>Unf</td>\n","      <td>0.0</td>\n","      <td>490.0</td>\n","      <td>1145.0</td>\n","      <td>GasA</td>\n","      <td>Ex</td>\n","      <td>Y</td>\n","      <td>SBrkr</td>\n","      <td>1145</td>\n","      <td>1053</td>\n","      <td>0</td>\n","      <td>2198</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>2</td>\n","      <td>1</td>\n","      <td>4</td>\n","      <td>1</td>\n","      <td>Gd</td>\n","      <td>9</td>\n","      <td>Typ</td>\n","      <td>1</td>\n","      <td>TA</td>\n","      <td>Attchd</td>\n","      <td>2000.000000</td>\n","      <td>RFn</td>\n","      <td>3.0</td>\n","      <td>836.0</td>\n","      <td>TA</td>\n","      <td>TA</td>\n","      <td>Y</td>\n","      <td>192</td>\n","      <td>84</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>12</td>\n","      <td>2008</td>\n","      <td>WD</td>\n","      <td>Normal</td>\n","      <td>250000.00000</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>1454</th>\n","      <td>2915</td>\n","      <td>160</td>\n","      <td>RM</td>\n","      <td>21.0</td>\n","      <td>1936</td>\n","      <td>Pave</td>\n","      <td>Reg</td>\n","      <td>Lvl</td>\n","      <td>AllPub</td>\n","      <td>Inside</td>\n","      <td>Gtl</td>\n","      <td>MeadowV</td>\n","      <td>Norm</td>\n","      <td>Norm</td>\n","      <td>Twnhs</td>\n","      <td>2Story</td>\n","      <td>4</td>\n","      <td>7</td>\n","      <td>1970</td>\n","      <td>1970</td>\n","      <td>Gable</td>\n","      <td>CompShg</td>\n","      <td>CemntBd</td>\n","      <td>CmentBd</td>\n","      <td>None</td>\n","      <td>0.0</td>\n","      <td>TA</td>\n","      <td>TA</td>\n","      <td>CBlock</td>\n","      <td>TA</td>\n","      <td>TA</td>\n","      <td>No</td>\n","      <td>Unf</td>\n","      <td>0.0</td>\n","      <td>Unf</td>\n","      <td>0.0</td>\n","      <td>546.0</td>\n","      <td>546.0</td>\n","      <td>GasA</td>\n","      <td>Gd</td>\n","      <td>Y</td>\n","      <td>SBrkr</td>\n","      <td>546</td>\n","      <td>546</td>\n","      <td>0</td>\n","      <td>1092</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>3</td>\n","      <td>1</td>\n","      <td>TA</td>\n","      <td>5</td>\n","      <td>Typ</td>\n","      <td>0</td>\n","      <td>Gd</td>\n","      <td>Attchd</td>\n","      <td>1978.113406</td>\n","      <td>Unf</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>TA</td>\n","      <td>TA</td>\n","      <td>Y</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>6</td>\n","      <td>2006</td>\n","      <td>WD</td>\n","      <td>Normal</td>\n","      <td>180921.19589</td>\n","    </tr>\n","    <tr>\n","      <th>1455</th>\n","      <td>2916</td>\n","      <td>160</td>\n","      <td>RM</td>\n","      <td>21.0</td>\n","      <td>1894</td>\n","      <td>Pave</td>\n","      <td>Reg</td>\n","      <td>Lvl</td>\n","      <td>AllPub</td>\n","      <td>Inside</td>\n","      <td>Gtl</td>\n","      <td>MeadowV</td>\n","      <td>Norm</td>\n","      <td>Norm</td>\n","      <td>TwnhsE</td>\n","      <td>2Story</td>\n","      <td>4</td>\n","      <td>5</td>\n","      <td>1970</td>\n","      <td>1970</td>\n","      <td>Gable</td>\n","      <td>CompShg</td>\n","      <td>CemntBd</td>\n","      <td>CmentBd</td>\n","      <td>None</td>\n","      <td>0.0</td>\n","      <td>TA</td>\n","      <td>TA</td>\n","      <td>CBlock</td>\n","      <td>TA</td>\n","      <td>TA</td>\n","      <td>No</td>\n","      <td>Rec</td>\n","      <td>252.0</td>\n","      <td>Unf</td>\n","      <td>0.0</td>\n","      <td>294.0</td>\n","      <td>546.0</td>\n","      <td>GasA</td>\n","      <td>TA</td>\n","      <td>Y</td>\n","      <td>SBrkr</td>\n","      <td>546</td>\n","      <td>546</td>\n","      <td>0</td>\n","      <td>1092</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>3</td>\n","      <td>1</td>\n","      <td>TA</td>\n","      <td>6</td>\n","      <td>Typ</td>\n","      <td>0</td>\n","      <td>Gd</td>\n","      <td>CarPort</td>\n","      <td>1970.000000</td>\n","      <td>Unf</td>\n","      <td>1.0</td>\n","      <td>286.0</td>\n","      <td>TA</td>\n","      <td>TA</td>\n","      <td>Y</td>\n","      <td>0</td>\n","      <td>24</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>4</td>\n","      <td>2006</td>\n","      <td>WD</td>\n","      <td>Abnorml</td>\n","      <td>180921.19589</td>\n","    </tr>\n","    <tr>\n","      <th>1456</th>\n","      <td>2917</td>\n","      <td>20</td>\n","      <td>RL</td>\n","      <td>160.0</td>\n","      <td>20000</td>\n","      <td>Pave</td>\n","      <td>Reg</td>\n","      <td>Lvl</td>\n","      <td>AllPub</td>\n","      <td>Inside</td>\n","      <td>Gtl</td>\n","      <td>Mitchel</td>\n","      <td>Norm</td>\n","      <td>Norm</td>\n","      <td>1Fam</td>\n","      <td>1Story</td>\n","      <td>5</td>\n","      <td>7</td>\n","      <td>1960</td>\n","      <td>1996</td>\n","      <td>Gable</td>\n","      <td>CompShg</td>\n","      <td>VinylSd</td>\n","      <td>VinylSd</td>\n","      <td>None</td>\n","      <td>0.0</td>\n","      <td>TA</td>\n","      <td>TA</td>\n","      <td>CBlock</td>\n","      <td>TA</td>\n","      <td>TA</td>\n","      <td>No</td>\n","      <td>ALQ</td>\n","      <td>1224.0</td>\n","      <td>Unf</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1224.0</td>\n","      <td>GasA</td>\n","      <td>Ex</td>\n","      <td>Y</td>\n","      <td>SBrkr</td>\n","      <td>1224</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1224</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>4</td>\n","      <td>1</td>\n","      <td>TA</td>\n","      <td>7</td>\n","      <td>Typ</td>\n","      <td>1</td>\n","      <td>TA</td>\n","      <td>Detchd</td>\n","      <td>1960.000000</td>\n","      <td>Unf</td>\n","      <td>2.0</td>\n","      <td>576.0</td>\n","      <td>TA</td>\n","      <td>TA</td>\n","      <td>Y</td>\n","      <td>474</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>9</td>\n","      <td>2006</td>\n","      <td>WD</td>\n","      <td>Abnorml</td>\n","      <td>180921.19589</td>\n","    </tr>\n","    <tr>\n","      <th>1457</th>\n","      <td>2918</td>\n","      <td>85</td>\n","      <td>RL</td>\n","      <td>62.0</td>\n","      <td>10441</td>\n","      <td>Pave</td>\n","      <td>Reg</td>\n","      <td>Lvl</td>\n","      <td>AllPub</td>\n","      <td>Inside</td>\n","      <td>Gtl</td>\n","      <td>Mitchel</td>\n","      <td>Norm</td>\n","      <td>Norm</td>\n","      <td>1Fam</td>\n","      <td>SFoyer</td>\n","      <td>5</td>\n","      <td>5</td>\n","      <td>1992</td>\n","      <td>1992</td>\n","      <td>Gable</td>\n","      <td>CompShg</td>\n","      <td>HdBoard</td>\n","      <td>Wd Shng</td>\n","      <td>None</td>\n","      <td>0.0</td>\n","      <td>TA</td>\n","      <td>TA</td>\n","      <td>PConc</td>\n","      <td>Gd</td>\n","      <td>TA</td>\n","      <td>Av</td>\n","      <td>GLQ</td>\n","      <td>337.0</td>\n","      <td>Unf</td>\n","      <td>0.0</td>\n","      <td>575.0</td>\n","      <td>912.0</td>\n","      <td>GasA</td>\n","      <td>TA</td>\n","      <td>Y</td>\n","      <td>SBrkr</td>\n","      <td>970</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>970</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>3</td>\n","      <td>1</td>\n","      <td>TA</td>\n","      <td>6</td>\n","      <td>Typ</td>\n","      <td>0</td>\n","      <td>Gd</td>\n","      <td>Attchd</td>\n","      <td>1978.113406</td>\n","      <td>Unf</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>TA</td>\n","      <td>TA</td>\n","      <td>Y</td>\n","      <td>80</td>\n","      <td>32</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>700</td>\n","      <td>7</td>\n","      <td>2006</td>\n","      <td>WD</td>\n","      <td>Normal</td>\n","      <td>180921.19589</td>\n","    </tr>\n","    <tr>\n","      <th>1458</th>\n","      <td>2919</td>\n","      <td>60</td>\n","      <td>RL</td>\n","      <td>74.0</td>\n","      <td>9627</td>\n","      <td>Pave</td>\n","      <td>Reg</td>\n","      <td>Lvl</td>\n","      <td>AllPub</td>\n","      <td>Inside</td>\n","      <td>Mod</td>\n","      <td>Mitchel</td>\n","      <td>Norm</td>\n","      <td>Norm</td>\n","      <td>1Fam</td>\n","      <td>2Story</td>\n","      <td>7</td>\n","      <td>5</td>\n","      <td>1993</td>\n","      <td>1994</td>\n","      <td>Gable</td>\n","      <td>CompShg</td>\n","      <td>HdBoard</td>\n","      <td>HdBoard</td>\n","      <td>BrkFace</td>\n","      <td>94.0</td>\n","      <td>TA</td>\n","      <td>TA</td>\n","      <td>PConc</td>\n","      <td>Gd</td>\n","      <td>TA</td>\n","      <td>Av</td>\n","      <td>LwQ</td>\n","      <td>758.0</td>\n","      <td>Unf</td>\n","      <td>0.0</td>\n","      <td>238.0</td>\n","      <td>996.0</td>\n","      <td>GasA</td>\n","      <td>Ex</td>\n","      <td>Y</td>\n","      <td>SBrkr</td>\n","      <td>996</td>\n","      <td>1004</td>\n","      <td>0</td>\n","      <td>2000</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>2</td>\n","      <td>1</td>\n","      <td>3</td>\n","      <td>1</td>\n","      <td>TA</td>\n","      <td>9</td>\n","      <td>Typ</td>\n","      <td>1</td>\n","      <td>TA</td>\n","      <td>Attchd</td>\n","      <td>1993.000000</td>\n","      <td>Fin</td>\n","      <td>3.0</td>\n","      <td>650.0</td>\n","      <td>TA</td>\n","      <td>TA</td>\n","      <td>Y</td>\n","      <td>190</td>\n","      <td>48</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>11</td>\n","      <td>2006</td>\n","      <td>WD</td>\n","      <td>Normal</td>\n","      <td>180921.19589</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>2919 rows × 77 columns</p>\n","</div>"],"text/plain":["        Id  MSSubClass MSZoning  ...  SaleType  SaleCondition     SalePrice\n","0        1          60       RL  ...        WD         Normal  208500.00000\n","1        2          20       RL  ...        WD         Normal  181500.00000\n","2        3          60       RL  ...        WD         Normal  223500.00000\n","3        4          70       RL  ...        WD        Abnorml  140000.00000\n","4        5          60       RL  ...        WD         Normal  250000.00000\n","...    ...         ...      ...  ...       ...            ...           ...\n","1454  2915         160       RM  ...        WD         Normal  180921.19589\n","1455  2916         160       RM  ...        WD        Abnorml  180921.19589\n","1456  2917          20       RL  ...        WD        Abnorml  180921.19589\n","1457  2918          85       RL  ...        WD         Normal  180921.19589\n","1458  2919          60       RL  ...        WD         Normal  180921.19589\n","\n","[2919 rows x 77 columns]"]},"metadata":{"tags":[]},"execution_count":7}]},{"cell_type":"code","metadata":{"id":"k1dQO20oX56e","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"5a42aecb-2b2b-4dd4-c7eb-1da46453c36c","executionInfo":{"status":"ok","timestamp":1591804740669,"user_tz":240,"elapsed":38130,"user":{"displayName":"Ravi Vattikuti","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjyKWq_9kMPQBBkVSgJCrYyroBcPqR0yjOIIrSQjcE=s64","userId":"12788632665998596893"}}},"source":["# convert categorical data into numerical values\n","X_data = pd.get_dummies(train_test_data, prefix_sep='_', drop_first=True)\n","X_data.shape\n"],"execution_count":8,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(2919, 238)"]},"metadata":{"tags":[]},"execution_count":8}]},{"cell_type":"code","metadata":{"id":"yoadOvU5cU43","colab_type":"code","colab":{}},"source":["# Remove any duplicates\n","X_data = X_data.loc[:,~X_data.columns.duplicated()]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"WKmru-pFdFqw","colab_type":"code","colab":{}},"source":["Y = X_data['SalePrice']\n","Y = Y[:1460,]\n","X = X_data.drop(['SalePrice'], axis = 1)\n","\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"mezF0PsOiY-m","colab_type":"code","colab":{}},"source":["X = preprocessing.normalize(X)\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"J9sRd8PrvpVK","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"07d1f1f6-b91c-4ff7-8e05-0820be4784d8","executionInfo":{"status":"ok","timestamp":1591804740689,"user_tz":240,"elapsed":37900,"user":{"displayName":"Ravi Vattikuti","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjyKWq_9kMPQBBkVSgJCrYyroBcPqR0yjOIIrSQjcE=s64","userId":"12788632665998596893"}}},"source":["X_train = X[:1460,:]\n","X_test = X[1460:,:]\n","X_train.shape"],"execution_count":12,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(1460, 237)"]},"metadata":{"tags":[]},"execution_count":12}]},{"cell_type":"code","metadata":{"id":"Juy57XrUvqY3","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":173},"outputId":"95a62f4e-d1f0-415c-cb53-d36244ad7809","executionInfo":{"status":"ok","timestamp":1591804742581,"user_tz":240,"elapsed":39738,"user":{"displayName":"Ravi Vattikuti","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjyKWq_9kMPQBBkVSgJCrYyroBcPqR0yjOIIrSQjcE=s64","userId":"12788632665998596893"}}},"source":["LR = linear_model.LinearRegression()\n","XGB = xgboost.XGBRegressor()\n","LR.fit(X_train,Y)\n","XGB.fit(X_train,Y)"],"execution_count":13,"outputs":[{"output_type":"stream","text":["[15:59:01] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n","             colsample_bynode=1, colsample_bytree=1, gamma=0,\n","             importance_type='gain', learning_rate=0.1, max_delta_step=0,\n","             max_depth=3, min_child_weight=1, missing=None, n_estimators=100,\n","             n_jobs=1, nthread=None, objective='reg:linear', random_state=0,\n","             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n","             silent=None, subsample=1, verbosity=1)"]},"metadata":{"tags":[]},"execution_count":13}]},{"cell_type":"code","metadata":{"id":"HzalpYUpv-ZQ","colab_type":"code","colab":{}},"source":["pred_LR = LR.predict(X_test)\n","pred_XGB = XGB.predict(X_test)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"aIfp7dGU0xmT","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":51},"outputId":"925c204d-9736-460e-fa87-ba06670ba003","executionInfo":{"status":"ok","timestamp":1591804742590,"user_tz":240,"elapsed":39633,"user":{"displayName":"Ravi Vattikuti","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjyKWq_9kMPQBBkVSgJCrYyroBcPqR0yjOIIrSQjcE=s64","userId":"12788632665998596893"}}},"source":["pred_XGB"],"execution_count":15,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([119969.17, 151100.1 , 176036.8 , ..., 154342.58, 130446.78,\n","       194403.16], dtype=float32)"]},"metadata":{"tags":[]},"execution_count":15}]},{"cell_type":"code","metadata":{"id":"2ZXbtLy50ypJ","colab_type":"code","colab":{}},"source":["prediction_LR = pd.DataFrame(pred_LR)\n","prediction_XGB = pd.DataFrame(pred_XGB)\n","submission_LR = pd.concat([pd.read_csv('sample_submission.csv')['Id'], prediction_LR], axis = 1)\n","submission_LR.columns = ['Id', 'SalePrice']\n","submission_XGB = pd.concat([pd.read_csv('sample_submission.csv')['Id'], prediction_XGB], axis = 1)\n","submission_XGB.columns = ['Id', 'SalePrice']\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"0JJePgOl20ts","colab_type":"code","colab":{}},"source":["submission_LR.to_csv('Submission_LR.csv', index = False)\n","submission_XGB.to_csv('Submission_XGB.csv', index = False)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ax4WckAA22mK","colab_type":"code","colab":{}},"source":["## Hyper Parameter Optimization\n","\n","\n","n_estimators = [100, 500, 900, 1100, 1500]\n","max_depth = [2, 3, 5, 10, 15]\n","booster=['gbtree','gblinear']\n","learning_rate=[0.05,0.1,0.15,0.20]\n","min_child_weight=[1,2,3,4]\n","base_score=[0.25,0.5,0.75,1]\n","\n","# Define the grid of hyperparameters to search\n","hyperparameter_grid = {\n","    'n_estimators': n_estimators,\n","    'max_depth':max_depth,\n","    'learning_rate':learning_rate,\n","    'min_child_weight':min_child_weight,\n","    'booster':booster,\n","    'base_score':base_score\n","    }"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"UDaMGTYYruMO","colab_type":"code","colab":{}},"source":["# Set up the random search with 4-fold cross validation\n","random_cv = RandomizedSearchCV(estimator=XGB,\n","            param_distributions=hyperparameter_grid,\n","            cv=5, n_iter=50,\n","            scoring = 'neg_mean_absolute_error',n_jobs = 4,\n","            verbose = 5, \n","            return_train_score = True,\n","            random_state=42)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"NhvvUDHur-_i","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":547},"outputId":"60c7f160-8b21-408d-b7e1-3eed4d429a4a","executionInfo":{"status":"ok","timestamp":1591806282731,"user_tz":240,"elapsed":1579518,"user":{"displayName":"Ravi Vattikuti","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjyKWq_9kMPQBBkVSgJCrYyroBcPqR0yjOIIrSQjcE=s64","userId":"12788632665998596893"}}},"source":["random_cv.fit(X_train,Y)"],"execution_count":20,"outputs":[{"output_type":"stream","text":["Fitting 5 folds for each of 50 candidates, totalling 250 fits\n"],"name":"stdout"},{"output_type":"stream","text":["[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n","[Parallel(n_jobs=4)]: Done  10 tasks      | elapsed:   46.6s\n","[Parallel(n_jobs=4)]: Done  64 tasks      | elapsed:  8.4min\n","[Parallel(n_jobs=4)]: Done 154 tasks      | elapsed: 17.0min\n","[Parallel(n_jobs=4)]: Done 250 out of 250 | elapsed: 25.6min finished\n"],"name":"stderr"},{"output_type":"stream","text":["[16:24:36] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["RandomizedSearchCV(cv=5, error_score=nan,\n","                   estimator=XGBRegressor(base_score=0.5, booster='gbtree',\n","                                          colsample_bylevel=1,\n","                                          colsample_bynode=1,\n","                                          colsample_bytree=1, gamma=0,\n","                                          importance_type='gain',\n","                                          learning_rate=0.1, max_delta_step=0,\n","                                          max_depth=3, min_child_weight=1,\n","                                          missing=None, n_estimators=100,\n","                                          n_jobs=1, nthread=None,\n","                                          objective='reg:linear',\n","                                          random_state=0, reg_alpha=...\n","                   iid='deprecated', n_iter=50, n_jobs=4,\n","                   param_distributions={'base_score': [0.25, 0.5, 0.75, 1],\n","                                        'booster': ['gbtree', 'gblinear'],\n","                                        'learning_rate': [0.05, 0.1, 0.15, 0.2],\n","                                        'max_depth': [2, 3, 5, 10, 15],\n","                                        'min_child_weight': [1, 2, 3, 4],\n","                                        'n_estimators': [100, 500, 900, 1100,\n","                                                         1500]},\n","                   pre_dispatch='2*n_jobs', random_state=42, refit=True,\n","                   return_train_score=True, scoring='neg_mean_absolute_error',\n","                   verbose=5)"]},"metadata":{"tags":[]},"execution_count":20}]},{"cell_type":"code","metadata":{"id":"lj49uCTW1hUZ","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":136},"outputId":"23f8e373-47ac-4e0b-dbba-c6bd40845050","executionInfo":{"status":"ok","timestamp":1591806282752,"user_tz":240,"elapsed":1579471,"user":{"displayName":"Ravi Vattikuti","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjyKWq_9kMPQBBkVSgJCrYyroBcPqR0yjOIIrSQjcE=s64","userId":"12788632665998596893"}}},"source":["random_cv.best_estimator_"],"execution_count":21,"outputs":[{"output_type":"execute_result","data":{"text/plain":["XGBRegressor(base_score=0.25, booster='gbtree', colsample_bylevel=1,\n","             colsample_bynode=1, colsample_bytree=1, gamma=0,\n","             importance_type='gain', learning_rate=0.1, max_delta_step=0,\n","             max_depth=2, min_child_weight=1, missing=None, n_estimators=900,\n","             n_jobs=1, nthread=None, objective='reg:linear', random_state=0,\n","             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n","             silent=None, subsample=1, verbosity=1)"]},"metadata":{"tags":[]},"execution_count":21}]},{"cell_type":"code","metadata":{"id":"-MGwVO7_1xli","colab_type":"code","colab":{}},"source":["XGB = xgboost.XGBRegressor(base_score=0.25, booster='gbtree', colsample_bylevel=1,\n","             colsample_bynode=1, colsample_bytree=1, gamma=0,\n","             importance_type='gain', learning_rate=0.1, max_delta_step=0,\n","             max_depth=2, min_child_weight=1, missing=None, n_estimators=900,\n","             n_jobs=1, nthread=None, objective='reg:linear', random_state=0,\n","             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n","             silent=None, subsample=1, verbosity=1)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"EcSCyeJq2FUh","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":173},"outputId":"c5fb6795-ce26-4f5c-b90e-6f9913c56eb4","executionInfo":{"status":"ok","timestamp":1591806287882,"user_tz":240,"elapsed":1584501,"user":{"displayName":"Ravi Vattikuti","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjyKWq_9kMPQBBkVSgJCrYyroBcPqR0yjOIIrSQjcE=s64","userId":"12788632665998596893"}}},"source":["XGB.fit(X_train,Y)"],"execution_count":23,"outputs":[{"output_type":"stream","text":["[16:24:42] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["XGBRegressor(base_score=0.25, booster='gbtree', colsample_bylevel=1,\n","             colsample_bynode=1, colsample_bytree=1, gamma=0,\n","             importance_type='gain', learning_rate=0.1, max_delta_step=0,\n","             max_depth=2, min_child_weight=1, missing=None, n_estimators=900,\n","             n_jobs=1, nthread=None, objective='reg:linear', random_state=0,\n","             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n","             silent=None, subsample=1, verbosity=1)"]},"metadata":{"tags":[]},"execution_count":23}]},{"cell_type":"code","metadata":{"id":"5z_PQUXO2I7G","colab_type":"code","colab":{}},"source":["pred_XGB1 = XGB.predict(X_test)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"3X-hi0O-2aYp","colab_type":"code","colab":{}},"source":["prediction_XGB1 = pd.DataFrame(pred_XGB1)\n","submission_XGB1 = pd.concat([pd.read_csv('sample_submission.csv')['Id'], prediction_XGB1], axis = 1)\n","submission_XGB1.columns = ['Id', 'SalePrice']\n","submission_XGB1.to_csv('Submission_XGB1.csv', index = False)\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"YdfQtVrD24ny","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"outputId":"b960d003-a164-40f6-a936-6919f6658b3f","executionInfo":{"status":"ok","timestamp":1591806499559,"user_tz":240,"elapsed":1796037,"user":{"displayName":"Ravi Vattikuti","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjyKWq_9kMPQBBkVSgJCrYyroBcPqR0yjOIIrSQjcE=s64","userId":"12788632665998596893"}}},"source":["# Neural Networks\n","import keras\n","from keras.models import Sequential\n","from keras.layers import Dense\n","from keras.layers import LeakyReLU,PReLU,ELU\n","from keras.layers import Dropout\n","\n","\n","\n","\n","# Initialising the ANN\n","Model = Sequential()\n","\n","# Adding the input layer and the first hidden layer\n","Model.add(Dense(50, init = 'he_uniform',activation='relu',input_dim = 237))\n","\n","# Adding the second hidden layer\n","Model.add(Dense(25, init = 'he_uniform',activation='relu'))\n","\n","# Adding the third hidden layer\n","Model.add(Dense(50, init = 'he_uniform',activation='relu'))\n","# Adding the output layer\n","Model.add(Dense(1, init = 'he_uniform'))\n","\n","# Compiling the ANN\n","Model.compile(loss='mse', optimizer='Adamax')\n","\n","# Fitting the ANN to the Training set\n","Model.fit(X_train, Y,validation_split=0.20, batch_size = 16, nb_epoch = 1000)"],"execution_count":26,"outputs":[{"output_type":"stream","text":["Using TensorFlow backend.\n","/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:15: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(50, activation=\"relu\", input_dim=237, kernel_initializer=\"he_uniform\")`\n","  from ipykernel import kernelapp as app\n","/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:18: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(25, activation=\"relu\", kernel_initializer=\"he_uniform\")`\n","/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:21: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(50, activation=\"relu\", kernel_initializer=\"he_uniform\")`\n","/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:23: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(1, kernel_initializer=\"he_uniform\")`\n","/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:29: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n"],"name":"stderr"},{"output_type":"stream","text":["Train on 1168 samples, validate on 292 samples\n","Epoch 1/1000\n","1168/1168 [==============================] - 2s 2ms/step - loss: 38818473450.9589 - val_loss: 39915401608.7671\n","Epoch 2/1000\n","1168/1168 [==============================] - 0s 180us/step - loss: 38808280049.9726 - val_loss: 39892635030.7945\n","Epoch 3/1000\n","1168/1168 [==============================] - 0s 164us/step - loss: 38761827875.0685 - val_loss: 39809954072.5479\n","Epoch 4/1000\n","1168/1168 [==============================] - 0s 162us/step - loss: 38623553059.0685 - val_loss: 39598801064.3288\n","Epoch 5/1000\n","1168/1168 [==============================] - 0s 174us/step - loss: 38313276373.9178 - val_loss: 39158483028.1644\n","Epoch 6/1000\n","1168/1168 [==============================] - 0s 162us/step - loss: 37722956814.0274 - val_loss: 38368515562.9589\n","Epoch 7/1000\n","1168/1168 [==============================] - 0s 179us/step - loss: 36721918148.3836 - val_loss: 37102809480.7671\n","Epoch 8/1000\n","1168/1168 [==============================] - 0s 161us/step - loss: 35196702215.0137 - val_loss: 35187389538.1918\n","Epoch 9/1000\n","1168/1168 [==============================] - 0s 162us/step - loss: 32958717362.8493 - val_loss: 32563951503.7808\n","Epoch 10/1000\n","1168/1168 [==============================] - 0s 162us/step - loss: 30091841592.1096 - val_loss: 29381781756.4931\n","Epoch 11/1000\n","1168/1168 [==============================] - 0s 159us/step - loss: 26680648199.0137 - val_loss: 25476244297.6438\n","Epoch 12/1000\n","1168/1168 [==============================] - 0s 184us/step - loss: 22470563194.7397 - val_loss: 20953720832.0000\n","Epoch 13/1000\n","1168/1168 [==============================] - 0s 168us/step - loss: 17956652480.8767 - val_loss: 16362301468.0548\n","Epoch 14/1000\n","1168/1168 [==============================] - 0s 167us/step - loss: 13789475250.8493 - val_loss: 12469094722.6301\n","Epoch 15/1000\n","1168/1168 [==============================] - 0s 203us/step - loss: 10600867356.0548 - val_loss: 9957066057.6438\n","Epoch 16/1000\n","1168/1168 [==============================] - 0s 158us/step - loss: 8577035802.3014 - val_loss: 8484661500.4932\n","Epoch 17/1000\n","1168/1168 [==============================] - 0s 190us/step - loss: 7556345380.8219 - val_loss: 7829773315.5068\n","Epoch 18/1000\n","1168/1168 [==============================] - 0s 162us/step - loss: 7155817559.6712 - val_loss: 7628599916.7123\n","Epoch 19/1000\n","1168/1168 [==============================] - 0s 160us/step - loss: 7023345867.3973 - val_loss: 7551706350.4658\n","Epoch 20/1000\n","1168/1168 [==============================] - 0s 157us/step - loss: 6965249972.6027 - val_loss: 7505836081.0959\n","Epoch 21/1000\n","1168/1168 [==============================] - 0s 163us/step - loss: 6927883590.1370 - val_loss: 7470515122.8493\n","Epoch 22/1000\n","1168/1168 [==============================] - 0s 168us/step - loss: 6893665918.2466 - val_loss: 7434851692.7123\n","Epoch 23/1000\n","1168/1168 [==============================] - 0s 201us/step - loss: 6858409621.0411 - val_loss: 7397341184.0000\n","Epoch 24/1000\n","1168/1168 [==============================] - 0s 158us/step - loss: 6825752866.1918 - val_loss: 7359069569.7534\n","Epoch 25/1000\n","1168/1168 [==============================] - 0s 160us/step - loss: 6787054085.2603 - val_loss: 7321736654.9041\n","Epoch 26/1000\n","1168/1168 [==============================] - 0s 162us/step - loss: 6758775997.3699 - val_loss: 7286517486.4658\n","Epoch 27/1000\n","1168/1168 [==============================] - 0s 160us/step - loss: 6724373507.5068 - val_loss: 7250725831.8904\n","Epoch 28/1000\n","1168/1168 [==============================] - 0s 187us/step - loss: 6684271477.4795 - val_loss: 7211059943.4521\n","Epoch 29/1000\n","1168/1168 [==============================] - 0s 164us/step - loss: 6650647397.6986 - val_loss: 7173920732.9315\n","Epoch 30/1000\n","1168/1168 [==============================] - 0s 163us/step - loss: 6615128316.4932 - val_loss: 7136921642.0822\n","Epoch 31/1000\n","1168/1168 [==============================] - 0s 161us/step - loss: 6583408959.1233 - val_loss: 7108904977.5342\n","Epoch 32/1000\n","1168/1168 [==============================] - 0s 163us/step - loss: 6554437845.9178 - val_loss: 7077666675.7260\n","Epoch 33/1000\n","1168/1168 [==============================] - 0s 177us/step - loss: 6525637500.4932 - val_loss: 7046435661.1507\n","Epoch 34/1000\n","1168/1168 [==============================] - 0s 159us/step - loss: 6496662103.6712 - val_loss: 7013808387.5068\n","Epoch 35/1000\n","1168/1168 [==============================] - 0s 161us/step - loss: 6463754848.4384 - val_loss: 6978730369.7534\n","Epoch 36/1000\n","1168/1168 [==============================] - 0s 159us/step - loss: 6432092189.8082 - val_loss: 6941572043.3973\n","Epoch 37/1000\n","1168/1168 [==============================] - 0s 171us/step - loss: 6395007447.6712 - val_loss: 6904369583.3425\n","Epoch 38/1000\n","1168/1168 [==============================] - 0s 209us/step - loss: 6359747022.9041 - val_loss: 6869307844.3836\n","Epoch 39/1000\n","1168/1168 [==============================] - 0s 209us/step - loss: 6329686934.7945 - val_loss: 6837381831.8904\n","Epoch 40/1000\n","1168/1168 [==============================] - 0s 192us/step - loss: 6301327006.6849 - val_loss: 6804137955.9452\n","Epoch 41/1000\n","1168/1168 [==============================] - 0s 159us/step - loss: 6272021744.2192 - val_loss: 6771280359.4521\n","Epoch 42/1000\n","1168/1168 [==============================] - 0s 168us/step - loss: 6242974034.4110 - val_loss: 6738102745.4247\n","Epoch 43/1000\n","1168/1168 [==============================] - 0s 194us/step - loss: 6203683363.0685 - val_loss: 6698607682.6301\n","Epoch 44/1000\n","1168/1168 [==============================] - 0s 163us/step - loss: 6166673527.2329 - val_loss: 6660263627.3973\n","Epoch 45/1000\n","1168/1168 [==============================] - 0s 157us/step - loss: 6130900534.3562 - val_loss: 6626055658.9589\n","Epoch 46/1000\n","1168/1168 [==============================] - 0s 159us/step - loss: 6097671027.7260 - val_loss: 6590057587.7260\n","Epoch 47/1000\n","1168/1168 [==============================] - 0s 167us/step - loss: 6066134485.9178 - val_loss: 6553883118.4658\n","Epoch 48/1000\n","1168/1168 [==============================] - 0s 192us/step - loss: 6027194199.6712 - val_loss: 6515162052.3836\n","Epoch 49/1000\n","1168/1168 [==============================] - 0s 162us/step - loss: 5989868026.7397 - val_loss: 6473930078.6849\n","Epoch 50/1000\n","1168/1168 [==============================] - 0s 158us/step - loss: 5961322538.0822 - val_loss: 6439693971.2877\n","Epoch 51/1000\n","1168/1168 [==============================] - 0s 178us/step - loss: 5927270210.6301 - val_loss: 6412812032.0000\n","Epoch 52/1000\n","1168/1168 [==============================] - 0s 224us/step - loss: 5897352590.0274 - val_loss: 6379910817.3151\n","Epoch 53/1000\n","1168/1168 [==============================] - 0s 185us/step - loss: 5868672880.2192 - val_loss: 6347049500.0548\n","Epoch 54/1000\n","1168/1168 [==============================] - 0s 174us/step - loss: 5836765887.1233 - val_loss: 6316572584.3288\n","Epoch 55/1000\n","1168/1168 [==============================] - 0s 180us/step - loss: 5806227906.6301 - val_loss: 6280109347.0685\n","Epoch 56/1000\n","1168/1168 [==============================] - 0s 178us/step - loss: 5776172458.0822 - val_loss: 6247610855.4521\n","Epoch 57/1000\n","1168/1168 [==============================] - 0s 179us/step - loss: 5740902854.1370 - val_loss: 6213077980.9315\n","Epoch 58/1000\n","1168/1168 [==============================] - 0s 183us/step - loss: 5708305706.0822 - val_loss: 6176911784.3288\n","Epoch 59/1000\n","1168/1168 [==============================] - 0s 176us/step - loss: 5673839933.3699 - val_loss: 6137972252.0548\n","Epoch 60/1000\n","1168/1168 [==============================] - 0s 178us/step - loss: 5642163333.2603 - val_loss: 6101523638.3562\n","Epoch 61/1000\n","1168/1168 [==============================] - 0s 212us/step - loss: 5608906538.0822 - val_loss: 6066910211.5068\n","Epoch 62/1000\n","1168/1168 [==============================] - 0s 199us/step - loss: 5577028567.6712 - val_loss: 6037243234.1918\n","Epoch 63/1000\n","1168/1168 [==============================] - 0s 258us/step - loss: 5544997709.1507 - val_loss: 6003978990.4658\n","Epoch 64/1000\n","1168/1168 [==============================] - 0s 182us/step - loss: 5512879735.2329 - val_loss: 5968394387.2877\n","Epoch 65/1000\n","1168/1168 [==============================] - 0s 195us/step - loss: 5477746787.9452 - val_loss: 5930017143.2329\n","Epoch 66/1000\n","1168/1168 [==============================] - 0s 177us/step - loss: 5442810456.5479 - val_loss: 5895234917.6986\n","Epoch 67/1000\n","1168/1168 [==============================] - 0s 188us/step - loss: 5410449381.6986 - val_loss: 5860258531.9452\n","Epoch 68/1000\n","1168/1168 [==============================] - 0s 173us/step - loss: 5377250838.7945 - val_loss: 5820991235.5068\n","Epoch 69/1000\n","1168/1168 [==============================] - 0s 171us/step - loss: 5339572273.9726 - val_loss: 5790504202.5205\n","Epoch 70/1000\n","1168/1168 [==============================] - 0s 177us/step - loss: 5310498212.8219 - val_loss: 5751484773.6986\n","Epoch 71/1000\n","1168/1168 [==============================] - 0s 224us/step - loss: 5271624116.6027 - val_loss: 5710304876.7123\n","Epoch 72/1000\n","1168/1168 [==============================] - 0s 229us/step - loss: 5231836000.4384 - val_loss: 5673071454.6849\n","Epoch 73/1000\n","1168/1168 [==============================] - 0s 221us/step - loss: 5198656578.6301 - val_loss: 5632763209.6438\n","Epoch 74/1000\n","1168/1168 [==============================] - 0s 174us/step - loss: 5162710059.8356 - val_loss: 5596761424.6575\n","Epoch 75/1000\n","1168/1168 [==============================] - 0s 178us/step - loss: 5127766578.8493 - val_loss: 5555474680.9863\n","Epoch 76/1000\n","1168/1168 [==============================] - 0s 213us/step - loss: 5090129080.1096 - val_loss: 5525886320.2192\n","Epoch 77/1000\n","1168/1168 [==============================] - 0s 224us/step - loss: 5058941753.8630 - val_loss: 5495045551.3425\n","Epoch 78/1000\n","1168/1168 [==============================] - 0s 230us/step - loss: 5022477911.6712 - val_loss: 5448402621.3699\n","Epoch 79/1000\n","1168/1168 [==============================] - 0s 219us/step - loss: 4995870833.9726 - val_loss: 5423508746.5205\n","Epoch 80/1000\n","1168/1168 [==============================] - 0s 229us/step - loss: 4958920726.7945 - val_loss: 5383426335.5616\n","Epoch 81/1000\n","1168/1168 [==============================] - 0s 220us/step - loss: 4922861953.7534 - val_loss: 5340587078.1370\n","Epoch 82/1000\n","1168/1168 [==============================] - 0s 205us/step - loss: 4885818127.7808 - val_loss: 5308051673.4247\n","Epoch 83/1000\n","1168/1168 [==============================] - 0s 181us/step - loss: 4848296342.7945 - val_loss: 5267509889.7534\n","Epoch 84/1000\n","1168/1168 [==============================] - 0s 171us/step - loss: 4815682172.4932 - val_loss: 5238101584.6575\n","Epoch 85/1000\n","1168/1168 [==============================] - 0s 186us/step - loss: 4784736871.4521 - val_loss: 5212741716.1644\n","Epoch 86/1000\n","1168/1168 [==============================] - 0s 178us/step - loss: 4753974063.3425 - val_loss: 5175510250.9589\n","Epoch 87/1000\n","1168/1168 [==============================] - 0s 188us/step - loss: 4723191006.6849 - val_loss: 5144420057.4247\n","Epoch 88/1000\n","1168/1168 [==============================] - 0s 171us/step - loss: 4691817876.1644 - val_loss: 5111938391.6712\n","Epoch 89/1000\n","1168/1168 [==============================] - 0s 173us/step - loss: 4658904877.5890 - val_loss: 5079755902.2466\n","Epoch 90/1000\n","1168/1168 [==============================] - 0s 184us/step - loss: 4626151448.5479 - val_loss: 5045927557.2603\n","Epoch 91/1000\n","1168/1168 [==============================] - 0s 171us/step - loss: 4593617746.4110 - val_loss: 5009194029.5890\n","Epoch 92/1000\n","1168/1168 [==============================] - 0s 178us/step - loss: 4558172451.0685 - val_loss: 4970580550.1370\n","Epoch 93/1000\n","1168/1168 [==============================] - 0s 180us/step - loss: 4524693667.0685 - val_loss: 4937627953.0959\n","Epoch 94/1000\n","1168/1168 [==============================] - 0s 175us/step - loss: 4492573959.0137 - val_loss: 4905301539.0685\n","Epoch 95/1000\n","1168/1168 [==============================] - 0s 191us/step - loss: 4460670812.0548 - val_loss: 4865859403.3973\n","Epoch 96/1000\n","1168/1168 [==============================] - 0s 220us/step - loss: 4428526195.7260 - val_loss: 4844786742.3562\n","Epoch 97/1000\n","1168/1168 [==============================] - 0s 230us/step - loss: 4400074864.2192 - val_loss: 4814836671.1233\n","Epoch 98/1000\n","1168/1168 [==============================] - 0s 198us/step - loss: 4371601513.2055 - val_loss: 4782350406.1370\n","Epoch 99/1000\n","1168/1168 [==============================] - 0s 181us/step - loss: 4341162872.9863 - val_loss: 4755745788.4932\n","Epoch 100/1000\n","1168/1168 [==============================] - 0s 189us/step - loss: 4312313691.1781 - val_loss: 4730514596.8219\n","Epoch 101/1000\n","1168/1168 [==============================] - 0s 170us/step - loss: 4275622936.5479 - val_loss: 4692061818.7397\n","Epoch 102/1000\n","1168/1168 [==============================] - 0s 169us/step - loss: 4240954520.5479 - val_loss: 4653121646.4658\n","Epoch 103/1000\n","1168/1168 [==============================] - 0s 162us/step - loss: 4206152360.3288 - val_loss: 4628707384.1096\n","Epoch 104/1000\n","1168/1168 [==============================] - 0s 170us/step - loss: 4171843320.9863 - val_loss: 4578072228.8219\n","Epoch 105/1000\n","1168/1168 [==============================] - 0s 175us/step - loss: 4134048248.9863 - val_loss: 4550732526.4658\n","Epoch 106/1000\n","1168/1168 [==============================] - 0s 177us/step - loss: 4106790152.7671 - val_loss: 4517001998.0274\n","Epoch 107/1000\n","1168/1168 [==============================] - 0s 168us/step - loss: 4081679594.9589 - val_loss: 4500073580.7123\n","Epoch 108/1000\n","1168/1168 [==============================] - 0s 161us/step - loss: 4052811151.7808 - val_loss: 4481749767.0137\n","Epoch 109/1000\n","1168/1168 [==============================] - 0s 175us/step - loss: 4029852112.6575 - val_loss: 4449323050.0822\n","Epoch 110/1000\n","1168/1168 [==============================] - 0s 188us/step - loss: 4001946655.5616 - val_loss: 4423432142.9041\n","Epoch 111/1000\n","1168/1168 [==============================] - 0s 218us/step - loss: 3977184501.4795 - val_loss: 4391770317.1507\n","Epoch 112/1000\n","1168/1168 [==============================] - 0s 207us/step - loss: 3946871811.5068 - val_loss: 4366186297.8630\n","Epoch 113/1000\n","1168/1168 [==============================] - 0s 184us/step - loss: 3918723252.6027 - val_loss: 4338711373.1507\n","Epoch 114/1000\n","1168/1168 [==============================] - 0s 174us/step - loss: 3888223693.1507 - val_loss: 4313007977.2055\n","Epoch 115/1000\n","1168/1168 [==============================] - 0s 186us/step - loss: 3862814954.9589 - val_loss: 4277546204.9315\n","Epoch 116/1000\n","1168/1168 [==============================] - 0s 169us/step - loss: 3832707758.4658 - val_loss: 4261674138.3014\n","Epoch 117/1000\n","1168/1168 [==============================] - 0s 171us/step - loss: 3804317438.2466 - val_loss: 4232638258.8493\n","Epoch 118/1000\n","1168/1168 [==============================] - 0s 166us/step - loss: 3778240446.2466 - val_loss: 4216131001.8630\n","Epoch 119/1000\n","1168/1168 [==============================] - 0s 173us/step - loss: 3755103551.1233 - val_loss: 4198044405.4795\n","Epoch 120/1000\n","1168/1168 [==============================] - 0s 177us/step - loss: 3730496699.6164 - val_loss: 4158925057.7534\n","Epoch 121/1000\n","1168/1168 [==============================] - 0s 216us/step - loss: 3709925861.6986 - val_loss: 4161682551.2329\n","Epoch 122/1000\n","1168/1168 [==============================] - 0s 204us/step - loss: 3685081579.8356 - val_loss: 4129569330.8493\n","Epoch 123/1000\n","1168/1168 [==============================] - 0s 217us/step - loss: 3657328712.7671 - val_loss: 4099124269.5890\n","Epoch 124/1000\n","1168/1168 [==============================] - 0s 191us/step - loss: 3631628470.3562 - val_loss: 4078106673.0959\n","Epoch 125/1000\n","1168/1168 [==============================] - 0s 221us/step - loss: 3606357206.3562 - val_loss: 4052820911.3425\n","Epoch 126/1000\n","1168/1168 [==============================] - 0s 201us/step - loss: 3578184490.9589 - val_loss: 4014841866.5205\n","Epoch 127/1000\n","1168/1168 [==============================] - 0s 217us/step - loss: 3556374827.8356 - val_loss: 3990865292.2740\n","Epoch 128/1000\n","1168/1168 [==============================] - 0s 177us/step - loss: 3528487634.4110 - val_loss: 3968581148.0548\n","Epoch 129/1000\n","1168/1168 [==============================] - 0s 212us/step - loss: 3506972663.2329 - val_loss: 3959422753.3151\n","Epoch 130/1000\n","1168/1168 [==============================] - 0s 206us/step - loss: 3482133276.0548 - val_loss: 3942890225.9726\n","Epoch 131/1000\n","1168/1168 [==============================] - 0s 170us/step - loss: 3461128195.5068 - val_loss: 3911406965.4795\n","Epoch 132/1000\n","1168/1168 [==============================] - 0s 217us/step - loss: 3441026150.5753 - val_loss: 3888719144.3288\n","Epoch 133/1000\n","1168/1168 [==============================] - 0s 208us/step - loss: 3415749796.8219 - val_loss: 3867146026.0822\n","Epoch 134/1000\n","1168/1168 [==============================] - 0s 210us/step - loss: 3391752054.3562 - val_loss: 3852834070.7945\n","Epoch 135/1000\n","1168/1168 [==============================] - 0s 183us/step - loss: 3369605511.8904 - val_loss: 3825194436.3836\n","Epoch 136/1000\n","1168/1168 [==============================] - 0s 164us/step - loss: 3349474441.6438 - val_loss: 3805050662.5753\n","Epoch 137/1000\n","1168/1168 [==============================] - 0s 181us/step - loss: 3327229295.3425 - val_loss: 3794164720.2192\n","Epoch 138/1000\n","1168/1168 [==============================] - 0s 168us/step - loss: 3310857282.6301 - val_loss: 3764981707.3973\n","Epoch 139/1000\n","1168/1168 [==============================] - 0s 160us/step - loss: 3285458098.8493 - val_loss: 3749773622.3562\n","Epoch 140/1000\n","1168/1168 [==============================] - 0s 158us/step - loss: 3264523228.0548 - val_loss: 3737055328.4384\n","Epoch 141/1000\n","1168/1168 [==============================] - 0s 164us/step - loss: 3244986158.4658 - val_loss: 3723338529.3151\n","Epoch 142/1000\n","1168/1168 [==============================] - 0s 178us/step - loss: 3224155055.3425 - val_loss: 3692498887.8904\n","Epoch 143/1000\n","1168/1168 [==============================] - 0s 162us/step - loss: 3199024772.3836 - val_loss: 3682648993.3151\n","Epoch 144/1000\n","1168/1168 [==============================] - 0s 164us/step - loss: 3176270233.4247 - val_loss: 3660363218.4110\n","Epoch 145/1000\n","1168/1168 [==============================] - 0s 161us/step - loss: 3153500068.8219 - val_loss: 3629726623.5616\n","Epoch 146/1000\n","1168/1168 [==============================] - 0s 162us/step - loss: 3134847431.8904 - val_loss: 3618790733.1507\n","Epoch 147/1000\n","1168/1168 [==============================] - 0s 203us/step - loss: 3120085796.8219 - val_loss: 3597018974.6849\n","Epoch 148/1000\n","1168/1168 [==============================] - 0s 159us/step - loss: 3102900141.5890 - val_loss: 3593270471.8904\n","Epoch 149/1000\n","1168/1168 [==============================] - 0s 162us/step - loss: 3091122580.1644 - val_loss: 3586693677.5890\n","Epoch 150/1000\n","1168/1168 [==============================] - 0s 168us/step - loss: 3077077201.5342 - val_loss: 3560319912.3288\n","Epoch 151/1000\n","1168/1168 [==============================] - 0s 160us/step - loss: 3061656592.6575 - val_loss: 3562058250.5205\n","Epoch 152/1000\n","1168/1168 [==============================] - 0s 182us/step - loss: 3047743686.1370 - val_loss: 3541484579.0685\n","Epoch 153/1000\n","1168/1168 [==============================] - 0s 217us/step - loss: 3030852054.7945 - val_loss: 3523478506.9589\n","Epoch 154/1000\n","1168/1168 [==============================] - 0s 215us/step - loss: 3017058940.4932 - val_loss: 3503616862.6849\n","Epoch 155/1000\n","1168/1168 [==============================] - 0s 164us/step - loss: 2996980841.2055 - val_loss: 3492421390.0274\n","Epoch 156/1000\n","1168/1168 [==============================] - 0s 164us/step - loss: 2980402514.4110 - val_loss: 3477788721.0959\n","Epoch 157/1000\n","1168/1168 [==============================] - 0s 201us/step - loss: 2962486122.9589 - val_loss: 3462389184.8767\n","Epoch 158/1000\n","1168/1168 [==============================] - 0s 233us/step - loss: 2945603682.1918 - val_loss: 3440299234.1918\n","Epoch 159/1000\n","1168/1168 [==============================] - 0s 187us/step - loss: 2926239429.2603 - val_loss: 3419830673.5342\n","Epoch 160/1000\n","1168/1168 [==============================] - 0s 165us/step - loss: 2909030450.8493 - val_loss: 3404360044.7123\n","Epoch 161/1000\n","1168/1168 [==============================] - 0s 176us/step - loss: 2888837358.4658 - val_loss: 3390014727.0137\n","Epoch 162/1000\n","1168/1168 [==============================] - 0s 185us/step - loss: 2870332211.7260 - val_loss: 3371552568.1096\n","Epoch 163/1000\n","1168/1168 [==============================] - 0s 176us/step - loss: 2854604241.5342 - val_loss: 3342975707.1781\n","Epoch 164/1000\n","1168/1168 [==============================] - 0s 183us/step - loss: 2834176327.8904 - val_loss: 3327026449.5342\n","Epoch 165/1000\n","1168/1168 [==============================] - 0s 167us/step - loss: 2813375475.2877 - val_loss: 3324256974.9041\n","Epoch 166/1000\n","1168/1168 [==============================] - 0s 174us/step - loss: 2796632846.9041 - val_loss: 3313358912.8767\n","Epoch 167/1000\n","1168/1168 [==============================] - 0s 179us/step - loss: 2781094343.8904 - val_loss: 3290876826.3014\n","Epoch 168/1000\n","1168/1168 [==============================] - 0s 166us/step - loss: 2766384576.8767 - val_loss: 3279561815.6712\n","Epoch 169/1000\n","1168/1168 [==============================] - 0s 165us/step - loss: 2749898390.7945 - val_loss: 3248788283.6164\n","Epoch 170/1000\n","1168/1168 [==============================] - 0s 183us/step - loss: 2733408543.5616 - val_loss: 3248384320.8767\n","Epoch 171/1000\n","1168/1168 [==============================] - 0s 177us/step - loss: 2721165351.4521 - val_loss: 3239903381.0411\n","Epoch 172/1000\n","1168/1168 [==============================] - 0s 180us/step - loss: 2701961700.8219 - val_loss: 3205934090.5205\n","Epoch 173/1000\n","1168/1168 [==============================] - 0s 166us/step - loss: 2689029264.6575 - val_loss: 3194758307.0685\n","Epoch 174/1000\n","1168/1168 [==============================] - 0s 163us/step - loss: 2675011532.2740 - val_loss: 3184388408.1096\n","Epoch 175/1000\n","1168/1168 [==============================] - 0s 162us/step - loss: 2660735330.1918 - val_loss: 3158807153.9726\n","Epoch 176/1000\n","1168/1168 [==============================] - 0s 165us/step - loss: 2646729936.6575 - val_loss: 3167396192.4384\n","Epoch 177/1000\n","1168/1168 [==============================] - 0s 176us/step - loss: 2629955181.5890 - val_loss: 3155907450.7397\n","Epoch 178/1000\n","1168/1168 [==============================] - 0s 218us/step - loss: 2614979748.8219 - val_loss: 3128984456.7671\n","Epoch 179/1000\n","1168/1168 [==============================] - 0s 172us/step - loss: 2601644792.1096 - val_loss: 3102470456.1096\n","Epoch 180/1000\n","1168/1168 [==============================] - 0s 164us/step - loss: 2583700298.5205 - val_loss: 3097391568.6575\n","Epoch 181/1000\n","1168/1168 [==============================] - 0s 172us/step - loss: 2570301294.4658 - val_loss: 3095265953.3151\n","Epoch 182/1000\n","1168/1168 [==============================] - 0s 169us/step - loss: 2556123269.2603 - val_loss: 3065811719.0137\n","Epoch 183/1000\n","1168/1168 [==============================] - 0s 163us/step - loss: 2544302459.6164 - val_loss: 3058160587.3973\n","Epoch 184/1000\n","1168/1168 [==============================] - 0s 160us/step - loss: 2529143418.7397 - val_loss: 3045191646.6849\n","Epoch 185/1000\n","1168/1168 [==============================] - 0s 163us/step - loss: 2516695659.8356 - val_loss: 3038814451.7260\n","Epoch 186/1000\n","1168/1168 [==============================] - 0s 164us/step - loss: 2504161120.4384 - val_loss: 3020288241.9726\n","Epoch 187/1000\n","1168/1168 [==============================] - 0s 179us/step - loss: 2490888585.2055 - val_loss: 3014292679.8904\n","Epoch 188/1000\n","1168/1168 [==============================] - 0s 170us/step - loss: 2477359614.2466 - val_loss: 3006999569.5342\n","Epoch 189/1000\n","1168/1168 [==============================] - 0s 164us/step - loss: 2466826788.8219 - val_loss: 2993511934.2466\n","Epoch 190/1000\n","1168/1168 [==============================] - 0s 161us/step - loss: 2455204712.7671 - val_loss: 2976824507.6164\n","Epoch 191/1000\n","1168/1168 [==============================] - 0s 195us/step - loss: 2444896737.3151 - val_loss: 2964467073.7534\n","Epoch 192/1000\n","1168/1168 [==============================] - 0s 223us/step - loss: 2432428048.6575 - val_loss: 2944254819.9452\n","Epoch 193/1000\n","1168/1168 [==============================] - 0s 206us/step - loss: 2420013135.7808 - val_loss: 2939093812.6027\n","Epoch 194/1000\n","1168/1168 [==============================] - 0s 170us/step - loss: 2406792644.3836 - val_loss: 2935382996.1644\n","Epoch 195/1000\n","1168/1168 [==============================] - 0s 168us/step - loss: 2394350801.5342 - val_loss: 2919087896.5479\n","Epoch 196/1000\n","1168/1168 [==============================] - 0s 169us/step - loss: 2381946244.3836 - val_loss: 2913286422.7945\n","Epoch 197/1000\n","1168/1168 [==============================] - 0s 186us/step - loss: 2371162090.0822 - val_loss: 2889682845.8082\n","Epoch 198/1000\n","1168/1168 [==============================] - 0s 161us/step - loss: 2359339149.1507 - val_loss: 2880876673.7534\n","Epoch 199/1000\n","1168/1168 [==============================] - 0s 161us/step - loss: 2347472888.1096 - val_loss: 2867959443.2877\n","Epoch 200/1000\n","1168/1168 [==============================] - 0s 165us/step - loss: 2337700404.1644 - val_loss: 2857530534.5753\n","Epoch 201/1000\n","1168/1168 [==============================] - 0s 160us/step - loss: 2326851817.2055 - val_loss: 2858231862.3562\n","Epoch 202/1000\n","1168/1168 [==============================] - 0s 174us/step - loss: 2317203789.1507 - val_loss: 2830483059.7260\n","Epoch 203/1000\n","1168/1168 [==============================] - 0s 161us/step - loss: 2307761645.1507 - val_loss: 2824608943.3425\n","Epoch 204/1000\n","1168/1168 [==============================] - 0s 166us/step - loss: 2295638168.5479 - val_loss: 2812357479.4521\n","Epoch 205/1000\n","1168/1168 [==============================] - 0s 163us/step - loss: 2286356173.5890 - val_loss: 2796641083.6164\n","Epoch 206/1000\n","1168/1168 [==============================] - 0s 159us/step - loss: 2274783506.4110 - val_loss: 2796221966.0274\n","Epoch 207/1000\n","1168/1168 [==============================] - 0s 161us/step - loss: 2264510365.8082 - val_loss: 2786195596.2740\n","Epoch 208/1000\n","1168/1168 [==============================] - 0s 192us/step - loss: 2253537455.3425 - val_loss: 2778193550.0274\n","Epoch 209/1000\n","1168/1168 [==============================] - 0s 161us/step - loss: 2244143980.7123 - val_loss: 2771171782.1370\n","Epoch 210/1000\n","1168/1168 [==============================] - 0s 211us/step - loss: 2235893421.5890 - val_loss: 2764836548.3836\n","Epoch 211/1000\n","1168/1168 [==============================] - 0s 203us/step - loss: 2222343189.9178 - val_loss: 2754423104.8767\n","Epoch 212/1000\n","1168/1168 [==============================] - 0s 233us/step - loss: 2211761149.3699 - val_loss: 2720833257.2055\n","Epoch 213/1000\n","1168/1168 [==============================] - 0s 191us/step - loss: 2202866756.8219 - val_loss: 2703717633.7534\n","Epoch 214/1000\n","1168/1168 [==============================] - 0s 193us/step - loss: 2190798709.4795 - val_loss: 2700192927.5616\n","Epoch 215/1000\n","1168/1168 [==============================] - 0s 164us/step - loss: 2178873171.2877 - val_loss: 2704141973.0411\n","Epoch 216/1000\n","1168/1168 [==============================] - 0s 164us/step - loss: 2168574173.8082 - val_loss: 2697146076.9315\n","Epoch 217/1000\n","1168/1168 [==============================] - 0s 181us/step - loss: 2160333411.9452 - val_loss: 2690377894.5753\n","Epoch 218/1000\n","1168/1168 [==============================] - 0s 161us/step - loss: 2148541486.4658 - val_loss: 2668522385.5342\n","Epoch 219/1000\n","1168/1168 [==============================] - 0s 198us/step - loss: 2138241635.9452 - val_loss: 2656469966.9041\n","Epoch 220/1000\n","1168/1168 [==============================] - 0s 216us/step - loss: 2127942396.4932 - val_loss: 2649646993.5342\n","Epoch 221/1000\n","1168/1168 [==============================] - 0s 210us/step - loss: 2119543629.5890 - val_loss: 2642732365.1507\n","Epoch 222/1000\n","1168/1168 [==============================] - 0s 188us/step - loss: 2111095566.4658 - val_loss: 2631961121.3151\n","Epoch 223/1000\n","1168/1168 [==============================] - 0s 163us/step - loss: 2102977550.9041 - val_loss: 2624505315.9452\n","Epoch 224/1000\n","1168/1168 [==============================] - 0s 190us/step - loss: 2094166130.8493 - val_loss: 2609635219.2877\n","Epoch 225/1000\n","1168/1168 [==============================] - 0s 204us/step - loss: 2087216681.2055 - val_loss: 2606480578.6301\n","Epoch 226/1000\n","1168/1168 [==============================] - 0s 219us/step - loss: 2079790480.6575 - val_loss: 2602579359.5616\n","Epoch 227/1000\n","1168/1168 [==============================] - 0s 166us/step - loss: 2072995624.7671 - val_loss: 2593621924.8219\n","Epoch 228/1000\n","1168/1168 [==============================] - 0s 161us/step - loss: 2067127332.3836 - val_loss: 2579899248.2192\n","Epoch 229/1000\n","1168/1168 [==============================] - 0s 165us/step - loss: 2059972797.3699 - val_loss: 2578788227.5068\n","Epoch 230/1000\n","1168/1168 [==============================] - 0s 161us/step - loss: 2057405464.9863 - val_loss: 2565440194.6301\n","Epoch 231/1000\n","1168/1168 [==============================] - 0s 169us/step - loss: 2047814819.5068 - val_loss: 2555017971.7260\n","Epoch 232/1000\n","1168/1168 [==============================] - 0s 176us/step - loss: 2042880584.3288 - val_loss: 2558514274.1918\n","Epoch 233/1000\n","1168/1168 [==============================] - 0s 161us/step - loss: 2036231324.4932 - val_loss: 2545321733.2603\n","Epoch 234/1000\n","1168/1168 [==============================] - 0s 159us/step - loss: 2028254740.1644 - val_loss: 2542209209.8630\n","Epoch 235/1000\n","1168/1168 [==============================] - 0s 166us/step - loss: 2021495523.0685 - val_loss: 2541948875.3973\n","Epoch 236/1000\n","1168/1168 [==============================] - 0s 167us/step - loss: 2017431054.9041 - val_loss: 2517802680.1096\n","Epoch 237/1000\n","1168/1168 [==============================] - 0s 167us/step - loss: 2009221975.2329 - val_loss: 2527961301.9178\n","Epoch 238/1000\n","1168/1168 [==============================] - 0s 209us/step - loss: 2003534910.6849 - val_loss: 2518039047.0137\n","Epoch 239/1000\n","1168/1168 [==============================] - 0s 157us/step - loss: 2005706776.1096 - val_loss: 2521385037.1507\n","Epoch 240/1000\n","1168/1168 [==============================] - 0s 169us/step - loss: 1993551089.5342 - val_loss: 2506293572.3836\n","Epoch 241/1000\n","1168/1168 [==============================] - 0s 169us/step - loss: 1987108005.6986 - val_loss: 2488592557.5890\n","Epoch 242/1000\n","1168/1168 [==============================] - 0s 168us/step - loss: 1979387226.7397 - val_loss: 2500866696.7671\n","Epoch 243/1000\n","1168/1168 [==============================] - 0s 182us/step - loss: 1978025464.5479 - val_loss: 2492363290.3014\n","Epoch 244/1000\n","1168/1168 [==============================] - 0s 162us/step - loss: 1969740807.0137 - val_loss: 2488487157.4795\n","Epoch 245/1000\n","1168/1168 [==============================] - 0s 166us/step - loss: 1965421994.9589 - val_loss: 2479203626.0822\n","Epoch 246/1000\n","1168/1168 [==============================] - 0s 168us/step - loss: 1961793099.8356 - val_loss: 2481421525.9178\n","Epoch 247/1000\n","1168/1168 [==============================] - 0s 169us/step - loss: 1954055564.2740 - val_loss: 2459943448.5479\n","Epoch 248/1000\n","1168/1168 [==============================] - 0s 168us/step - loss: 1950867144.7671 - val_loss: 2455625007.3425\n","Epoch 249/1000\n","1168/1168 [==============================] - 0s 213us/step - loss: 1944198186.0822 - val_loss: 2458853668.8219\n","Epoch 250/1000\n","1168/1168 [==============================] - 0s 204us/step - loss: 1940686428.9315 - val_loss: 2447993996.2740\n","Epoch 251/1000\n","1168/1168 [==============================] - 0s 185us/step - loss: 1937600317.3699 - val_loss: 2445463473.0959\n","Epoch 252/1000\n","1168/1168 [==============================] - 0s 174us/step - loss: 1931264860.0548 - val_loss: 2447415660.7123\n","Epoch 253/1000\n","1168/1168 [==============================] - 0s 173us/step - loss: 1926327623.4521 - val_loss: 2423357457.5342\n","Epoch 254/1000\n","1168/1168 [==============================] - 0s 157us/step - loss: 1924140441.4247 - val_loss: 2439567544.1096\n","Epoch 255/1000\n","1168/1168 [==============================] - 0s 161us/step - loss: 1918597720.5479 - val_loss: 2413065594.7397\n","Epoch 256/1000\n","1168/1168 [==============================] - 0s 161us/step - loss: 1912684909.5890 - val_loss: 2421095422.2466\n","Epoch 257/1000\n","1168/1168 [==============================] - 0s 209us/step - loss: 1909534288.6575 - val_loss: 2414799663.3425\n","Epoch 258/1000\n","1168/1168 [==============================] - 0s 207us/step - loss: 1905212558.0274 - val_loss: 2411530129.5342\n","Epoch 259/1000\n","1168/1168 [==============================] - 0s 209us/step - loss: 1902731693.5890 - val_loss: 2405213511.8904\n","Epoch 260/1000\n","1168/1168 [==============================] - 0s 215us/step - loss: 1898691410.4110 - val_loss: 2410885286.5753\n","Epoch 261/1000\n","1168/1168 [==============================] - 0s 214us/step - loss: 1894929348.3836 - val_loss: 2404124286.2466\n","Epoch 262/1000\n","1168/1168 [==============================] - 0s 171us/step - loss: 1892404131.0685 - val_loss: 2400062085.2603\n","Epoch 263/1000\n","1168/1168 [==============================] - 0s 161us/step - loss: 1888336084.6027 - val_loss: 2386563496.3288\n","Epoch 264/1000\n","1168/1168 [==============================] - 0s 163us/step - loss: 1884037532.4932 - val_loss: 2393650749.3699\n","Epoch 265/1000\n","1168/1168 [==============================] - 0s 176us/step - loss: 1879361091.5068 - val_loss: 2383631447.6712\n","Epoch 266/1000\n","1168/1168 [==============================] - 0s 223us/step - loss: 1876094712.5479 - val_loss: 2386120781.1507\n","Epoch 267/1000\n","1168/1168 [==============================] - 0s 184us/step - loss: 1871646919.0137 - val_loss: 2371535081.2055\n","Epoch 268/1000\n","1168/1168 [==============================] - 0s 170us/step - loss: 1866999676.0548 - val_loss: 2386388988.4932\n","Epoch 269/1000\n","1168/1168 [==============================] - 0s 163us/step - loss: 1865638635.8356 - val_loss: 2373122286.4658\n","Epoch 270/1000\n","1168/1168 [==============================] - 0s 162us/step - loss: 1859422160.6575 - val_loss: 2370215129.4247\n","Epoch 271/1000\n","1168/1168 [==============================] - 0s 179us/step - loss: 1855083552.4384 - val_loss: 2362356732.4932\n","Epoch 272/1000\n","1168/1168 [==============================] - 0s 164us/step - loss: 1852931309.1507 - val_loss: 2359586070.7945\n","Epoch 273/1000\n","1168/1168 [==============================] - 0s 157us/step - loss: 1851149179.6164 - val_loss: 2359241328.2192\n","Epoch 274/1000\n","1168/1168 [==============================] - 0s 157us/step - loss: 1844933893.6986 - val_loss: 2355680475.1781\n","Epoch 275/1000\n","1168/1168 [==============================] - 0s 166us/step - loss: 1837938248.7671 - val_loss: 2344717880.1096\n","Epoch 276/1000\n","1168/1168 [==============================] - 0s 196us/step - loss: 1835399816.3288 - val_loss: 2333728492.7123\n","Epoch 277/1000\n","1168/1168 [==============================] - 0s 211us/step - loss: 1830886020.3836 - val_loss: 2338365375.1233\n","Epoch 278/1000\n","1168/1168 [==============================] - 0s 213us/step - loss: 1826793437.8082 - val_loss: 2341108227.5068\n","Epoch 279/1000\n","1168/1168 [==============================] - 0s 203us/step - loss: 1823354301.8082 - val_loss: 2334894928.6575\n","Epoch 280/1000\n","1168/1168 [==============================] - 0s 168us/step - loss: 1820394592.4384 - val_loss: 2319318724.3836\n","Epoch 281/1000\n","1168/1168 [==============================] - 0s 178us/step - loss: 1816009753.8630 - val_loss: 2315136140.2740\n","Epoch 282/1000\n","1168/1168 [==============================] - 0s 163us/step - loss: 1811271788.4932 - val_loss: 2311316153.8630\n","Epoch 283/1000\n","1168/1168 [==============================] - 0s 165us/step - loss: 1804902962.8493 - val_loss: 2313806276.3836\n","Epoch 284/1000\n","1168/1168 [==============================] - 0s 169us/step - loss: 1803814335.1233 - val_loss: 2306720164.8219\n","Epoch 285/1000\n","1168/1168 [==============================] - 0s 182us/step - loss: 1801201025.3151 - val_loss: 2308832969.6438\n","Epoch 286/1000\n","1168/1168 [==============================] - 0s 206us/step - loss: 1796804333.8082 - val_loss: 2308370533.6986\n","Epoch 287/1000\n","1168/1168 [==============================] - 0s 160us/step - loss: 1791947738.3014 - val_loss: 2297680038.5753\n","Epoch 288/1000\n","1168/1168 [==============================] - 0s 159us/step - loss: 1786766464.4384 - val_loss: 2291079530.9589\n","Epoch 289/1000\n","1168/1168 [==============================] - 0s 167us/step - loss: 1784002936.3288 - val_loss: 2288086531.5068\n","Epoch 290/1000\n","1168/1168 [==============================] - 0s 163us/step - loss: 1778809199.7808 - val_loss: 2284023665.9726\n","Epoch 291/1000\n","1168/1168 [==============================] - 0s 172us/step - loss: 1775932320.4384 - val_loss: 2282778450.4110\n","Epoch 292/1000\n","1168/1168 [==============================] - 0s 169us/step - loss: 1771485708.7123 - val_loss: 2275892299.3973\n","Epoch 293/1000\n","1168/1168 [==============================] - 0s 163us/step - loss: 1766994108.9315 - val_loss: 2271030571.8356\n","Epoch 294/1000\n","1168/1168 [==============================] - 0s 162us/step - loss: 1764499400.7671 - val_loss: 2264999264.4384\n","Epoch 295/1000\n","1168/1168 [==============================] - 0s 163us/step - loss: 1767065160.7671 - val_loss: 2266545907.7260\n","Epoch 296/1000\n","1168/1168 [==============================] - 0s 166us/step - loss: 1758124634.7397 - val_loss: 2268017783.2329\n","Epoch 297/1000\n","1168/1168 [==============================] - 0s 178us/step - loss: 1754575560.3288 - val_loss: 2265262623.5616\n","Epoch 298/1000\n","1168/1168 [==============================] - 0s 166us/step - loss: 1752348415.1233 - val_loss: 2258431033.8630\n","Epoch 299/1000\n","1168/1168 [==============================] - 0s 163us/step - loss: 1748538529.3151 - val_loss: 2258772935.8904\n","Epoch 300/1000\n","1168/1168 [==============================] - 0s 167us/step - loss: 1745074269.8082 - val_loss: 2258433430.7945\n","Epoch 301/1000\n","1168/1168 [==============================] - 0s 163us/step - loss: 1743090129.9726 - val_loss: 2249797902.0274\n","Epoch 302/1000\n","1168/1168 [==============================] - 0s 210us/step - loss: 1738181447.8904 - val_loss: 2250494744.5479\n","Epoch 303/1000\n","1168/1168 [==============================] - 0s 214us/step - loss: 1735517974.7945 - val_loss: 2244739212.2740\n","Epoch 304/1000\n","1168/1168 [==============================] - 0s 187us/step - loss: 1732273105.9726 - val_loss: 2241415350.3562\n","Epoch 305/1000\n","1168/1168 [==============================] - 0s 159us/step - loss: 1730246512.2192 - val_loss: 2238999779.9452\n","Epoch 306/1000\n","1168/1168 [==============================] - 0s 187us/step - loss: 1724636487.0137 - val_loss: 2232742322.8493\n","Epoch 307/1000\n","1168/1168 [==============================] - 0s 165us/step - loss: 1722644300.2740 - val_loss: 2232713889.3151\n","Epoch 308/1000\n","1168/1168 [==============================] - 0s 160us/step - loss: 1721325629.8082 - val_loss: 2231371579.6164\n","Epoch 309/1000\n","1168/1168 [==============================] - 0s 162us/step - loss: 1716771678.6849 - val_loss: 2224781601.3151\n","Epoch 310/1000\n","1168/1168 [==============================] - 0s 164us/step - loss: 1714173891.9452 - val_loss: 2222631918.4658\n","Epoch 311/1000\n","1168/1168 [==============================] - 0s 173us/step - loss: 1712825213.8082 - val_loss: 2219230485.0411\n","Epoch 312/1000\n","1168/1168 [==============================] - 0s 171us/step - loss: 1710118298.7397 - val_loss: 2217371392.0000\n","Epoch 313/1000\n","1168/1168 [==============================] - 0s 164us/step - loss: 1705841645.1507 - val_loss: 2223399048.7671\n","Epoch 314/1000\n","1168/1168 [==============================] - 0s 166us/step - loss: 1706926726.5753 - val_loss: 2217330393.4247\n","Epoch 315/1000\n","1168/1168 [==============================] - 0s 167us/step - loss: 1701051985.0959 - val_loss: 2209052817.5342\n","Epoch 316/1000\n","1168/1168 [==============================] - 0s 164us/step - loss: 1700343651.9452 - val_loss: 2211254335.1233\n","Epoch 317/1000\n","1168/1168 [==============================] - 0s 173us/step - loss: 1697748659.7260 - val_loss: 2205308535.2329\n","Epoch 318/1000\n","1168/1168 [==============================] - 0s 163us/step - loss: 1694535574.3562 - val_loss: 2208028265.2055\n","Epoch 319/1000\n","1168/1168 [==============================] - 0s 166us/step - loss: 1690268454.7945 - val_loss: 2194616448.0000\n","Epoch 320/1000\n","1168/1168 [==============================] - 0s 165us/step - loss: 1686541763.5068 - val_loss: 2200500108.2740\n","Epoch 321/1000\n","1168/1168 [==============================] - 0s 166us/step - loss: 1684365727.5616 - val_loss: 2192173162.9589\n","Epoch 322/1000\n","1168/1168 [==============================] - 0s 174us/step - loss: 1688767211.3973 - val_loss: 2200932828.9315\n","Epoch 323/1000\n","1168/1168 [==============================] - 0s 162us/step - loss: 1679441291.3973 - val_loss: 2188742438.5753\n","Epoch 324/1000\n","1168/1168 [==============================] - 0s 169us/step - loss: 1676871949.1507 - val_loss: 2190120160.4384\n","Epoch 325/1000\n","1168/1168 [==============================] - 0s 173us/step - loss: 1674329082.3014 - val_loss: 2193740028.4932\n","Epoch 326/1000\n","1168/1168 [==============================] - 0s 164us/step - loss: 1673607284.6027 - val_loss: 2188947789.1507\n","Epoch 327/1000\n","1168/1168 [==============================] - 0s 160us/step - loss: 1669888315.6164 - val_loss: 2187920385.7534\n","Epoch 328/1000\n","1168/1168 [==============================] - 0s 175us/step - loss: 1668917280.0000 - val_loss: 2180881576.3288\n","Epoch 329/1000\n","1168/1168 [==============================] - 0s 159us/step - loss: 1666572775.4521 - val_loss: 2178645958.1370\n","Epoch 330/1000\n","1168/1168 [==============================] - 0s 166us/step - loss: 1661346089.2055 - val_loss: 2178125923.9452\n","Epoch 331/1000\n","1168/1168 [==============================] - 0s 161us/step - loss: 1659648373.4795 - val_loss: 2181725643.3973\n","Epoch 332/1000\n","1168/1168 [==============================] - 0s 159us/step - loss: 1658045734.5753 - val_loss: 2174017777.9726\n","Epoch 333/1000\n","1168/1168 [==============================] - 0s 174us/step - loss: 1653973918.2466 - val_loss: 2171191836.0548\n","Epoch 334/1000\n","1168/1168 [==============================] - 0s 164us/step - loss: 1652212679.4521 - val_loss: 2174006734.9041\n","Epoch 335/1000\n","1168/1168 [==============================] - 0s 171us/step - loss: 1649075944.7671 - val_loss: 2167947281.5342\n","Epoch 336/1000\n","1168/1168 [==============================] - 0s 164us/step - loss: 1647799100.0548 - val_loss: 2177885397.9178\n","Epoch 337/1000\n","1168/1168 [==============================] - 0s 159us/step - loss: 1647889015.2329 - val_loss: 2168629430.3562\n","Epoch 338/1000\n","1168/1168 [==============================] - 0s 177us/step - loss: 1643237610.9589 - val_loss: 2163690688.8767\n","Epoch 339/1000\n","1168/1168 [==============================] - 0s 158us/step - loss: 1642058711.6712 - val_loss: 2165111041.7534\n","Epoch 340/1000\n","1168/1168 [==============================] - 0s 168us/step - loss: 1639863491.0685 - val_loss: 2159801472.0000\n","Epoch 341/1000\n","1168/1168 [==============================] - 0s 160us/step - loss: 1637874919.8904 - val_loss: 2165465235.2877\n","Epoch 342/1000\n","1168/1168 [==============================] - 0s 180us/step - loss: 1638797882.7397 - val_loss: 2163141693.3699\n","Epoch 343/1000\n","1168/1168 [==============================] - 0s 164us/step - loss: 1633243769.8630 - val_loss: 2159237204.1644\n","Epoch 344/1000\n","1168/1168 [==============================] - 0s 173us/step - loss: 1634751247.7808 - val_loss: 2157814750.6849\n","Epoch 345/1000\n","1168/1168 [==============================] - 0s 163us/step - loss: 1630006029.1507 - val_loss: 2156597465.4247\n","Epoch 346/1000\n","1168/1168 [==============================] - 0s 167us/step - loss: 1629024552.7671 - val_loss: 2154217827.9452\n","Epoch 347/1000\n","1168/1168 [==============================] - 0s 190us/step - loss: 1626991246.0274 - val_loss: 2157579525.2603\n","Epoch 348/1000\n","1168/1168 [==============================] - 0s 161us/step - loss: 1624622033.5342 - val_loss: 2154306928.2192\n","Epoch 349/1000\n","1168/1168 [==============================] - 0s 176us/step - loss: 1624231845.2603 - val_loss: 2155124290.6301\n","Epoch 350/1000\n","1168/1168 [==============================] - 0s 158us/step - loss: 1620776912.6575 - val_loss: 2151126019.5068\n","Epoch 351/1000\n","1168/1168 [==============================] - 0s 166us/step - loss: 1620785668.1644 - val_loss: 2151979435.8356\n","Epoch 352/1000\n","1168/1168 [==============================] - 0s 169us/step - loss: 1618399645.3699 - val_loss: 2148195133.3699\n","Epoch 353/1000\n","1168/1168 [==============================] - 0s 159us/step - loss: 1616694100.1644 - val_loss: 2148851655.8904\n","Epoch 354/1000\n","1168/1168 [==============================] - 0s 172us/step - loss: 1620031057.5342 - val_loss: 2142625199.3425\n","Epoch 355/1000\n","1168/1168 [==============================] - 0s 162us/step - loss: 1613702880.8767 - val_loss: 2146261225.2055\n","Epoch 356/1000\n","1168/1168 [==============================] - 0s 171us/step - loss: 1614742155.6164 - val_loss: 2142396484.3836\n","Epoch 357/1000\n","1168/1168 [==============================] - 0s 171us/step - loss: 1612030345.6438 - val_loss: 2144084146.8493\n","Epoch 358/1000\n","1168/1168 [==============================] - 0s 222us/step - loss: 1609109355.3973 - val_loss: 2143262599.0137\n","Epoch 359/1000\n","1168/1168 [==============================] - 0s 221us/step - loss: 1609762168.9863 - val_loss: 2140086347.3973\n","Epoch 360/1000\n","1168/1168 [==============================] - 0s 191us/step - loss: 1607581121.7534 - val_loss: 2137197461.0411\n","Epoch 361/1000\n","1168/1168 [==============================] - 0s 184us/step - loss: 1605537433.4247 - val_loss: 2138978395.1781\n","Epoch 362/1000\n","1168/1168 [==============================] - 0s 223us/step - loss: 1606809310.2466 - val_loss: 2146515697.9726\n","Epoch 363/1000\n","1168/1168 [==============================] - 0s 223us/step - loss: 1601979110.5753 - val_loss: 2138094870.7945\n","Epoch 364/1000\n","1168/1168 [==============================] - 0s 168us/step - loss: 1600915729.5342 - val_loss: 2135904113.9726\n","Epoch 365/1000\n","1168/1168 [==============================] - 0s 179us/step - loss: 1597841930.9589 - val_loss: 2133694050.1918\n","Epoch 366/1000\n","1168/1168 [==============================] - 0s 173us/step - loss: 1596422403.9452 - val_loss: 2134716198.5753\n","Epoch 367/1000\n","1168/1168 [==============================] - 0s 178us/step - loss: 1593592981.4795 - val_loss: 2132116837.6986\n","Epoch 368/1000\n","1168/1168 [==============================] - 0s 179us/step - loss: 1591689771.3973 - val_loss: 2131967891.2877\n","Epoch 369/1000\n","1168/1168 [==============================] - 0s 172us/step - loss: 1591935587.5068 - val_loss: 2135562678.3562\n","Epoch 370/1000\n","1168/1168 [==============================] - 0s 208us/step - loss: 1588903195.6164 - val_loss: 2130250674.8493\n","Epoch 371/1000\n","1168/1168 [==============================] - 0s 228us/step - loss: 1586738567.4521 - val_loss: 2128342301.8082\n","Epoch 372/1000\n","1168/1168 [==============================] - 0s 211us/step - loss: 1584369532.9315 - val_loss: 2128760642.6301\n","Epoch 373/1000\n","1168/1168 [==============================] - 0s 234us/step - loss: 1583048186.7397 - val_loss: 2127816058.7397\n","Epoch 374/1000\n","1168/1168 [==============================] - 0s 196us/step - loss: 1583107820.2740 - val_loss: 2123929905.0959\n","Epoch 375/1000\n","1168/1168 [==============================] - 0s 172us/step - loss: 1580220104.7671 - val_loss: 2123380997.2603\n","Epoch 376/1000\n","1168/1168 [==============================] - 0s 176us/step - loss: 1578843553.3151 - val_loss: 2121143695.7808\n","Epoch 377/1000\n","1168/1168 [==============================] - 0s 174us/step - loss: 1585359748.3836 - val_loss: 2122346220.7123\n","Epoch 378/1000\n","1168/1168 [==============================] - 0s 175us/step - loss: 1575524913.3151 - val_loss: 2120364316.0548\n","Epoch 379/1000\n","1168/1168 [==============================] - 0s 185us/step - loss: 1573577676.7123 - val_loss: 2120979022.9041\n","Epoch 380/1000\n","1168/1168 [==============================] - 0s 168us/step - loss: 1571160846.6849 - val_loss: 2115840978.4110\n","Epoch 381/1000\n","1168/1168 [==============================] - 0s 173us/step - loss: 1570447111.4521 - val_loss: 2121471833.4247\n","Epoch 382/1000\n","1168/1168 [==============================] - 0s 182us/step - loss: 1568839114.5205 - val_loss: 2116667812.8219\n","Epoch 383/1000\n","1168/1168 [==============================] - 0s 178us/step - loss: 1568621492.1644 - val_loss: 2115519046.1370\n","Epoch 384/1000\n","1168/1168 [==============================] - 0s 179us/step - loss: 1564118974.6849 - val_loss: 2113334491.1781\n","Epoch 385/1000\n","1168/1168 [==============================] - 0s 212us/step - loss: 1563492875.1781 - val_loss: 2115374916.3836\n","Epoch 386/1000\n","1168/1168 [==============================] - 0s 201us/step - loss: 1560102592.0000 - val_loss: 2112586830.9041\n","Epoch 387/1000\n","1168/1168 [==============================] - 0s 173us/step - loss: 1559225475.7260 - val_loss: 2113997283.9452\n","Epoch 388/1000\n","1168/1168 [==============================] - 0s 195us/step - loss: 1556745539.9452 - val_loss: 2111377751.6712\n","Epoch 389/1000\n","1168/1168 [==============================] - 0s 166us/step - loss: 1554648215.2329 - val_loss: 2111816660.1644\n","Epoch 390/1000\n","1168/1168 [==============================] - 0s 209us/step - loss: 1556263414.3562 - val_loss: 2108234753.7534\n","Epoch 391/1000\n","1168/1168 [==============================] - 0s 176us/step - loss: 1552240763.1781 - val_loss: 2108993948.0548\n","Epoch 392/1000\n","1168/1168 [==============================] - 0s 188us/step - loss: 1550551452.4932 - val_loss: 2110144641.7534\n","Epoch 393/1000\n","1168/1168 [==============================] - 0s 177us/step - loss: 1547983711.1233 - val_loss: 2109390416.6575\n","Epoch 394/1000\n","1168/1168 [==============================] - 0s 161us/step - loss: 1548150655.5616 - val_loss: 2105169979.6164\n","Epoch 395/1000\n","1168/1168 [==============================] - 0s 164us/step - loss: 1545782455.6712 - val_loss: 2108119871.1233\n","Epoch 396/1000\n","1168/1168 [==============================] - 0s 178us/step - loss: 1543648390.5753 - val_loss: 2104897029.2603\n","Epoch 397/1000\n","1168/1168 [==============================] - 0s 205us/step - loss: 1542548912.6575 - val_loss: 2105322965.9178\n","Epoch 398/1000\n","1168/1168 [==============================] - 0s 159us/step - loss: 1540534370.1918 - val_loss: 2104877615.3425\n","Epoch 399/1000\n","1168/1168 [==============================] - 0s 162us/step - loss: 1543110296.9863 - val_loss: 2102396286.2466\n","Epoch 400/1000\n","1168/1168 [==============================] - 0s 164us/step - loss: 1537579403.3973 - val_loss: 2103818662.5753\n","Epoch 401/1000\n","1168/1168 [==============================] - 0s 161us/step - loss: 1538179782.5753 - val_loss: 2100793761.3151\n","Epoch 402/1000\n","1168/1168 [==============================] - 0s 179us/step - loss: 1537278722.6301 - val_loss: 2100466232.1096\n","Epoch 403/1000\n","1168/1168 [==============================] - 0s 197us/step - loss: 1534271740.9315 - val_loss: 2100812152.9863\n","Epoch 404/1000\n","1168/1168 [==============================] - 0s 222us/step - loss: 1532131859.9452 - val_loss: 2098763455.1233\n","Epoch 405/1000\n","1168/1168 [==============================] - 0s 222us/step - loss: 1536025624.9863 - val_loss: 2100381506.6301\n","Epoch 406/1000\n","1168/1168 [==============================] - 0s 213us/step - loss: 1532193445.2603 - val_loss: 2101373324.2740\n","Epoch 407/1000\n","1168/1168 [==============================] - 0s 189us/step - loss: 1527959945.2055 - val_loss: 2099875655.8904\n","Epoch 408/1000\n","1168/1168 [==============================] - 0s 211us/step - loss: 1526988768.0000 - val_loss: 2098540126.6849\n","Epoch 409/1000\n","1168/1168 [==============================] - 0s 196us/step - loss: 1524439278.9041 - val_loss: 2096672683.8356\n","Epoch 410/1000\n","1168/1168 [==============================] - 0s 163us/step - loss: 1523898049.7534 - val_loss: 2097097515.8356\n","Epoch 411/1000\n","1168/1168 [==============================] - 0s 189us/step - loss: 1521463686.5753 - val_loss: 2094567662.4658\n","Epoch 412/1000\n","1168/1168 [==============================] - 0s 216us/step - loss: 1520035561.2055 - val_loss: 2094064238.4658\n","Epoch 413/1000\n","1168/1168 [==============================] - 0s 211us/step - loss: 1518466334.6849 - val_loss: 2093489879.6712\n","Epoch 414/1000\n","1168/1168 [==============================] - 0s 166us/step - loss: 1517876036.8219 - val_loss: 2093190435.0685\n","Epoch 415/1000\n","1168/1168 [==============================] - 0s 174us/step - loss: 1517611366.5753 - val_loss: 2092380757.9178\n","Epoch 416/1000\n","1168/1168 [==============================] - 0s 167us/step - loss: 1515467155.0685 - val_loss: 2093337347.5068\n","Epoch 417/1000\n","1168/1168 [==============================] - 0s 160us/step - loss: 1513744711.6712 - val_loss: 2092315086.9041\n","Epoch 418/1000\n","1168/1168 [==============================] - 0s 161us/step - loss: 1516592096.8767 - val_loss: 2090074248.7671\n","Epoch 419/1000\n","1168/1168 [==============================] - 0s 156us/step - loss: 1511488991.1233 - val_loss: 2091298561.7534\n","Epoch 420/1000\n","1168/1168 [==============================] - 0s 184us/step - loss: 1509600455.6712 - val_loss: 2089433778.8493\n","Epoch 421/1000\n","1168/1168 [==============================] - 0s 190us/step - loss: 1508248022.3562 - val_loss: 2089630930.4110\n","Epoch 422/1000\n","1168/1168 [==============================] - 0s 156us/step - loss: 1509469106.8493 - val_loss: 2089547036.0548\n","Epoch 423/1000\n","1168/1168 [==============================] - 0s 165us/step - loss: 1506716903.4521 - val_loss: 2087782976.8767\n","Epoch 424/1000\n","1168/1168 [==============================] - 0s 166us/step - loss: 1504581923.9452 - val_loss: 2088946395.1781\n","Epoch 425/1000\n","1168/1168 [==============================] - 0s 178us/step - loss: 1503853416.3288 - val_loss: 2087029412.8219\n","Epoch 426/1000\n","1168/1168 [==============================] - 0s 190us/step - loss: 1505016311.6712 - val_loss: 2087065999.7808\n","Epoch 427/1000\n","1168/1168 [==============================] - 0s 164us/step - loss: 1503568629.9178 - val_loss: 2088419419.1781\n","Epoch 428/1000\n","1168/1168 [==============================] - 0s 158us/step - loss: 1501256809.4247 - val_loss: 2085396771.0685\n","Epoch 429/1000\n","1168/1168 [==============================] - 0s 159us/step - loss: 1497291561.6438 - val_loss: 2087234186.5205\n","Epoch 430/1000\n","1168/1168 [==============================] - 0s 160us/step - loss: 1500077172.1644 - val_loss: 2086948979.7260\n","Epoch 431/1000\n","1168/1168 [==============================] - 0s 181us/step - loss: 1495825088.0000 - val_loss: 2085235392.8767\n","Epoch 432/1000\n","1168/1168 [==============================] - 0s 152us/step - loss: 1496344096.2192 - val_loss: 2083884579.0685\n","Epoch 433/1000\n","1168/1168 [==============================] - 0s 164us/step - loss: 1492680506.3014 - val_loss: 2084245623.2329\n","Epoch 434/1000\n","1168/1168 [==============================] - 0s 161us/step - loss: 1494157011.2877 - val_loss: 2082778823.8904\n","Epoch 435/1000\n","1168/1168 [==============================] - 0s 170us/step - loss: 1491037581.5890 - val_loss: 2082942749.8082\n","Epoch 436/1000\n","1168/1168 [==============================] - 0s 201us/step - loss: 1490432582.5753 - val_loss: 2082724792.1096\n","Epoch 437/1000\n","1168/1168 [==============================] - 0s 167us/step - loss: 1489929756.4932 - val_loss: 2082706887.8904\n","Epoch 438/1000\n","1168/1168 [==============================] - 0s 163us/step - loss: 1489840309.0411 - val_loss: 2081641072.2192\n","Epoch 439/1000\n","1168/1168 [==============================] - 0s 162us/step - loss: 1487824758.3562 - val_loss: 2081934262.3562\n","Epoch 440/1000\n","1168/1168 [==============================] - 0s 161us/step - loss: 1486098446.9041 - val_loss: 2081152710.1370\n","Epoch 441/1000\n","1168/1168 [==============================] - 0s 180us/step - loss: 1486503236.8219 - val_loss: 2080454059.8356\n","Epoch 442/1000\n","1168/1168 [==============================] - 0s 177us/step - loss: 1484113062.1370 - val_loss: 2081391089.9726\n","Epoch 443/1000\n","1168/1168 [==============================] - 0s 159us/step - loss: 1483452598.3562 - val_loss: 2081273030.1370\n","Epoch 444/1000\n","1168/1168 [==============================] - 0s 152us/step - loss: 1481969749.0411 - val_loss: 2079890121.6438\n","Epoch 445/1000\n","1168/1168 [==============================] - 0s 162us/step - loss: 1481966475.8356 - val_loss: 2079491121.0959\n","Epoch 446/1000\n","1168/1168 [==============================] - 0s 181us/step - loss: 1479565812.6027 - val_loss: 2078791108.3836\n","Epoch 447/1000\n","1168/1168 [==============================] - 0s 173us/step - loss: 1477957146.7397 - val_loss: 2078697766.5753\n","Epoch 448/1000\n","1168/1168 [==============================] - 0s 167us/step - loss: 1478230302.2466 - val_loss: 2078997179.6164\n","Epoch 449/1000\n","1168/1168 [==============================] - 0s 159us/step - loss: 1477729184.0000 - val_loss: 2078394380.2740\n","Epoch 450/1000\n","1168/1168 [==============================] - 0s 166us/step - loss: 1475710060.7123 - val_loss: 2078526727.0137\n","Epoch 451/1000\n","1168/1168 [==============================] - 0s 167us/step - loss: 1474355220.1644 - val_loss: 2077835653.2603\n","Epoch 452/1000\n","1168/1168 [==============================] - 0s 176us/step - loss: 1473210072.1096 - val_loss: 2077744313.8630\n","Epoch 453/1000\n","1168/1168 [==============================] - 0s 173us/step - loss: 1476553761.7534 - val_loss: 2078739710.2466\n","Epoch 454/1000\n","1168/1168 [==============================] - 0s 187us/step - loss: 1475751131.1781 - val_loss: 2078414108.0548\n","Epoch 455/1000\n","1168/1168 [==============================] - 0s 180us/step - loss: 1471775268.1644 - val_loss: 2077406690.1918\n","Epoch 456/1000\n","1168/1168 [==============================] - 0s 170us/step - loss: 1472292230.1370 - val_loss: 2077174203.6164\n","Epoch 457/1000\n","1168/1168 [==============================] - 0s 173us/step - loss: 1468226993.9726 - val_loss: 2076281675.3973\n","Epoch 458/1000\n","1168/1168 [==============================] - 0s 161us/step - loss: 1473178126.0274 - val_loss: 2076879156.6027\n","Epoch 459/1000\n","1168/1168 [==============================] - 0s 165us/step - loss: 1467060194.6301 - val_loss: 2076171481.4247\n","Epoch 460/1000\n","1168/1168 [==============================] - 0s 164us/step - loss: 1465682282.9589 - val_loss: 2075563302.5753\n","Epoch 461/1000\n","1168/1168 [==============================] - 0s 162us/step - loss: 1467707500.7123 - val_loss: 2075024622.4658\n","Epoch 462/1000\n","1168/1168 [==============================] - 0s 164us/step - loss: 1464975980.9315 - val_loss: 2075281739.3973\n","Epoch 463/1000\n","1168/1168 [==============================] - 0s 172us/step - loss: 1461990480.6575 - val_loss: 2076358184.3288\n","Epoch 464/1000\n","1168/1168 [==============================] - 0s 163us/step - loss: 1464013077.9178 - val_loss: 2074989764.3836\n","Epoch 465/1000\n","1168/1168 [==============================] - 0s 157us/step - loss: 1465142192.2192 - val_loss: 2076203898.7397\n","Epoch 466/1000\n","1168/1168 [==============================] - 0s 167us/step - loss: 1460068648.3288 - val_loss: 2074727281.9726\n","Epoch 467/1000\n","1168/1168 [==============================] - 0s 173us/step - loss: 1459138929.7534 - val_loss: 2074583383.6712\n","Epoch 468/1000\n","1168/1168 [==============================] - 0s 173us/step - loss: 1459320663.6712 - val_loss: 2074733760.8767\n","Epoch 469/1000\n","1168/1168 [==============================] - 0s 160us/step - loss: 1458513498.3014 - val_loss: 2074781862.5753\n","Epoch 470/1000\n","1168/1168 [==============================] - 0s 160us/step - loss: 1456478989.1507 - val_loss: 2074648102.5753\n","Epoch 471/1000\n","1168/1168 [==============================] - 0s 168us/step - loss: 1457542308.3836 - val_loss: 2075209722.7397\n","Epoch 472/1000\n","1168/1168 [==============================] - 0s 176us/step - loss: 1456391939.9452 - val_loss: 2075025599.1233\n","Epoch 473/1000\n","1168/1168 [==============================] - 0s 176us/step - loss: 1454821306.3014 - val_loss: 2074103087.3425\n","Epoch 474/1000\n","1168/1168 [==============================] - 0s 164us/step - loss: 1456339367.2329 - val_loss: 2078175517.8082\n","Epoch 475/1000\n","1168/1168 [==============================] - 0s 163us/step - loss: 1454650987.8356 - val_loss: 2075628987.6164\n","Epoch 476/1000\n","1168/1168 [==============================] - 0s 206us/step - loss: 1454081370.7397 - val_loss: 2074061154.1918\n","Epoch 477/1000\n","1168/1168 [==============================] - 0s 175us/step - loss: 1452940242.6301 - val_loss: 2074423697.5342\n","Epoch 478/1000\n","1168/1168 [==============================] - 0s 173us/step - loss: 1453444482.6301 - val_loss: 2074924091.6164\n","Epoch 479/1000\n","1168/1168 [==============================] - 0s 164us/step - loss: 1455740642.6301 - val_loss: 2075489569.3151\n","Epoch 480/1000\n","1168/1168 [==============================] - 0s 162us/step - loss: 1450726103.2329 - val_loss: 2074913294.0274\n","Epoch 481/1000\n","1168/1168 [==============================] - 0s 173us/step - loss: 1451340721.9726 - val_loss: 2074445285.6986\n","Epoch 482/1000\n","1168/1168 [==============================] - 0s 165us/step - loss: 1447884011.8356 - val_loss: 2073943194.3014\n","Epoch 483/1000\n","1168/1168 [==============================] - 0s 182us/step - loss: 1449053382.3562 - val_loss: 2074582191.3425\n","Epoch 484/1000\n","1168/1168 [==============================] - 0s 164us/step - loss: 1447095830.2466 - val_loss: 2074468872.7671\n","Epoch 485/1000\n","1168/1168 [==============================] - 0s 161us/step - loss: 1446637665.7534 - val_loss: 2074299132.4932\n","Epoch 486/1000\n","1168/1168 [==============================] - 0s 188us/step - loss: 1445676073.6438 - val_loss: 2074149968.6575\n","Epoch 487/1000\n","1168/1168 [==============================] - 0s 170us/step - loss: 1445600401.0959 - val_loss: 2074637611.8356\n","Epoch 488/1000\n","1168/1168 [==============================] - 0s 180us/step - loss: 1445443082.9589 - val_loss: 2074588352.8767\n","Epoch 489/1000\n","1168/1168 [==============================] - 0s 165us/step - loss: 1448343425.3151 - val_loss: 2073884103.8904\n","Epoch 490/1000\n","1168/1168 [==============================] - 0s 166us/step - loss: 1442508082.6301 - val_loss: 2074778741.4795\n","Epoch 491/1000\n","1168/1168 [==============================] - 0s 166us/step - loss: 1444117730.6301 - val_loss: 2074719628.2740\n","Epoch 492/1000\n","1168/1168 [==============================] - 0s 166us/step - loss: 1444070192.2192 - val_loss: 2075830685.8082\n","Epoch 493/1000\n","1168/1168 [==============================] - 0s 163us/step - loss: 1441156854.3562 - val_loss: 2074164881.5342\n","Epoch 494/1000\n","1168/1168 [==============================] - 0s 173us/step - loss: 1444541306.9589 - val_loss: 2074285662.6849\n","Epoch 495/1000\n","1168/1168 [==============================] - 0s 165us/step - loss: 1442301291.3973 - val_loss: 2074243694.4658\n","Epoch 496/1000\n","1168/1168 [==============================] - 0s 166us/step - loss: 1441018461.3699 - val_loss: 2074484862.2466\n","Epoch 497/1000\n","1168/1168 [==============================] - 0s 171us/step - loss: 1440483965.5890 - val_loss: 2076136314.7397\n","Epoch 498/1000\n","1168/1168 [==============================] - 0s 166us/step - loss: 1441154246.1370 - val_loss: 2073885303.2329\n","Epoch 499/1000\n","1168/1168 [==============================] - 0s 171us/step - loss: 1438051258.0822 - val_loss: 2075150064.2192\n","Epoch 500/1000\n","1168/1168 [==============================] - 0s 169us/step - loss: 1436540693.4795 - val_loss: 2073501359.3425\n","Epoch 501/1000\n","1168/1168 [==============================] - 0s 169us/step - loss: 1437508978.4110 - val_loss: 2074107535.7808\n","Epoch 502/1000\n","1168/1168 [==============================] - 0s 211us/step - loss: 1438332075.8356 - val_loss: 2075194140.0548\n","Epoch 503/1000\n","1168/1168 [==============================] - 0s 208us/step - loss: 1436729023.3425 - val_loss: 2074002121.6438\n","Epoch 504/1000\n","1168/1168 [==============================] - 0s 189us/step - loss: 1435130443.3973 - val_loss: 2073617516.7123\n","Epoch 505/1000\n","1168/1168 [==============================] - 0s 161us/step - loss: 1433665122.1918 - val_loss: 2073395897.8630\n","Epoch 506/1000\n","1168/1168 [==============================] - 0s 177us/step - loss: 1433430857.2055 - val_loss: 2073593724.4932\n","Epoch 507/1000\n","1168/1168 [==============================] - 0s 184us/step - loss: 1433306041.6438 - val_loss: 2073954226.8493\n","Epoch 508/1000\n","1168/1168 [==============================] - 0s 163us/step - loss: 1435017519.7808 - val_loss: 2073928968.7671\n","Epoch 509/1000\n","1168/1168 [==============================] - 0s 173us/step - loss: 1431947642.5205 - val_loss: 2074159980.7123\n","Epoch 510/1000\n","1168/1168 [==============================] - 0s 163us/step - loss: 1431946135.8904 - val_loss: 2074931021.1507\n","Epoch 511/1000\n","1168/1168 [==============================] - 0s 167us/step - loss: 1433535295.1233 - val_loss: 2074316400.2192\n","Epoch 512/1000\n","1168/1168 [==============================] - 0s 165us/step - loss: 1431797348.8219 - val_loss: 2073277440.0000\n","Epoch 513/1000\n","1168/1168 [==============================] - 0s 166us/step - loss: 1433367869.3699 - val_loss: 2072454969.8630\n","Epoch 514/1000\n","1168/1168 [==============================] - 0s 173us/step - loss: 1433578119.2329 - val_loss: 2072706062.0274\n","Epoch 515/1000\n","1168/1168 [==============================] - 0s 157us/step - loss: 1431255749.0411 - val_loss: 2074380445.8082\n","Epoch 516/1000\n","1168/1168 [==============================] - 0s 164us/step - loss: 1431922201.4247 - val_loss: 2073057325.5890\n","Epoch 517/1000\n","1168/1168 [==============================] - 0s 172us/step - loss: 1428241625.8630 - val_loss: 2074266706.4110\n","Epoch 518/1000\n","1168/1168 [==============================] - 0s 162us/step - loss: 1428989070.4658 - val_loss: 2074365657.4247\n","Epoch 519/1000\n","1168/1168 [==============================] - 0s 170us/step - loss: 1427326877.8082 - val_loss: 2074913325.5890\n","Epoch 520/1000\n","1168/1168 [==============================] - 0s 161us/step - loss: 1427498135.6712 - val_loss: 2073044579.9452\n","Epoch 521/1000\n","1168/1168 [==============================] - 0s 164us/step - loss: 1426101630.2466 - val_loss: 2073458538.9589\n","Epoch 522/1000\n","1168/1168 [==============================] - 0s 166us/step - loss: 1426873745.0959 - val_loss: 2073850559.1233\n","Epoch 523/1000\n","1168/1168 [==============================] - 0s 160us/step - loss: 1425537591.2329 - val_loss: 2074825091.5068\n","Epoch 524/1000\n","1168/1168 [==============================] - 0s 159us/step - loss: 1424738002.4110 - val_loss: 2073816961.7534\n","Epoch 525/1000\n","1168/1168 [==============================] - 0s 173us/step - loss: 1426760937.6438 - val_loss: 2077148899.9452\n","Epoch 526/1000\n","1168/1168 [==============================] - 0s 163us/step - loss: 1424722247.0137 - val_loss: 2073389589.0411\n","Epoch 527/1000\n","1168/1168 [==============================] - 0s 152us/step - loss: 1423101141.9178 - val_loss: 2074559863.2329\n","Epoch 528/1000\n","1168/1168 [==============================] - 0s 167us/step - loss: 1422091855.7808 - val_loss: 2073839007.5616\n","Epoch 529/1000\n","1168/1168 [==============================] - 0s 167us/step - loss: 1425827605.0411 - val_loss: 2073348311.6712\n","Epoch 530/1000\n","1168/1168 [==============================] - 0s 174us/step - loss: 1423317214.2466 - val_loss: 2073394295.2329\n","Epoch 531/1000\n","1168/1168 [==============================] - 0s 160us/step - loss: 1422762289.0959 - val_loss: 2077325859.0685\n","Epoch 532/1000\n","1168/1168 [==============================] - 0s 163us/step - loss: 1422481982.6849 - val_loss: 2072232030.6849\n","Epoch 533/1000\n","1168/1168 [==============================] - 0s 176us/step - loss: 1422410211.9452 - val_loss: 2073006455.2329\n","Epoch 534/1000\n","1168/1168 [==============================] - 0s 206us/step - loss: 1421347237.2603 - val_loss: 2074851024.6575\n","Epoch 535/1000\n","1168/1168 [==============================] - 0s 176us/step - loss: 1419711689.8630 - val_loss: 2073526568.3288\n","Epoch 536/1000\n","1168/1168 [==============================] - 0s 166us/step - loss: 1419864456.7671 - val_loss: 2072929607.8904\n","Epoch 537/1000\n","1168/1168 [==============================] - 0s 163us/step - loss: 1424240041.6438 - val_loss: 2083615260.0548\n","Epoch 538/1000\n","1168/1168 [==============================] - 0s 164us/step - loss: 1421862053.6986 - val_loss: 2073239008.4384\n","Epoch 539/1000\n","1168/1168 [==============================] - 0s 168us/step - loss: 1420670439.0137 - val_loss: 2072801614.9041\n","Epoch 540/1000\n","1168/1168 [==============================] - 0s 190us/step - loss: 1416982778.0822 - val_loss: 2073600173.5890\n","Epoch 541/1000\n","1168/1168 [==============================] - 0s 170us/step - loss: 1418591284.6027 - val_loss: 2072525620.6027\n","Epoch 542/1000\n","1168/1168 [==============================] - 0s 209us/step - loss: 1417240091.1781 - val_loss: 2075872515.5068\n","Epoch 543/1000\n","1168/1168 [==============================] - 0s 211us/step - loss: 1418320004.8219 - val_loss: 2074004495.7808\n","Epoch 544/1000\n","1168/1168 [==============================] - 0s 192us/step - loss: 1416566939.6164 - val_loss: 2074888528.6575\n","Epoch 545/1000\n","1168/1168 [==============================] - 0s 181us/step - loss: 1414756119.6712 - val_loss: 2072999480.1096\n","Epoch 546/1000\n","1168/1168 [==============================] - 0s 175us/step - loss: 1415054034.4110 - val_loss: 2074429518.9041\n","Epoch 547/1000\n","1168/1168 [==============================] - 0s 167us/step - loss: 1415324977.9726 - val_loss: 2071856173.5890\n","Epoch 548/1000\n","1168/1168 [==============================] - 0s 163us/step - loss: 1415903649.9726 - val_loss: 2074267604.1644\n","Epoch 549/1000\n","1168/1168 [==============================] - 0s 168us/step - loss: 1414392978.4110 - val_loss: 2073815083.8356\n","Epoch 550/1000\n","1168/1168 [==============================] - 0s 201us/step - loss: 1413059232.6575 - val_loss: 2075919473.9726\n","Epoch 551/1000\n","1168/1168 [==============================] - 0s 174us/step - loss: 1417739263.5616 - val_loss: 2074330339.9452\n","Epoch 552/1000\n","1168/1168 [==============================] - 0s 170us/step - loss: 1413436688.2192 - val_loss: 2073272828.4932\n","Epoch 553/1000\n","1168/1168 [==============================] - 0s 176us/step - loss: 1413695195.6164 - val_loss: 2072259017.6438\n","Epoch 554/1000\n","1168/1168 [==============================] - 0s 166us/step - loss: 1413353546.5205 - val_loss: 2073754697.6438\n","Epoch 555/1000\n","1168/1168 [==============================] - 0s 183us/step - loss: 1414893031.2329 - val_loss: 2075900933.2603\n","Epoch 556/1000\n","1168/1168 [==============================] - 0s 192us/step - loss: 1412012929.7534 - val_loss: 2075502593.7534\n","Epoch 557/1000\n","1168/1168 [==============================] - 0s 227us/step - loss: 1412498296.9863 - val_loss: 2076896126.2466\n","Epoch 558/1000\n","1168/1168 [==============================] - 0s 187us/step - loss: 1411568472.9863 - val_loss: 2072654534.1370\n","Epoch 559/1000\n","1168/1168 [==============================] - 0s 163us/step - loss: 1412742666.5205 - val_loss: 2076892668.4932\n","Epoch 560/1000\n","1168/1168 [==============================] - 0s 214us/step - loss: 1409026797.5890 - val_loss: 2072703324.9315\n","Epoch 561/1000\n","1168/1168 [==============================] - 0s 165us/step - loss: 1411508459.3973 - val_loss: 2072369292.2740\n","Epoch 562/1000\n","1168/1168 [==============================] - 0s 164us/step - loss: 1408198548.1644 - val_loss: 2074074518.7945\n","Epoch 563/1000\n","1168/1168 [==============================] - 0s 170us/step - loss: 1409142774.3562 - val_loss: 2074633047.6712\n","Epoch 564/1000\n","1168/1168 [==============================] - 0s 163us/step - loss: 1407671665.3151 - val_loss: 2075393022.2466\n","Epoch 565/1000\n","1168/1168 [==============================] - 0s 193us/step - loss: 1412236394.0822 - val_loss: 2071379028.1644\n","Epoch 566/1000\n","1168/1168 [==============================] - 0s 167us/step - loss: 1406861356.2740 - val_loss: 2076137647.3425\n","Epoch 567/1000\n","1168/1168 [==============================] - 0s 165us/step - loss: 1405535712.4384 - val_loss: 2072417367.6712\n","Epoch 568/1000\n","1168/1168 [==============================] - 0s 163us/step - loss: 1405747450.7397 - val_loss: 2074801639.4521\n","Epoch 569/1000\n","1168/1168 [==============================] - 0s 162us/step - loss: 1405463351.2329 - val_loss: 2073290417.0959\n","Epoch 570/1000\n","1168/1168 [==============================] - 0s 179us/step - loss: 1404939360.8767 - val_loss: 2073180445.8082\n","Epoch 571/1000\n","1168/1168 [==============================] - 0s 174us/step - loss: 1404687935.5616 - val_loss: 2072673792.0000\n","Epoch 572/1000\n","1168/1168 [==============================] - 0s 162us/step - loss: 1404884104.1096 - val_loss: 2072404718.4658\n","Epoch 573/1000\n","1168/1168 [==============================] - 0s 164us/step - loss: 1404037163.8356 - val_loss: 2072890988.7123\n","Epoch 574/1000\n","1168/1168 [==============================] - 0s 203us/step - loss: 1403552217.2055 - val_loss: 2076732936.7671\n","Epoch 575/1000\n","1168/1168 [==============================] - 0s 189us/step - loss: 1404367009.7534 - val_loss: 2072870485.9178\n","Epoch 576/1000\n","1168/1168 [==============================] - 0s 168us/step - loss: 1405350557.3699 - val_loss: 2076392870.5753\n","Epoch 577/1000\n","1168/1168 [==============================] - 0s 164us/step - loss: 1403123177.2055 - val_loss: 2076831736.9863\n","Epoch 578/1000\n","1168/1168 [==============================] - 0s 166us/step - loss: 1402671503.3425 - val_loss: 2077906021.6986\n","Epoch 579/1000\n","1168/1168 [==============================] - 0s 161us/step - loss: 1403232112.6575 - val_loss: 2077863255.6712\n","Epoch 580/1000\n","1168/1168 [==============================] - 0s 185us/step - loss: 1402812339.5068 - val_loss: 2075533764.3836\n","Epoch 581/1000\n","1168/1168 [==============================] - 0s 164us/step - loss: 1404840858.9589 - val_loss: 2077201920.0000\n","Epoch 582/1000\n","1168/1168 [==============================] - 0s 167us/step - loss: 1400664058.3014 - val_loss: 2073279284.6027\n","Epoch 583/1000\n","1168/1168 [==============================] - 0s 162us/step - loss: 1400287146.0822 - val_loss: 2077645245.3699\n","Epoch 584/1000\n","1168/1168 [==============================] - 0s 170us/step - loss: 1400314355.7260 - val_loss: 2071735529.2055\n","Epoch 585/1000\n","1168/1168 [==============================] - 0s 180us/step - loss: 1404736924.0548 - val_loss: 2073692959.5616\n","Epoch 586/1000\n","1168/1168 [==============================] - 0s 161us/step - loss: 1403311427.5068 - val_loss: 2085857001.2055\n","Epoch 587/1000\n","1168/1168 [==============================] - 0s 165us/step - loss: 1398051827.7260 - val_loss: 2073375700.1644\n","Epoch 588/1000\n","1168/1168 [==============================] - 0s 166us/step - loss: 1399442936.9863 - val_loss: 2077245587.2877\n","Epoch 589/1000\n","1168/1168 [==============================] - 0s 169us/step - loss: 1397732719.3425 - val_loss: 2071308156.4932\n","Epoch 590/1000\n","1168/1168 [==============================] - 0s 166us/step - loss: 1400618233.4247 - val_loss: 2073167263.5616\n","Epoch 591/1000\n","1168/1168 [==============================] - 0s 176us/step - loss: 1396478642.8493 - val_loss: 2073692747.3973\n","Epoch 592/1000\n","1168/1168 [==============================] - 0s 164us/step - loss: 1397647038.2466 - val_loss: 2075294632.3288\n","Epoch 593/1000\n","1168/1168 [==============================] - 0s 161us/step - loss: 1394966510.9041 - val_loss: 2074743630.9041\n","Epoch 594/1000\n","1168/1168 [==============================] - 0s 162us/step - loss: 1395032311.2329 - val_loss: 2074215548.4932\n","Epoch 595/1000\n","1168/1168 [==============================] - 0s 177us/step - loss: 1394354801.0959 - val_loss: 2072222234.3014\n","Epoch 596/1000\n","1168/1168 [==============================] - 0s 179us/step - loss: 1393298017.7534 - val_loss: 2076665917.3699\n","Epoch 597/1000\n","1168/1168 [==============================] - 0s 167us/step - loss: 1394045116.4932 - val_loss: 2075176900.3836\n","Epoch 598/1000\n","1168/1168 [==============================] - 0s 163us/step - loss: 1394444877.1507 - val_loss: 2073513661.3699\n","Epoch 599/1000\n","1168/1168 [==============================] - 0s 161us/step - loss: 1391684125.1507 - val_loss: 2074402340.8219\n","Epoch 600/1000\n","1168/1168 [==============================] - 0s 166us/step - loss: 1394639885.5890 - val_loss: 2073349174.3562\n","Epoch 601/1000\n","1168/1168 [==============================] - 0s 166us/step - loss: 1392606264.1096 - val_loss: 2076012584.3288\n","Epoch 602/1000\n","1168/1168 [==============================] - 0s 161us/step - loss: 1397546655.1233 - val_loss: 2070829355.8356\n","Epoch 603/1000\n","1168/1168 [==============================] - 0s 164us/step - loss: 1391232363.8356 - val_loss: 2078558716.4932\n","Epoch 604/1000\n","1168/1168 [==============================] - 0s 158us/step - loss: 1391547161.4247 - val_loss: 2077756531.7260\n","Epoch 605/1000\n","1168/1168 [==============================] - 0s 168us/step - loss: 1389690437.2603 - val_loss: 2075388528.2192\n","Epoch 606/1000\n","1168/1168 [==============================] - 0s 167us/step - loss: 1389742793.2055 - val_loss: 2075269612.7123\n","Epoch 607/1000\n","1168/1168 [==============================] - 0s 178us/step - loss: 1391834245.2603 - val_loss: 2075464221.8082\n","Epoch 608/1000\n","1168/1168 [==============================] - 0s 172us/step - loss: 1389243185.5342 - val_loss: 2072997525.0411\n","Epoch 609/1000\n","1168/1168 [==============================] - 0s 169us/step - loss: 1392956733.8082 - val_loss: 2080352924.0548\n","Epoch 610/1000\n","1168/1168 [==============================] - 0s 163us/step - loss: 1388555083.3973 - val_loss: 2075809046.7945\n","Epoch 611/1000\n","1168/1168 [==============================] - 0s 164us/step - loss: 1388183292.4932 - val_loss: 2074236217.8630\n","Epoch 612/1000\n","1168/1168 [==============================] - 0s 176us/step - loss: 1387415441.3151 - val_loss: 2075072547.0685\n","Epoch 613/1000\n","1168/1168 [==============================] - 0s 159us/step - loss: 1390635868.9315 - val_loss: 2079757169.9726\n","Epoch 614/1000\n","1168/1168 [==============================] - 0s 164us/step - loss: 1390434343.0137 - val_loss: 2083547158.7945\n","Epoch 615/1000\n","1168/1168 [==============================] - 0s 167us/step - loss: 1385008581.6986 - val_loss: 2075228517.6986\n","Epoch 616/1000\n","1168/1168 [==============================] - 0s 160us/step - loss: 1386173501.8082 - val_loss: 2075988981.4795\n","Epoch 617/1000\n","1168/1168 [==============================] - 0s 172us/step - loss: 1384952706.6301 - val_loss: 2074204244.1644\n","Epoch 618/1000\n","1168/1168 [==============================] - 0s 172us/step - loss: 1385250049.7534 - val_loss: 2074299521.7534\n","Epoch 619/1000\n","1168/1168 [==============================] - 0s 164us/step - loss: 1386123769.2055 - val_loss: 2077074710.7945\n","Epoch 620/1000\n","1168/1168 [==============================] - 0s 159us/step - loss: 1384243893.0411 - val_loss: 2076006810.3014\n","Epoch 621/1000\n","1168/1168 [==============================] - 0s 165us/step - loss: 1383654844.2740 - val_loss: 2074893513.6438\n","Epoch 622/1000\n","1168/1168 [==============================] - 0s 192us/step - loss: 1385148132.8219 - val_loss: 2079212673.7534\n","Epoch 623/1000\n","1168/1168 [==============================] - 0s 171us/step - loss: 1383477602.1918 - val_loss: 2079213629.3699\n","Epoch 624/1000\n","1168/1168 [==============================] - 0s 163us/step - loss: 1383603037.8082 - val_loss: 2075979840.8767\n","Epoch 625/1000\n","1168/1168 [==============================] - 0s 161us/step - loss: 1385988647.0137 - val_loss: 2074047500.2740\n","Epoch 626/1000\n","1168/1168 [==============================] - 0s 160us/step - loss: 1381405757.3699 - val_loss: 2079562352.2192\n","Epoch 627/1000\n","1168/1168 [==============================] - 0s 170us/step - loss: 1382046280.1096 - val_loss: 2075060136.3288\n","Epoch 628/1000\n","1168/1168 [==============================] - 0s 178us/step - loss: 1381785308.4932 - val_loss: 2083443720.7671\n","Epoch 629/1000\n","1168/1168 [==============================] - 0s 167us/step - loss: 1381575579.1781 - val_loss: 2078505095.0137\n","Epoch 630/1000\n","1168/1168 [==============================] - 0s 169us/step - loss: 1381753851.8356 - val_loss: 2077817193.2055\n","Epoch 631/1000\n","1168/1168 [==============================] - 0s 167us/step - loss: 1381496866.6301 - val_loss: 2079618677.4795\n","Epoch 632/1000\n","1168/1168 [==============================] - 0s 162us/step - loss: 1383180970.5205 - val_loss: 2081243258.7397\n","Epoch 633/1000\n","1168/1168 [==============================] - 0s 184us/step - loss: 1380571519.4521 - val_loss: 2077577992.7671\n","Epoch 634/1000\n","1168/1168 [==============================] - 0s 163us/step - loss: 1379539064.9863 - val_loss: 2077963872.4384\n","Epoch 635/1000\n","1168/1168 [==============================] - 0s 159us/step - loss: 1382885009.7534 - val_loss: 2081779480.5479\n","Epoch 636/1000\n","1168/1168 [==============================] - 0s 193us/step - loss: 1378956263.2329 - val_loss: 2075405036.7123\n","Epoch 637/1000\n","1168/1168 [==============================] - 0s 207us/step - loss: 1378583862.3562 - val_loss: 2078576839.8904\n","Epoch 638/1000\n","1168/1168 [==============================] - 0s 215us/step - loss: 1377897325.1507 - val_loss: 2080809121.3151\n","Epoch 639/1000\n","1168/1168 [==============================] - 0s 166us/step - loss: 1379873956.3836 - val_loss: 2075039603.7260\n","Epoch 640/1000\n","1168/1168 [==============================] - 0s 192us/step - loss: 1377222751.5616 - val_loss: 2084413294.4658\n","Epoch 641/1000\n","1168/1168 [==============================] - 0s 172us/step - loss: 1379144123.6164 - val_loss: 2079465584.2192\n","Epoch 642/1000\n","1168/1168 [==============================] - 0s 219us/step - loss: 1383405922.6301 - val_loss: 2076183846.5753\n","Epoch 643/1000\n","1168/1168 [==============================] - 0s 175us/step - loss: 1377844354.8493 - val_loss: 2080232732.0548\n","Epoch 644/1000\n","1168/1168 [==============================] - 0s 161us/step - loss: 1376525326.0274 - val_loss: 2077730826.5205\n","Epoch 645/1000\n","1168/1168 [==============================] - 0s 169us/step - loss: 1376057366.7945 - val_loss: 2076713019.6164\n","Epoch 646/1000\n","1168/1168 [==============================] - 0s 161us/step - loss: 1378685602.1918 - val_loss: 2078161211.6164\n","Epoch 647/1000\n","1168/1168 [==============================] - 0s 171us/step - loss: 1378069764.3836 - val_loss: 2079557461.9178\n","Epoch 648/1000\n","1168/1168 [==============================] - 0s 177us/step - loss: 1374532921.2055 - val_loss: 2078866963.2877\n","Epoch 649/1000\n","1168/1168 [==============================] - 0s 188us/step - loss: 1378143328.4384 - val_loss: 2081503721.2055\n","Epoch 650/1000\n","1168/1168 [==============================] - 0s 201us/step - loss: 1374161980.0548 - val_loss: 2076090154.0822\n","Epoch 651/1000\n","1168/1168 [==============================] - 0s 205us/step - loss: 1376064016.0000 - val_loss: 2081231570.4110\n","Epoch 652/1000\n","1168/1168 [==============================] - 0s 205us/step - loss: 1374701021.8082 - val_loss: 2076718667.3973\n","Epoch 653/1000\n","1168/1168 [==============================] - 0s 159us/step - loss: 1378286012.0548 - val_loss: 2076944566.3562\n","Epoch 654/1000\n","1168/1168 [==============================] - 0s 166us/step - loss: 1375478876.4932 - val_loss: 2076559693.1507\n","Epoch 655/1000\n","1168/1168 [==============================] - 0s 159us/step - loss: 1373852245.0411 - val_loss: 2078722014.6849\n","Epoch 656/1000\n","1168/1168 [==============================] - 0s 161us/step - loss: 1372577394.8493 - val_loss: 2079015420.4932\n","Epoch 657/1000\n","1168/1168 [==============================] - 0s 178us/step - loss: 1372596478.6849 - val_loss: 2080836257.3151\n","Epoch 658/1000\n","1168/1168 [==============================] - 0s 163us/step - loss: 1373001242.3014 - val_loss: 2075793586.8493\n","Epoch 659/1000\n","1168/1168 [==============================] - 0s 186us/step - loss: 1372908147.7260 - val_loss: 2076292858.7397\n","Epoch 660/1000\n","1168/1168 [==============================] - 0s 160us/step - loss: 1373292922.7397 - val_loss: 2076030015.1233\n","Epoch 661/1000\n","1168/1168 [==============================] - 0s 208us/step - loss: 1374540737.3151 - val_loss: 2082526446.4658\n","Epoch 662/1000\n","1168/1168 [==============================] - 0s 235us/step - loss: 1372317133.5890 - val_loss: 2079074282.9589\n","Epoch 663/1000\n","1168/1168 [==============================] - 0s 210us/step - loss: 1371198883.0685 - val_loss: 2083874069.0411\n","Epoch 664/1000\n","1168/1168 [==============================] - 0s 166us/step - loss: 1370971032.5479 - val_loss: 2080614215.8904\n","Epoch 665/1000\n","1168/1168 [==============================] - 0s 164us/step - loss: 1372176611.5068 - val_loss: 2076832694.3562\n","Epoch 666/1000\n","1168/1168 [==============================] - 0s 173us/step - loss: 1371776989.3699 - val_loss: 2075782231.6712\n","Epoch 667/1000\n","1168/1168 [==============================] - 0s 182us/step - loss: 1370417793.3151 - val_loss: 2079016348.0548\n","Epoch 668/1000\n","1168/1168 [==============================] - 0s 169us/step - loss: 1370752927.1233 - val_loss: 2080557171.7260\n","Epoch 669/1000\n","1168/1168 [==============================] - 0s 158us/step - loss: 1369219804.7123 - val_loss: 2078096859.1781\n","Epoch 670/1000\n","1168/1168 [==============================] - 0s 162us/step - loss: 1371243006.9041 - val_loss: 2075148975.3425\n","Epoch 671/1000\n","1168/1168 [==============================] - 0s 154us/step - loss: 1369676450.1918 - val_loss: 2075144162.1918\n","Epoch 672/1000\n","1168/1168 [==============================] - 0s 183us/step - loss: 1369827267.7260 - val_loss: 2077617153.7534\n","Epoch 673/1000\n","1168/1168 [==============================] - 0s 209us/step - loss: 1370604607.5616 - val_loss: 2076466509.1507\n","Epoch 674/1000\n","1168/1168 [==============================] - 0s 157us/step - loss: 1369203594.0822 - val_loss: 2077376398.0274\n","Epoch 675/1000\n","1168/1168 [==============================] - 0s 151us/step - loss: 1369830663.4521 - val_loss: 2085103240.7671\n","Epoch 676/1000\n","1168/1168 [==============================] - 0s 177us/step - loss: 1369007664.8767 - val_loss: 2083134478.0274\n","Epoch 677/1000\n","1168/1168 [==============================] - 0s 178us/step - loss: 1368397008.2192 - val_loss: 2077231998.2466\n","Epoch 678/1000\n","1168/1168 [==============================] - 0s 162us/step - loss: 1370576765.8082 - val_loss: 2074111852.7123\n","Epoch 679/1000\n","1168/1168 [==============================] - 0s 172us/step - loss: 1368255663.7808 - val_loss: 2078076871.8904\n","Epoch 680/1000\n","1168/1168 [==============================] - 0s 163us/step - loss: 1370024425.4247 - val_loss: 2074292599.2329\n","Epoch 681/1000\n","1168/1168 [==============================] - 0s 164us/step - loss: 1367536570.5205 - val_loss: 2077590333.3699\n","Epoch 682/1000\n","1168/1168 [==============================] - 0s 162us/step - loss: 1366320285.1507 - val_loss: 2079673887.5616\n","Epoch 683/1000\n","1168/1168 [==============================] - 0s 169us/step - loss: 1366553450.5205 - val_loss: 2080197342.6849\n","Epoch 684/1000\n","1168/1168 [==============================] - 0s 164us/step - loss: 1367481619.9452 - val_loss: 2086779455.1233\n","Epoch 685/1000\n","1168/1168 [==============================] - 0s 159us/step - loss: 1366986367.5616 - val_loss: 2080749020.9315\n","Epoch 686/1000\n","1168/1168 [==============================] - 0s 158us/step - loss: 1365293651.7260 - val_loss: 2077171776.8767\n","Epoch 687/1000\n","1168/1168 [==============================] - 0s 171us/step - loss: 1366156728.7671 - val_loss: 2081149119.1233\n","Epoch 688/1000\n","1168/1168 [==============================] - 0s 173us/step - loss: 1365942293.9178 - val_loss: 2073183600.2192\n","Epoch 689/1000\n","1168/1168 [==============================] - 0s 164us/step - loss: 1365533771.8356 - val_loss: 2077845097.2055\n","Epoch 690/1000\n","1168/1168 [==============================] - 0s 158us/step - loss: 1365419748.6027 - val_loss: 2081587426.1918\n","Epoch 691/1000\n","1168/1168 [==============================] - 0s 163us/step - loss: 1364476941.5890 - val_loss: 2076983672.9863\n","Epoch 692/1000\n","1168/1168 [==============================] - 0s 167us/step - loss: 1365672781.1507 - val_loss: 2081606727.8904\n","Epoch 693/1000\n","1168/1168 [==============================] - 0s 178us/step - loss: 1363750887.4521 - val_loss: 2078991787.8356\n","Epoch 694/1000\n","1168/1168 [==============================] - 0s 186us/step - loss: 1363382259.5068 - val_loss: 2074889303.6712\n","Epoch 695/1000\n","1168/1168 [==============================] - 0s 209us/step - loss: 1363478399.1233 - val_loss: 2080058578.4110\n","Epoch 696/1000\n","1168/1168 [==============================] - 0s 208us/step - loss: 1363142698.5205 - val_loss: 2082737427.2877\n","Epoch 697/1000\n","1168/1168 [==============================] - 0s 211us/step - loss: 1362031320.9863 - val_loss: 2079865671.8904\n","Epoch 698/1000\n","1168/1168 [==============================] - 0s 221us/step - loss: 1364005983.1233 - val_loss: 2079307218.4110\n","Epoch 699/1000\n","1168/1168 [==============================] - 0s 210us/step - loss: 1362372635.1781 - val_loss: 2076896426.0822\n","Epoch 700/1000\n","1168/1168 [==============================] - 0s 206us/step - loss: 1362914280.5479 - val_loss: 2080028549.2603\n","Epoch 701/1000\n","1168/1168 [==============================] - 0s 221us/step - loss: 1362903020.2740 - val_loss: 2077927979.8356\n","Epoch 702/1000\n","1168/1168 [==============================] - 0s 190us/step - loss: 1361001706.5205 - val_loss: 2077142373.6986\n","Epoch 703/1000\n","1168/1168 [==============================] - 0s 167us/step - loss: 1361582777.8630 - val_loss: 2074011328.8767\n","Epoch 704/1000\n","1168/1168 [==============================] - 0s 163us/step - loss: 1363865300.3836 - val_loss: 2073818531.0685\n","Epoch 705/1000\n","1168/1168 [==============================] - 0s 166us/step - loss: 1361710127.3425 - val_loss: 2080955249.9726\n","Epoch 706/1000\n","1168/1168 [==============================] - 0s 157us/step - loss: 1359958604.0548 - val_loss: 2078402973.8082\n","Epoch 707/1000\n","1168/1168 [==============================] - 0s 182us/step - loss: 1359736078.4658 - val_loss: 2081955463.0137\n","Epoch 708/1000\n","1168/1168 [==============================] - 0s 207us/step - loss: 1358650950.7945 - val_loss: 2075818627.5068\n","Epoch 709/1000\n","1168/1168 [==============================] - 0s 227us/step - loss: 1358756115.7260 - val_loss: 2081408213.9178\n","Epoch 710/1000\n","1168/1168 [==============================] - 0s 172us/step - loss: 1359275637.4795 - val_loss: 2074694698.0822\n","Epoch 711/1000\n","1168/1168 [==============================] - 0s 160us/step - loss: 1359235213.5890 - val_loss: 2075647619.5068\n","Epoch 712/1000\n","1168/1168 [==============================] - 0s 181us/step - loss: 1358496035.5068 - val_loss: 2080415156.6027\n","Epoch 713/1000\n","1168/1168 [==============================] - 0s 163us/step - loss: 1360251185.5342 - val_loss: 2076467748.8219\n","Epoch 714/1000\n","1168/1168 [==============================] - 0s 170us/step - loss: 1356681550.9041 - val_loss: 2084945513.2055\n","Epoch 715/1000\n","1168/1168 [==============================] - 0s 174us/step - loss: 1357262385.9726 - val_loss: 2083494808.5479\n","Epoch 716/1000\n","1168/1168 [==============================] - 0s 162us/step - loss: 1356522644.6027 - val_loss: 2076490851.9452\n","Epoch 717/1000\n","1168/1168 [==============================] - 0s 202us/step - loss: 1357148231.6712 - val_loss: 2078954615.2329\n","Epoch 718/1000\n","1168/1168 [==============================] - 0s 208us/step - loss: 1357987137.3151 - val_loss: 2079316783.3425\n","Epoch 719/1000\n","1168/1168 [==============================] - 0s 193us/step - loss: 1358374473.6438 - val_loss: 2076261097.2055\n","Epoch 720/1000\n","1168/1168 [==============================] - 0s 167us/step - loss: 1359574177.3151 - val_loss: 2081030559.5616\n","Epoch 721/1000\n","1168/1168 [==============================] - 0s 167us/step - loss: 1356789145.2055 - val_loss: 2078103118.9041\n","Epoch 722/1000\n","1168/1168 [==============================] - 0s 173us/step - loss: 1356963435.8356 - val_loss: 2075543266.1918\n","Epoch 723/1000\n","1168/1168 [==============================] - 0s 161us/step - loss: 1358103111.8904 - val_loss: 2074154687.1233\n","Epoch 724/1000\n","1168/1168 [==============================] - 0s 163us/step - loss: 1355469938.1918 - val_loss: 2082714120.7671\n","Epoch 725/1000\n","1168/1168 [==============================] - 0s 167us/step - loss: 1355977636.1644 - val_loss: 2075063965.8082\n","Epoch 726/1000\n","1168/1168 [==============================] - 0s 168us/step - loss: 1355883577.4247 - val_loss: 2084455832.5479\n","Epoch 727/1000\n","1168/1168 [==============================] - 0s 176us/step - loss: 1355302716.9315 - val_loss: 2084949281.3151\n","Epoch 728/1000\n","1168/1168 [==============================] - 0s 163us/step - loss: 1357627121.0959 - val_loss: 2089992854.7945\n","Epoch 729/1000\n","1168/1168 [==============================] - 0s 160us/step - loss: 1357674947.9452 - val_loss: 2078861483.8356\n","Epoch 730/1000\n","1168/1168 [==============================] - 0s 162us/step - loss: 1354575365.4795 - val_loss: 2082004779.8356\n","Epoch 731/1000\n","1168/1168 [==============================] - 0s 162us/step - loss: 1354363402.5205 - val_loss: 2084937971.7260\n","Epoch 732/1000\n","1168/1168 [==============================] - 0s 172us/step - loss: 1355152193.7534 - val_loss: 2083713029.2603\n","Epoch 733/1000\n","1168/1168 [==============================] - 0s 157us/step - loss: 1353056692.1644 - val_loss: 2078988317.8082\n","Epoch 734/1000\n","1168/1168 [==============================] - 0s 156us/step - loss: 1352585147.6164 - val_loss: 2081781879.2329\n","Epoch 735/1000\n","1168/1168 [==============================] - 0s 166us/step - loss: 1353795006.2466 - val_loss: 2078733806.4658\n","Epoch 736/1000\n","1168/1168 [==============================] - 0s 164us/step - loss: 1352309720.9863 - val_loss: 2080517211.1781\n","Epoch 737/1000\n","1168/1168 [==============================] - 0s 162us/step - loss: 1352914750.6849 - val_loss: 2076163315.7260\n","Epoch 738/1000\n","1168/1168 [==============================] - 0s 173us/step - loss: 1351870698.3014 - val_loss: 2081305724.4932\n","Epoch 739/1000\n","1168/1168 [==============================] - 0s 164us/step - loss: 1351867772.7123 - val_loss: 2079868259.9452\n","Epoch 740/1000\n","1168/1168 [==============================] - 0s 163us/step - loss: 1352169504.0000 - val_loss: 2079773966.0274\n","Epoch 741/1000\n","1168/1168 [==============================] - 0s 173us/step - loss: 1350661640.7671 - val_loss: 2081000158.6849\n","Epoch 742/1000\n","1168/1168 [==============================] - 0s 176us/step - loss: 1350932661.0411 - val_loss: 2079511324.0548\n","Epoch 743/1000\n","1168/1168 [==============================] - 0s 175us/step - loss: 1352667197.1507 - val_loss: 2081272905.6438\n","Epoch 744/1000\n","1168/1168 [==============================] - 0s 163us/step - loss: 1350690129.9726 - val_loss: 2077659846.1370\n","Epoch 745/1000\n","1168/1168 [==============================] - 0s 167us/step - loss: 1353806198.7945 - val_loss: 2072978512.6575\n","Epoch 746/1000\n","1168/1168 [==============================] - 0s 164us/step - loss: 1352161318.1370 - val_loss: 2075467393.7534\n","Epoch 747/1000\n","1168/1168 [==============================] - 0s 175us/step - loss: 1349722769.9726 - val_loss: 2079528889.8630\n","Epoch 748/1000\n","1168/1168 [==============================] - 0s 222us/step - loss: 1349718504.1096 - val_loss: 2076648211.2877\n","Epoch 749/1000\n","1168/1168 [==============================] - 0s 209us/step - loss: 1350813063.0137 - val_loss: 2078275214.0274\n","Epoch 750/1000\n","1168/1168 [==============================] - 0s 166us/step - loss: 1350269179.1781 - val_loss: 2077280199.8904\n","Epoch 751/1000\n","1168/1168 [==============================] - 0s 176us/step - loss: 1350335251.0685 - val_loss: 2082997286.5753\n","Epoch 752/1000\n","1168/1168 [==============================] - 0s 203us/step - loss: 1349510337.7534 - val_loss: 2090917042.8493\n","Epoch 753/1000\n","1168/1168 [==============================] - 0s 215us/step - loss: 1349704563.2877 - val_loss: 2080376177.9726\n","Epoch 754/1000\n","1168/1168 [==============================] - 0s 180us/step - loss: 1348618275.0685 - val_loss: 2078047561.6438\n","Epoch 755/1000\n","1168/1168 [==============================] - 0s 163us/step - loss: 1347702490.7397 - val_loss: 2079255529.2055\n","Epoch 756/1000\n","1168/1168 [==============================] - 0s 163us/step - loss: 1347759461.9178 - val_loss: 2081881168.6575\n","Epoch 757/1000\n","1168/1168 [==============================] - 0s 160us/step - loss: 1347810613.0411 - val_loss: 2083224730.3014\n","Epoch 758/1000\n","1168/1168 [==============================] - 0s 172us/step - loss: 1353240057.8630 - val_loss: 2087674792.3288\n","Epoch 759/1000\n","1168/1168 [==============================] - 0s 162us/step - loss: 1348795120.8767 - val_loss: 2083827983.7808\n","Epoch 760/1000\n","1168/1168 [==============================] - 0s 184us/step - loss: 1347826994.8493 - val_loss: 2082220556.2740\n","Epoch 761/1000\n","1168/1168 [==============================] - 0s 162us/step - loss: 1346754762.0822 - val_loss: 2078462698.9589\n","Epoch 762/1000\n","1168/1168 [==============================] - 0s 162us/step - loss: 1346178218.5205 - val_loss: 2081263491.5068\n","Epoch 763/1000\n","1168/1168 [==============================] - 0s 173us/step - loss: 1346844919.2329 - val_loss: 2080752473.4247\n","Epoch 764/1000\n","1168/1168 [==============================] - 0s 157us/step - loss: 1346307139.9452 - val_loss: 2077140564.1644\n","Epoch 765/1000\n","1168/1168 [==============================] - 0s 166us/step - loss: 1347448448.8767 - val_loss: 2073305521.0959\n","Epoch 766/1000\n","1168/1168 [==============================] - 0s 161us/step - loss: 1346695416.5479 - val_loss: 2079766524.4932\n","Epoch 767/1000\n","1168/1168 [==============================] - 0s 160us/step - loss: 1345891612.9315 - val_loss: 2078934436.8219\n","Epoch 768/1000\n","1168/1168 [==============================] - 0s 171us/step - loss: 1346302040.9863 - val_loss: 2080915392.8767\n","Epoch 769/1000\n","1168/1168 [==============================] - 0s 156us/step - loss: 1345988245.9178 - val_loss: 2080937878.7945\n","Epoch 770/1000\n","1168/1168 [==============================] - 0s 166us/step - loss: 1347979951.3425 - val_loss: 2080259335.0137\n","Epoch 771/1000\n","1168/1168 [==============================] - 0s 169us/step - loss: 1344739521.5342 - val_loss: 2083416281.4247\n","Epoch 772/1000\n","1168/1168 [==============================] - 0s 159us/step - loss: 1344568579.0685 - val_loss: 2078579610.3014\n","Epoch 773/1000\n","1168/1168 [==============================] - 0s 165us/step - loss: 1347310750.4658 - val_loss: 2080825941.9178\n","Epoch 774/1000\n","1168/1168 [==============================] - 0s 174us/step - loss: 1347222439.0137 - val_loss: 2076171744.4384\n","Epoch 775/1000\n","1168/1168 [==============================] - 0s 158us/step - loss: 1343320611.5068 - val_loss: 2079772782.4658\n","Epoch 776/1000\n","1168/1168 [==============================] - 0s 167us/step - loss: 1344478623.1233 - val_loss: 2080217526.3562\n","Epoch 777/1000\n","1168/1168 [==============================] - 0s 160us/step - loss: 1343315192.9863 - val_loss: 2079700972.7123\n","Epoch 778/1000\n","1168/1168 [==============================] - 0s 164us/step - loss: 1342656445.3699 - val_loss: 2082921391.3425\n","Epoch 779/1000\n","1168/1168 [==============================] - 0s 175us/step - loss: 1345539142.1370 - val_loss: 2085308992.8767\n","Epoch 780/1000\n","1168/1168 [==============================] - 0s 162us/step - loss: 1343382026.7397 - val_loss: 2083511844.8219\n","Epoch 781/1000\n","1168/1168 [==============================] - 0s 173us/step - loss: 1344523280.8767 - val_loss: 2094648688.2192\n","Epoch 782/1000\n","1168/1168 [==============================] - 0s 204us/step - loss: 1342254641.5342 - val_loss: 2080606041.4247\n","Epoch 783/1000\n","1168/1168 [==============================] - 0s 224us/step - loss: 1343062729.6438 - val_loss: 2082025962.9589\n","Epoch 784/1000\n","1168/1168 [==============================] - 0s 190us/step - loss: 1343438033.0959 - val_loss: 2081934167.6712\n","Epoch 785/1000\n","1168/1168 [==============================] - 0s 165us/step - loss: 1342169866.0822 - val_loss: 2080060933.2603\n","Epoch 786/1000\n","1168/1168 [==============================] - 0s 164us/step - loss: 1341776494.0274 - val_loss: 2080418831.7808\n","Epoch 787/1000\n","1168/1168 [==============================] - 0s 164us/step - loss: 1342704846.9041 - val_loss: 2074298273.3151\n","Epoch 788/1000\n","1168/1168 [==============================] - 0s 203us/step - loss: 1342196446.2466 - val_loss: 2076723257.8630\n","Epoch 789/1000\n","1168/1168 [==============================] - 0s 182us/step - loss: 1344467871.5616 - val_loss: 2077162003.2877\n","Epoch 790/1000\n","1168/1168 [==============================] - 0s 165us/step - loss: 1341422154.0822 - val_loss: 2081958457.8630\n","Epoch 791/1000\n","1168/1168 [==============================] - 0s 168us/step - loss: 1341968756.8219 - val_loss: 2077965822.2466\n","Epoch 792/1000\n","1168/1168 [==============================] - 0s 167us/step - loss: 1341127957.9178 - val_loss: 2074741831.8904\n","Epoch 793/1000\n","1168/1168 [==============================] - 0s 159us/step - loss: 1339005047.2329 - val_loss: 2086302623.5616\n","Epoch 794/1000\n","1168/1168 [==============================] - 0s 176us/step - loss: 1339488072.3288 - val_loss: 2083627612.9315\n","Epoch 795/1000\n","1168/1168 [==============================] - 0s 163us/step - loss: 1339084192.8767 - val_loss: 2077003207.8904\n","Epoch 796/1000\n","1168/1168 [==============================] - 0s 164us/step - loss: 1340578862.4658 - val_loss: 2081048283.1781\n","Epoch 797/1000\n","1168/1168 [==============================] - 0s 159us/step - loss: 1339704360.9863 - val_loss: 2077985027.5068\n","Epoch 798/1000\n","1168/1168 [==============================] - 0s 164us/step - loss: 1338494260.6027 - val_loss: 2081208893.3699\n","Epoch 799/1000\n","1168/1168 [==============================] - 0s 177us/step - loss: 1338714514.8493 - val_loss: 2085152378.7397\n","Epoch 800/1000\n","1168/1168 [==============================] - 0s 162us/step - loss: 1339308110.0274 - val_loss: 2084138211.9452\n","Epoch 801/1000\n","1168/1168 [==============================] - 0s 168us/step - loss: 1337704004.3836 - val_loss: 2082230116.8219\n","Epoch 802/1000\n","1168/1168 [==============================] - 0s 156us/step - loss: 1339305477.2603 - val_loss: 2081481146.7397\n","Epoch 803/1000\n","1168/1168 [==============================] - 0s 165us/step - loss: 1339344929.3151 - val_loss: 2079114913.3151\n","Epoch 804/1000\n","1168/1168 [==============================] - 0s 179us/step - loss: 1337160058.7397 - val_loss: 2082746513.5342\n","Epoch 805/1000\n","1168/1168 [==============================] - 0s 159us/step - loss: 1337721221.9178 - val_loss: 2087877074.4110\n","Epoch 806/1000\n","1168/1168 [==============================] - 0s 205us/step - loss: 1338601778.6301 - val_loss: 2085857316.8219\n","Epoch 807/1000\n","1168/1168 [==============================] - 0s 161us/step - loss: 1337835837.5890 - val_loss: 2079307509.4795\n","Epoch 808/1000\n","1168/1168 [==============================] - 0s 161us/step - loss: 1338248950.3562 - val_loss: 2081346521.4247\n","Epoch 809/1000\n","1168/1168 [==============================] - 0s 158us/step - loss: 1338922288.2192 - val_loss: 2077999025.9726\n","Epoch 810/1000\n","1168/1168 [==============================] - 0s 184us/step - loss: 1336656005.9178 - val_loss: 2090233346.6301\n","Epoch 811/1000\n","1168/1168 [==============================] - 0s 170us/step - loss: 1336402873.4247 - val_loss: 2080761420.2740\n","Epoch 812/1000\n","1168/1168 [==============================] - 0s 174us/step - loss: 1337721923.0685 - val_loss: 2082560523.3973\n","Epoch 813/1000\n","1168/1168 [==============================] - 0s 193us/step - loss: 1335153759.5616 - val_loss: 2082826280.3288\n","Epoch 814/1000\n","1168/1168 [==============================] - 0s 203us/step - loss: 1335366223.7808 - val_loss: 2077962531.9452\n","Epoch 815/1000\n","1168/1168 [==============================] - 0s 163us/step - loss: 1335110377.2055 - val_loss: 2080742370.1918\n","Epoch 816/1000\n","1168/1168 [==============================] - 0s 190us/step - loss: 1335032967.8904 - val_loss: 2079187286.7945\n","Epoch 817/1000\n","1168/1168 [==============================] - 0s 209us/step - loss: 1335696133.0411 - val_loss: 2077686186.9589\n","Epoch 818/1000\n","1168/1168 [==============================] - 0s 209us/step - loss: 1335906645.0411 - val_loss: 2080363085.1507\n","Epoch 819/1000\n","1168/1168 [==============================] - 0s 167us/step - loss: 1334816572.9315 - val_loss: 2090859504.2192\n","Epoch 820/1000\n","1168/1168 [==============================] - 0s 169us/step - loss: 1338283940.3836 - val_loss: 2092203171.0685\n","Epoch 821/1000\n","1168/1168 [==============================] - 0s 202us/step - loss: 1334693308.9315 - val_loss: 2080606543.7808\n","Epoch 822/1000\n","1168/1168 [==============================] - 0s 205us/step - loss: 1335102698.5205 - val_loss: 2084375680.8767\n","Epoch 823/1000\n","1168/1168 [==============================] - 0s 214us/step - loss: 1333782051.9452 - val_loss: 2080891707.6164\n","Epoch 824/1000\n","1168/1168 [==============================] - 0s 227us/step - loss: 1334036502.7945 - val_loss: 2083264143.7808\n","Epoch 825/1000\n","1168/1168 [==============================] - 0s 208us/step - loss: 1334378891.8356 - val_loss: 2084238861.1507\n","Epoch 826/1000\n","1168/1168 [==============================] - 0s 210us/step - loss: 1332289214.6849 - val_loss: 2077351545.8630\n","Epoch 827/1000\n","1168/1168 [==============================] - 0s 212us/step - loss: 1332117942.7945 - val_loss: 2077969723.6164\n","Epoch 828/1000\n","1168/1168 [==============================] - 0s 225us/step - loss: 1333317731.2877 - val_loss: 2084843674.3014\n","Epoch 829/1000\n","1168/1168 [==============================] - 0s 162us/step - loss: 1333767791.7808 - val_loss: 2080772023.2329\n","Epoch 830/1000\n","1168/1168 [==============================] - 0s 165us/step - loss: 1332404653.1507 - val_loss: 2079640751.3425\n","Epoch 831/1000\n","1168/1168 [==============================] - 0s 160us/step - loss: 1333353852.9315 - val_loss: 2077474592.4384\n","Epoch 832/1000\n","1168/1168 [==============================] - 0s 167us/step - loss: 1333595012.3836 - val_loss: 2083720609.3151\n","Epoch 833/1000\n","1168/1168 [==============================] - 0s 225us/step - loss: 1331766471.4521 - val_loss: 2085219161.4247\n","Epoch 834/1000\n","1168/1168 [==============================] - 0s 214us/step - loss: 1330758320.0000 - val_loss: 2083201735.0137\n","Epoch 835/1000\n","1168/1168 [==============================] - 0s 207us/step - loss: 1331346548.6027 - val_loss: 2079046079.1233\n","Epoch 836/1000\n","1168/1168 [==============================] - 0s 205us/step - loss: 1329851792.2192 - val_loss: 2074829218.1918\n","Epoch 837/1000\n","1168/1168 [==============================] - 0s 190us/step - loss: 1331575510.3562 - val_loss: 2077038300.9315\n","Epoch 838/1000\n","1168/1168 [==============================] - 0s 167us/step - loss: 1330849882.3014 - val_loss: 2083526689.3151\n","Epoch 839/1000\n","1168/1168 [==============================] - 0s 164us/step - loss: 1328797491.7260 - val_loss: 2079088656.6575\n","Epoch 840/1000\n","1168/1168 [==============================] - 0s 199us/step - loss: 1330750422.3562 - val_loss: 2080939029.9178\n","Epoch 841/1000\n","1168/1168 [==============================] - 0s 212us/step - loss: 1330780526.0274 - val_loss: 2082618933.4795\n","Epoch 842/1000\n","1168/1168 [==============================] - 0s 230us/step - loss: 1330404013.1507 - val_loss: 2079260565.0411\n","Epoch 843/1000\n","1168/1168 [==============================] - 0s 201us/step - loss: 1329040091.1781 - val_loss: 2083098022.5753\n","Epoch 844/1000\n","1168/1168 [==============================] - 0s 215us/step - loss: 1328624115.7260 - val_loss: 2079496139.3973\n","Epoch 845/1000\n","1168/1168 [==============================] - 0s 203us/step - loss: 1329580808.3288 - val_loss: 2081917617.0959\n","Epoch 846/1000\n","1168/1168 [==============================] - 0s 209us/step - loss: 1328445219.9452 - val_loss: 2076007113.6438\n","Epoch 847/1000\n","1168/1168 [==============================] - 0s 212us/step - loss: 1327786568.7671 - val_loss: 2075746921.2055\n","Epoch 848/1000\n","1168/1168 [==============================] - 0s 205us/step - loss: 1326234244.3836 - val_loss: 2079356742.1370\n","Epoch 849/1000\n","1168/1168 [==============================] - 0s 162us/step - loss: 1326548568.3288 - val_loss: 2080582575.3425\n","Epoch 850/1000\n","1168/1168 [==============================] - 0s 209us/step - loss: 1325871415.8904 - val_loss: 2082097455.3425\n","Epoch 851/1000\n","1168/1168 [==============================] - 0s 167us/step - loss: 1325868886.3562 - val_loss: 2082866146.1918\n","Epoch 852/1000\n","1168/1168 [==============================] - 0s 158us/step - loss: 1326496577.0959 - val_loss: 2087887404.7123\n","Epoch 853/1000\n","1168/1168 [==============================] - 0s 161us/step - loss: 1326421109.0411 - val_loss: 2084275236.8219\n","Epoch 854/1000\n","1168/1168 [==============================] - 0s 160us/step - loss: 1325554246.1370 - val_loss: 2079536715.3973\n","Epoch 855/1000\n","1168/1168 [==============================] - 0s 163us/step - loss: 1326658392.5479 - val_loss: 2093435178.0822\n","Epoch 856/1000\n","1168/1168 [==============================] - 0s 175us/step - loss: 1325004462.9041 - val_loss: 2080709232.2192\n","Epoch 857/1000\n","1168/1168 [==============================] - 0s 174us/step - loss: 1324942668.4932 - val_loss: 2086516999.8904\n","Epoch 858/1000\n","1168/1168 [==============================] - 0s 159us/step - loss: 1326394103.0137 - val_loss: 2082025532.4932\n","Epoch 859/1000\n","1168/1168 [==============================] - 0s 159us/step - loss: 1325955131.1781 - val_loss: 2077431829.0411\n","Epoch 860/1000\n","1168/1168 [==============================] - 0s 155us/step - loss: 1325191682.6301 - val_loss: 2075175189.9178\n","Epoch 861/1000\n","1168/1168 [==============================] - 0s 207us/step - loss: 1325382049.0959 - val_loss: 2081940190.6849\n","Epoch 862/1000\n","1168/1168 [==============================] - 0s 215us/step - loss: 1324532828.7123 - val_loss: 2081231293.3699\n","Epoch 863/1000\n","1168/1168 [==============================] - 0s 208us/step - loss: 1324808396.2740 - val_loss: 2086293845.9178\n","Epoch 864/1000\n","1168/1168 [==============================] - 0s 196us/step - loss: 1324085068.7123 - val_loss: 2085361501.8082\n","Epoch 865/1000\n","1168/1168 [==============================] - 0s 205us/step - loss: 1322501898.5205 - val_loss: 2079632182.3562\n","Epoch 866/1000\n","1168/1168 [==============================] - 0s 164us/step - loss: 1323062193.5342 - val_loss: 2076926163.2877\n","Epoch 867/1000\n","1168/1168 [==============================] - 0s 163us/step - loss: 1325407935.7808 - val_loss: 2081760254.2466\n","Epoch 868/1000\n","1168/1168 [==============================] - 0s 156us/step - loss: 1323168583.8904 - val_loss: 2086357654.7945\n","Epoch 869/1000\n","1168/1168 [==============================] - 0s 162us/step - loss: 1324322055.8904 - val_loss: 2078888695.2329\n","Epoch 870/1000\n","1168/1168 [==============================] - 0s 187us/step - loss: 1324612920.9863 - val_loss: 2086105426.4110\n","Epoch 871/1000\n","1168/1168 [==============================] - 0s 230us/step - loss: 1321126544.2192 - val_loss: 2078071015.4521\n","Epoch 872/1000\n","1168/1168 [==============================] - 0s 211us/step - loss: 1322786749.1507 - val_loss: 2082063844.8219\n","Epoch 873/1000\n","1168/1168 [==============================] - 0s 203us/step - loss: 1321444126.2466 - val_loss: 2083674714.3014\n","Epoch 874/1000\n","1168/1168 [==============================] - 0s 194us/step - loss: 1322312375.2329 - val_loss: 2079182239.5616\n","Epoch 875/1000\n","1168/1168 [==============================] - 0s 170us/step - loss: 1323983643.1781 - val_loss: 2077629132.2740\n","Epoch 876/1000\n","1168/1168 [==============================] - 0s 162us/step - loss: 1325440580.1644 - val_loss: 2087975214.4658\n","Epoch 877/1000\n","1168/1168 [==============================] - 0s 160us/step - loss: 1321532585.2055 - val_loss: 2074987056.2192\n","Epoch 878/1000\n","1168/1168 [==============================] - 0s 159us/step - loss: 1323179279.7808 - val_loss: 2074345807.7808\n","Epoch 879/1000\n","1168/1168 [==============================] - 0s 175us/step - loss: 1322657500.9315 - val_loss: 2074404813.1507\n","Epoch 880/1000\n","1168/1168 [==============================] - 0s 239us/step - loss: 1320948276.3836 - val_loss: 2078853357.5890\n","Epoch 881/1000\n","1168/1168 [==============================] - 0s 213us/step - loss: 1322171395.0685 - val_loss: 2079433551.7808\n","Epoch 882/1000\n","1168/1168 [==============================] - 0s 168us/step - loss: 1319950062.4658 - val_loss: 2078066812.4932\n","Epoch 883/1000\n","1168/1168 [==============================] - 0s 170us/step - loss: 1319317622.7945 - val_loss: 2083166594.6301\n","Epoch 884/1000\n","1168/1168 [==============================] - 0s 166us/step - loss: 1320846647.2329 - val_loss: 2078019969.7534\n","Epoch 885/1000\n","1168/1168 [==============================] - 0s 171us/step - loss: 1322388103.4521 - val_loss: 2091695072.4384\n","Epoch 886/1000\n","1168/1168 [==============================] - 0s 160us/step - loss: 1317766380.4932 - val_loss: 2075882266.3014\n","Epoch 887/1000\n","1168/1168 [==============================] - 0s 159us/step - loss: 1319726197.0411 - val_loss: 2074483314.8493\n","Epoch 888/1000\n","1168/1168 [==============================] - 0s 169us/step - loss: 1320629829.6986 - val_loss: 2087864777.6438\n","Epoch 889/1000\n","1168/1168 [==============================] - 0s 172us/step - loss: 1317965800.3288 - val_loss: 2078902591.1233\n","Epoch 890/1000\n","1168/1168 [==============================] - 0s 170us/step - loss: 1321417506.4110 - val_loss: 2086688569.8630\n","Epoch 891/1000\n","1168/1168 [==============================] - 0s 164us/step - loss: 1322642432.8767 - val_loss: 2083709501.3699\n","Epoch 892/1000\n","1168/1168 [==============================] - 0s 160us/step - loss: 1318576477.1507 - val_loss: 2077758372.8219\n","Epoch 893/1000\n","1168/1168 [==============================] - 0s 172us/step - loss: 1317454304.8767 - val_loss: 2079500589.5890\n","Epoch 894/1000\n","1168/1168 [==============================] - 0s 170us/step - loss: 1318610906.3014 - val_loss: 2082956706.1918\n","Epoch 895/1000\n","1168/1168 [==============================] - 0s 175us/step - loss: 1318372289.9726 - val_loss: 2088186951.8904\n","Epoch 896/1000\n","1168/1168 [==============================] - 0s 161us/step - loss: 1319962792.3288 - val_loss: 2074245287.4521\n","Epoch 897/1000\n","1168/1168 [==============================] - 0s 172us/step - loss: 1320084426.7397 - val_loss: 2075941006.0274\n","Epoch 898/1000\n","1168/1168 [==============================] - 0s 160us/step - loss: 1318274946.6301 - val_loss: 2075190708.6027\n","Epoch 899/1000\n","1168/1168 [==============================] - 0s 189us/step - loss: 1318324143.3425 - val_loss: 2081048735.5616\n","Epoch 900/1000\n","1168/1168 [==============================] - 0s 222us/step - loss: 1318283519.3425 - val_loss: 2079426481.0959\n","Epoch 901/1000\n","1168/1168 [==============================] - 0s 209us/step - loss: 1319043640.1096 - val_loss: 2077494758.5753\n","Epoch 902/1000\n","1168/1168 [==============================] - 0s 211us/step - loss: 1315691580.0548 - val_loss: 2081315333.2603\n","Epoch 903/1000\n","1168/1168 [==============================] - 0s 180us/step - loss: 1315352770.9589 - val_loss: 2078301653.9178\n","Epoch 904/1000\n","1168/1168 [==============================] - 0s 170us/step - loss: 1315872615.0137 - val_loss: 2077709853.8082\n","Epoch 905/1000\n","1168/1168 [==============================] - 0s 178us/step - loss: 1316013532.9315 - val_loss: 2085001900.7123\n","Epoch 906/1000\n","1168/1168 [==============================] - 0s 175us/step - loss: 1316182928.6575 - val_loss: 2085132576.4384\n","Epoch 907/1000\n","1168/1168 [==============================] - 0s 158us/step - loss: 1318363915.1781 - val_loss: 2077644263.4521\n","Epoch 908/1000\n","1168/1168 [==============================] - 0s 182us/step - loss: 1317398978.1918 - val_loss: 2079828724.6027\n","Epoch 909/1000\n","1168/1168 [==============================] - 0s 215us/step - loss: 1317095189.4795 - val_loss: 2087517534.6849\n","Epoch 910/1000\n","1168/1168 [==============================] - 0s 223us/step - loss: 1315873828.6027 - val_loss: 2072846396.4932\n","Epoch 911/1000\n","1168/1168 [==============================] - 0s 204us/step - loss: 1318938099.2877 - val_loss: 2079453471.5616\n","Epoch 912/1000\n","1168/1168 [==============================] - 0s 191us/step - loss: 1315837849.4247 - val_loss: 2076664972.2740\n","Epoch 913/1000\n","1168/1168 [==============================] - 0s 167us/step - loss: 1314354872.5479 - val_loss: 2085542685.8082\n","Epoch 914/1000\n","1168/1168 [==============================] - 0s 176us/step - loss: 1315324413.1507 - val_loss: 2081090670.4658\n","Epoch 915/1000\n","1168/1168 [==============================] - 0s 163us/step - loss: 1314344383.3425 - val_loss: 2081874968.5479\n","Epoch 916/1000\n","1168/1168 [==============================] - 0s 166us/step - loss: 1314377449.2055 - val_loss: 2083993989.2603\n","Epoch 917/1000\n","1168/1168 [==============================] - 0s 167us/step - loss: 1313927384.9863 - val_loss: 2086794097.0959\n","Epoch 918/1000\n","1168/1168 [==============================] - 0s 158us/step - loss: 1313172289.7534 - val_loss: 2080422497.3151\n","Epoch 919/1000\n","1168/1168 [==============================] - 0s 159us/step - loss: 1314148803.9452 - val_loss: 2079727296.0000\n","Epoch 920/1000\n","1168/1168 [==============================] - 0s 168us/step - loss: 1314011573.9178 - val_loss: 2077567372.2740\n","Epoch 921/1000\n","1168/1168 [==============================] - 0s 164us/step - loss: 1315093311.7808 - val_loss: 2077579748.8219\n","Epoch 922/1000\n","1168/1168 [==============================] - 0s 163us/step - loss: 1312645972.6027 - val_loss: 2084559216.2192\n","Epoch 923/1000\n","1168/1168 [==============================] - 0s 165us/step - loss: 1314736686.0274 - val_loss: 2076767999.1233\n","Epoch 924/1000\n","1168/1168 [==============================] - 0s 158us/step - loss: 1313611820.7123 - val_loss: 2078400331.3973\n","Epoch 925/1000\n","1168/1168 [==============================] - 0s 195us/step - loss: 1316700069.4795 - val_loss: 2074324431.7808\n","Epoch 926/1000\n","1168/1168 [==============================] - 0s 158us/step - loss: 1313801181.8082 - val_loss: 2086403480.5479\n","Epoch 927/1000\n","1168/1168 [==============================] - 0s 162us/step - loss: 1315064492.8219 - val_loss: 2093901603.9452\n","Epoch 928/1000\n","1168/1168 [==============================] - 0s 168us/step - loss: 1313275409.7534 - val_loss: 2079534912.8767\n","Epoch 929/1000\n","1168/1168 [==============================] - 0s 192us/step - loss: 1313320818.4110 - val_loss: 2077558820.8219\n","Epoch 930/1000\n","1168/1168 [==============================] - 0s 223us/step - loss: 1314400813.5890 - val_loss: 2081256071.8904\n","Epoch 931/1000\n","1168/1168 [==============================] - 0s 195us/step - loss: 1311523194.7397 - val_loss: 2082025612.2740\n","Epoch 932/1000\n","1168/1168 [==============================] - 0s 187us/step - loss: 1312033242.7397 - val_loss: 2074200711.8904\n","Epoch 933/1000\n","1168/1168 [==============================] - 0s 212us/step - loss: 1313074328.5479 - val_loss: 2078772550.1370\n","Epoch 934/1000\n","1168/1168 [==============================] - 0s 219us/step - loss: 1313620785.0959 - val_loss: 2084488054.3562\n","Epoch 935/1000\n","1168/1168 [==============================] - 0s 167us/step - loss: 1310492384.6575 - val_loss: 2082928184.1096\n","Epoch 936/1000\n","1168/1168 [==============================] - 0s 169us/step - loss: 1313447424.4384 - val_loss: 2078592313.8630\n","Epoch 937/1000\n","1168/1168 [==============================] - 0s 175us/step - loss: 1313363369.4247 - val_loss: 2079153748.1644\n","Epoch 938/1000\n","1168/1168 [==============================] - 0s 165us/step - loss: 1309207971.0685 - val_loss: 2084154029.5890\n","Epoch 939/1000\n","1168/1168 [==============================] - 0s 186us/step - loss: 1310541242.6301 - val_loss: 2081165998.4658\n","Epoch 940/1000\n","1168/1168 [==============================] - 0s 162us/step - loss: 1309021089.7534 - val_loss: 2079859294.6849\n","Epoch 941/1000\n","1168/1168 [==============================] - 0s 168us/step - loss: 1311660175.7808 - val_loss: 2076458236.4932\n","Epoch 942/1000\n","1168/1168 [==============================] - 0s 167us/step - loss: 1309799651.0685 - val_loss: 2082853966.0274\n","Epoch 943/1000\n","1168/1168 [==============================] - 0s 163us/step - loss: 1309523080.3288 - val_loss: 2078562555.6164\n","Epoch 944/1000\n","1168/1168 [==============================] - 0s 180us/step - loss: 1310502766.0274 - val_loss: 2076141992.3288\n","Epoch 945/1000\n","1168/1168 [==============================] - 0s 182us/step - loss: 1310085639.6712 - val_loss: 2077998500.8219\n","Epoch 946/1000\n","1168/1168 [==============================] - 0s 163us/step - loss: 1311031364.8219 - val_loss: 2081025287.8904\n","Epoch 947/1000\n","1168/1168 [==============================] - 0s 158us/step - loss: 1307955223.4521 - val_loss: 2090708299.3973\n","Epoch 948/1000\n","1168/1168 [==============================] - 0s 168us/step - loss: 1307884472.9863 - val_loss: 2077418109.3699\n","Epoch 949/1000\n","1168/1168 [==============================] - 0s 160us/step - loss: 1306646876.0548 - val_loss: 2086275114.0822\n","Epoch 950/1000\n","1168/1168 [==============================] - 0s 192us/step - loss: 1309372004.6027 - val_loss: 2082947694.4658\n","Epoch 951/1000\n","1168/1168 [==============================] - 0s 165us/step - loss: 1308173221.6986 - val_loss: 2078694556.0548\n","Epoch 952/1000\n","1168/1168 [==============================] - 0s 210us/step - loss: 1307758232.7671 - val_loss: 2084741896.7671\n","Epoch 953/1000\n","1168/1168 [==============================] - 0s 217us/step - loss: 1307866235.8356 - val_loss: 2078192589.1507\n","Epoch 954/1000\n","1168/1168 [==============================] - 0s 186us/step - loss: 1311566941.1507 - val_loss: 2083586567.8904\n","Epoch 955/1000\n","1168/1168 [==============================] - 0s 180us/step - loss: 1307660029.3699 - val_loss: 2084969182.6849\n","Epoch 956/1000\n","1168/1168 [==============================] - 0s 166us/step - loss: 1308078252.7123 - val_loss: 2085577273.8630\n","Epoch 957/1000\n","1168/1168 [==============================] - 0s 163us/step - loss: 1307531655.0137 - val_loss: 2088993144.9863\n","Epoch 958/1000\n","1168/1168 [==============================] - 0s 160us/step - loss: 1307122475.3973 - val_loss: 2087818656.4384\n","Epoch 959/1000\n","1168/1168 [==============================] - 0s 173us/step - loss: 1306679556.1644 - val_loss: 2080826625.7534\n","Epoch 960/1000\n","1168/1168 [==============================] - 0s 178us/step - loss: 1311113905.0959 - val_loss: 2085666277.6986\n","Epoch 961/1000\n","1168/1168 [==============================] - 0s 167us/step - loss: 1305529950.0274 - val_loss: 2082582215.8904\n","Epoch 962/1000\n","1168/1168 [==============================] - 0s 166us/step - loss: 1306218870.3562 - val_loss: 2081608115.7260\n","Epoch 963/1000\n","1168/1168 [==============================] - 0s 167us/step - loss: 1305806756.8219 - val_loss: 2080798133.4795\n","Epoch 964/1000\n","1168/1168 [==============================] - 0s 167us/step - loss: 1305749472.6575 - val_loss: 2091445172.6027\n","Epoch 965/1000\n","1168/1168 [==============================] - 0s 176us/step - loss: 1306149177.6438 - val_loss: 2088424127.1233\n","Epoch 966/1000\n","1168/1168 [==============================] - 0s 193us/step - loss: 1307487334.5753 - val_loss: 2093501744.2192\n","Epoch 967/1000\n","1168/1168 [==============================] - 0s 213us/step - loss: 1305779459.2877 - val_loss: 2079383141.6986\n","Epoch 968/1000\n","1168/1168 [==============================] - 0s 197us/step - loss: 1307404111.7808 - val_loss: 2089861755.6164\n","Epoch 969/1000\n","1168/1168 [==============================] - 0s 168us/step - loss: 1305894302.2466 - val_loss: 2077994389.9178\n","Epoch 970/1000\n","1168/1168 [==============================] - 0s 172us/step - loss: 1303926630.1370 - val_loss: 2083210055.8904\n","Epoch 971/1000\n","1168/1168 [==============================] - 0s 210us/step - loss: 1304842719.1233 - val_loss: 2083253471.5616\n","Epoch 972/1000\n","1168/1168 [==============================] - 0s 215us/step - loss: 1307170082.6301 - val_loss: 2082437354.9589\n","Epoch 973/1000\n","1168/1168 [==============================] - 0s 170us/step - loss: 1305082865.0959 - val_loss: 2085794582.7945\n","Epoch 974/1000\n","1168/1168 [==============================] - 0s 180us/step - loss: 1302648067.7260 - val_loss: 2080408241.0959\n","Epoch 975/1000\n","1168/1168 [==============================] - 0s 167us/step - loss: 1303824360.1096 - val_loss: 2083974175.5616\n","Epoch 976/1000\n","1168/1168 [==============================] - 0s 166us/step - loss: 1303068343.6712 - val_loss: 2076676354.6301\n","Epoch 977/1000\n","1168/1168 [==============================] - 0s 165us/step - loss: 1305685868.2740 - val_loss: 2076326257.9726\n","Epoch 978/1000\n","1168/1168 [==============================] - 0s 159us/step - loss: 1305246164.1644 - val_loss: 2079344079.7808\n","Epoch 979/1000\n","1168/1168 [==============================] - 0s 194us/step - loss: 1304283615.5616 - val_loss: 2091209891.0685\n","Epoch 980/1000\n","1168/1168 [==============================] - 0s 170us/step - loss: 1313894618.3014 - val_loss: 2075671501.1507\n","Epoch 981/1000\n","1168/1168 [==============================] - 0s 170us/step - loss: 1301497226.9589 - val_loss: 2083082657.3151\n","Epoch 982/1000\n","1168/1168 [==============================] - 0s 162us/step - loss: 1305275212.9315 - val_loss: 2077416998.5753\n","Epoch 983/1000\n","1168/1168 [==============================] - 0s 159us/step - loss: 1302291244.9315 - val_loss: 2085448669.8082\n","Epoch 984/1000\n","1168/1168 [==============================] - 0s 176us/step - loss: 1302638880.8767 - val_loss: 2090001179.1781\n","Epoch 985/1000\n","1168/1168 [==============================] - 0s 206us/step - loss: 1301665638.1370 - val_loss: 2084655971.0685\n","Epoch 986/1000\n","1168/1168 [==============================] - 0s 220us/step - loss: 1304356378.9589 - val_loss: 2078223625.6438\n","Epoch 987/1000\n","1168/1168 [==============================] - 0s 207us/step - loss: 1303495352.1096 - val_loss: 2083401089.7534\n","Epoch 988/1000\n","1168/1168 [==============================] - 0s 193us/step - loss: 1301865598.2466 - val_loss: 2081654769.0959\n","Epoch 989/1000\n","1168/1168 [==============================] - 0s 179us/step - loss: 1301198642.4110 - val_loss: 2090017057.3151\n","Epoch 990/1000\n","1168/1168 [==============================] - 0s 160us/step - loss: 1306082248.1096 - val_loss: 2085301722.3014\n","Epoch 991/1000\n","1168/1168 [==============================] - 0s 169us/step - loss: 1301883003.1781 - val_loss: 2081614627.9452\n","Epoch 992/1000\n","1168/1168 [==============================] - 0s 158us/step - loss: 1301401178.3014 - val_loss: 2084893555.7260\n","Epoch 993/1000\n","1168/1168 [==============================] - 0s 164us/step - loss: 1302051176.1096 - val_loss: 2079897223.0137\n","Epoch 994/1000\n","1168/1168 [==============================] - 0s 177us/step - loss: 1300594776.9863 - val_loss: 2093327968.4384\n","Epoch 995/1000\n","1168/1168 [==============================] - 0s 172us/step - loss: 1301268383.3425 - val_loss: 2078461172.6027\n","Epoch 996/1000\n","1168/1168 [==============================] - 0s 166us/step - loss: 1301098281.6438 - val_loss: 2082304436.6027\n","Epoch 997/1000\n","1168/1168 [==============================] - 0s 162us/step - loss: 1300679173.2603 - val_loss: 2083450594.1918\n","Epoch 998/1000\n","1168/1168 [==============================] - 0s 165us/step - loss: 1300625522.1918 - val_loss: 2082272705.7534\n","Epoch 999/1000\n","1168/1168 [==============================] - 0s 164us/step - loss: 1298341743.7808 - val_loss: 2085166066.8493\n","Epoch 1000/1000\n","1168/1168 [==============================] - 0s 175us/step - loss: 1301843707.6164 - val_loss: 2087066075.1781\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["<keras.callbacks.callbacks.History at 0x7fbe2dab15c0>"]},"metadata":{"tags":[]},"execution_count":26}]},{"cell_type":"code","metadata":{"id":"wGgeMHdP4CjU","colab_type":"code","colab":{}},"source":["Pred_NN = Model.predict(X_test)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"grm5snUVFHR5","colab_type":"code","colab":{}},"source":["prediction_NN = pd.DataFrame(Pred_NN)\n","submission_NN = pd.concat([pd.read_csv('sample_submission.csv')['Id'], prediction_NN], axis = 1)\n","submission_NN.columns = ['Id', 'SalePrice']\n","submission_NN.to_csv('Submission_NN.csv', index = False)\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"3TAI5uVKGDBi","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}